{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Chapter 11.1: Using GPT4All for Applications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**GPT4All** is a revolutionary open-source ecosystem designed to run powerful Large Language Models (LLMs) locally on consumer-grade hardware. It enables developers to build AI-powered applications **without** requiring:\n",
    "\n",
    "- ‚òÅÔ∏è Cloud API calls\n",
    "- üí∞ Subscription fees\n",
    "- üîå Constant internet connection\n",
    "- üñ•Ô∏è Expensive GPU hardware\n",
    "\n",
    "### Why Use GPT4All?\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Privacy** | Your data never leaves your device |\n",
    "| **Cost-Effective** | No API costs, completely free |\n",
    "| **Offline Capable** | Works without internet after initial model download |\n",
    "| **Open Source** | BSD-licensed, community-driven |\n",
    "| **Cross-Platform** | Runs on Windows, macOS, and Linux |\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. üõ†Ô∏è Installation and Setup\n",
    "2. üöÄ Loading Your First Model\n",
    "3. üí¨ Chat Sessions vs Direct Generation\n",
    "4. üéõÔ∏è Controlling Generation Parameters\n",
    "5. üìä Streaming Responses\n",
    "6. üß© Text Embeddings\n",
    "7. üèóÔ∏è Building Practical Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üõ†Ô∏è Installation and Setup\n",
    "\n",
    "The GPT4All Python library provides a simple interface to interact with locally-running LLMs. The library uses the `llama.cpp` backend for efficient CPU/GPU inference.\n",
    "\n",
    "### Installation\n",
    "\n",
    "The gpt4all package can be installed via pip. It's recommended to create a virtual environment before installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install GPT4All\n",
    "# !pip install gpt4all -q\n",
    "\n",
    "# # For embeddings functionality (optional)\n",
    "# !pip install nomic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4All version: 2.8.2\n",
      "‚úÖ GPT4All imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "import gpt4all\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(f\"GPT4All version: {version('gpt4all')}\")\n",
    "print(\"‚úÖ GPT4All imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üöÄ Loading Your First Model\n",
    "\n",
    "GPT4All uses GGUF (GPT-Generated Unified Format) models which are optimized for CPU inference. When you load a model for the first time, it will be automatically downloaded and cached locally.\n",
    "\n",
    "### Available Models\n",
    "\n",
    "| Model | Size | Description | License |\n",
    "|-------|------|-------------|---------|\n",
    "| `Phi-3-mini-4k-instruct.Q4_0.gguf` | ~2GB | Microsoft's small but capable model | MIT |\n",
    "| `orca-mini-3b-gguf2-q4_0.gguf` | ~2GB | Efficient small model | CC-BY-NC-SA |\n",
    "| `Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf` | ~4GB | High-quality Mistral-based | Apache 2.0 |\n",
    "| `Meta-Llama-3-8B-Instruct.Q4_0.gguf` | ~4.6GB | Meta's Llama 3 | Llama 3 License |\n",
    "\n",
    "> **Note:** Quantized models (Q4_0, Q5_1, etc.) use less memory while maintaining reasonable quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
      "This may take a few minutes on first run as the model downloads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.11G/4.11G [00:07<00:00, 565MiB/s]\n",
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.11G/4.11G [00:05<00:00, 721MiB/s]\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "# Load a high-quality Mistral-based model\n",
    "# This will download the model on first run (~4GB)\n",
    "MODEL_NAME = \"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes on first run as the model downloads...\")\n",
    "\n",
    "model = GPT4All(MODEL_NAME)\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Directory\n",
    "\n",
    "You can specify where models should be stored using the `model_path` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Custom directory for model storage\n",
    "custom_path = os.path.expanduser(\"~/my_gpt4all_models\")\n",
    "\n",
    "# Load model with custom path (commented to avoid duplicate downloads)\n",
    "# model = GPT4All(MODEL_NAME, model_path=custom_path)\n",
    "\n",
    "# print(f\"Default model directory: {GPT4All.list_models()[0] if GPT4All.list_models() else 'Check documentation'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üí¨ Chat Sessions vs Direct Generation\n",
    "\n",
    "GPT4All provides two ways to generate text:\n",
    "\n",
    "1. **Chat Session** - Applies chat templates and maintains conversation context\n",
    "2. **Direct Generation** - Raw text completion without formatting\n",
    "\n",
    "### Understanding the Difference\n",
    "\n",
    "| Aspect | Chat Session | Direct Generation |\n",
    "|--------|-------------|------------------|\n",
    "| Context | Maintains conversation history | Stateless |\n",
    "| Response Style | Helpful assistant | Text completion |\n",
    "| Use Case | Chatbots, Q&A | Text completion, creative writing |\n",
    "| Template | Applied automatically | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Chat Session Mode\n",
    "\n",
    "Chat sessions wrap your prompts with appropriate templates (like system prompts and special tokens) that help the model understand it should respond as a helpful assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç≥ Recipe Assistant Demo\n",
      "\n",
      "==================================================\n",
      "Q: What are the main ingredients for making pasta carbonara?\n",
      "\n",
      "A: The main ingredients for making pasta carbonara include spaghetti or other long pasta, eggs, grated Pecorino Romano or Parmesan cheese, pancetta or bacon, garlic, black pepper, and olive oil. Optional ingredients may also include salt, parsley, and red pepper flakes for added flavor.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q: Can I substitute the guanciale with something else?\n",
      "\n",
      "A: Yes, you can substitute guanciale with pancetta or bacon in a traditional carbonara recipe. Both pancetta and bacon provide similar flavors and textures to the dish. If using bacon, make sure to use thick-cut bacon for better results. You may also consider using speck if it's available in your area.\n"
     ]
    }
   ],
   "source": [
    "# Example: Using chat session for a recipe assistant\n",
    "print(\"üç≥ Recipe Assistant Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "with model.chat_session():\n",
    "    # First question\n",
    "    response1 = model.generate(\n",
    "        \"What are the main ingredients for making pasta carbonara?\",\n",
    "        max_tokens=200\n",
    "    )\n",
    "    print(f\"Q: What are the main ingredients for making pasta carbonara?\\n\")\n",
    "    print(f\"A: {response1}\\n\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Follow-up question (the model remembers context!)\n",
    "    response2 = model.generate(\n",
    "        \"Can I substitute the guanciale with something else?\",\n",
    "        max_tokens=150\n",
    "    )\n",
    "    print(f\"Q: Can I substitute the guanciale with something else?\\n\")\n",
    "    print(f\"A: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Direct Generation Mode\n",
    "\n",
    "Direct generation doesn't apply chat templates. The model treats your input as the beginning of a text to complete, rather than a question to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Story Completion Demo\n",
      "\n",
      "==================================================\n",
      "Prompt:\n",
      "The old lighthouse keeper had seen many storms, but this one was different.\n",
      "As the waves crashed against the rocks below, he noticed something glowing in the water.\n",
      "\n",
      "Continuation:\n",
      " Curiosity got the better of him and he decided to investigate. As he approached the shoreline, he saw a figure struggling in the surf. It was a young woman, her clothes torn and drenched, clutching onto a small box that seemed to be pulsating with an otherworldly energy.\n",
      "The lighthouse keeper quickly retrieved the girl from the water and brought her inside his home. He tended to her wounds and gave her something warm to drink while he tried to figure out what was going on with the mysterious box. As he examined it, he noticed that there were strange symbols etched into its surface.\n",
      "Suddenly, a loud boom shook the room as the box began to glow even br\n"
     ]
    }
   ],
   "source": [
    "# Example: Text completion for creative writing\n",
    "print(\"üìù Story Completion Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "story_prompt = \"\"\"The old lighthouse keeper had seen many storms, but this one was different.\n",
    "As the waves crashed against the rocks below, he noticed something glowing in the water.\"\"\"\n",
    "\n",
    "# Direct generation - no chat template\n",
    "continuation = model.generate(\n",
    "    story_prompt,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(f\"Prompt:\\n{story_prompt}\\n\")\n",
    "print(f\"Continuation:\\n{continuation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üéõÔ∏è Controlling Generation Parameters\n",
    "\n",
    "Fine-tune the model's output using various generation parameters:\n",
    "\n",
    "| Parameter | Description | Default | Range |\n",
    "|-----------|-------------|---------|---------|\n",
    "| `max_tokens` | Maximum tokens to generate | 200 | 1-‚àû |\n",
    "| `temp` | Temperature (randomness) | 0.7 | 0.0-2.0 |\n",
    "| `top_k` | Top-k sampling | 40 | 1-100 |\n",
    "| `top_p` | Nucleus sampling | 0.9 | 0.0-1.0 |\n",
    "| `repeat_penalty` | Penalize repetition | 1.18 | 1.0-2.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå°Ô∏è Temperature Comparison Demo\n",
      "\n",
      "==================================================\n",
      "Low Temperature (0.1) - Focused:\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making and language translation. It involves creating algorithms and machines capable of learning from data,\n",
      "\n",
      "Medium Temperature (0.7) - Balanced:\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making and language translation. It involves the creation of algorithms and machines capable of learning from\n",
      "\n",
      "High Temperature (1.2) - Creative:\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision making and language understanding. It involves creating algorithms and machines capable of thinking, reasoning,\n"
     ]
    }
   ],
   "source": [
    "# Example: Comparing different temperature settings\n",
    "print(\"üå°Ô∏è Temperature Comparison Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = \"Describe artificial intelligence in two sentence:\"\n",
    "\n",
    "# Low temperature - more focused and deterministic\n",
    "with model.chat_session():\n",
    "    response_low = model.generate(prompt, max_tokens=50, temp=0.1)\n",
    "    print(f\"Low Temperature (0.1) - Focused:\\n{response_low}\\n\")\n",
    "\n",
    "# Medium temperature - balanced\n",
    "with model.chat_session():\n",
    "    response_med = model.generate(prompt, max_tokens=50, temp=0.7)\n",
    "    print(f\"Medium Temperature (0.7) - Balanced:\\n{response_med}\\n\")\n",
    "\n",
    "# High temperature - more creative but potentially less coherent\n",
    "with model.chat_session():\n",
    "    response_high = model.generate(prompt, max_tokens=50, temp=1.2)\n",
    "    print(f\"High Temperature (1.2) - Creative:\\n{response_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Sampling Strategies Demo\n",
      "\n",
      "==================================================\n",
      "Generated Haiku:\n",
      "Lines of code unfold,  \n",
      "Logic flows like river's bend,  \n",
      "Silent dance transcends.\n"
     ]
    }
   ],
   "source": [
    "# Example: Using top_k and top_p for controlled randomness\n",
    "print(\"üé≤ Sampling Strategies Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "creative_prompt = \"Write a haiku about programming:\"\n",
    "\n",
    "with model.chat_session():\n",
    "    haiku = model.generate(\n",
    "        creative_prompt,\n",
    "        max_tokens=50,\n",
    "        temp=0.8,\n",
    "        top_k=40,     # Consider top 40 tokens\n",
    "        top_p=0.9,    # With 90% cumulative probability\n",
    "        repeat_penalty=1.2  # Avoid repetition\n",
    "    )\n",
    "    print(f\"Generated Haiku:\\n{haiku}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìä Streaming Responses\n",
    "\n",
    "For real-time applications, you can stream responses token by token using a callback function. This is useful for:\n",
    "\n",
    "- üñ•Ô∏è Displaying responses as they're generated\n",
    "- ‚è±Ô∏è Reducing perceived latency\n",
    "- üîÑ Early stopping based on content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming Response Demo\n",
      "\n",
      "==================================================\n",
      "Generating: Machine learning is a type of artificial intelligence where computer systems are able to learn and improve from experience without being explicitly programmed. It involves feeding large amounts of data into an algorithm, which then analyzes it and makes predictions or decisions based on patterns it identifies within the data. The more data the system processes, the better it becomes at making accurate predictions or decisions in new situations. This is useful for tasks such as image recognition, speech recognition, language translation, and predicting customer behavior.\n",
      "\n",
      "‚úÖ Generation complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"üåä Streaming Response Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Generating: \", end=\"\")\n",
    "\n",
    "with model.chat_session():\n",
    "    # With streaming=True, generate() returns a generator\n",
    "    for token in model.generate(\n",
    "        \"Explain the concept of machine learning in simple terms.\",\n",
    "        max_tokens=200,\n",
    "        streaming=True\n",
    "    ):\n",
    "        print(token, end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early Stopping Demo\n",
      "\n",
      "==================================================\n",
      "Generating (max 30 words): The history of computers dates back to ancient times, with early mechanical devices and counting machines being used for calculations. However, modern computing can be traced back to the late ... [STOPPED]\n",
      "\n",
      "\n",
      "üìä Total tokens collected: 35\n"
     ]
    }
   ],
   "source": [
    "# Example: Early stopping with streaming\n",
    "print(\"üõë Early Stopping Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "collected_tokens = []\n",
    "word_count = 0\n",
    "max_words = 30\n",
    "\n",
    "print(f\"Generating (max {max_words} words): \", end=\"\")\n",
    "\n",
    "with model.chat_session():\n",
    "    for token in model.generate(\n",
    "        \"Tell me about the history of computers.\",\n",
    "        max_tokens=500,\n",
    "        streaming=True\n",
    "    ):\n",
    "        collected_tokens.append(token)\n",
    "        print(token, end='', flush=True)\n",
    "        \n",
    "        # Count words (rough approximation)\n",
    "        if ' ' in token or token.strip() == '':\n",
    "            word_count += 1\n",
    "        \n",
    "        # Stop after max_words\n",
    "        if word_count >= max_words:\n",
    "            print(\"... [STOPPED]\")\n",
    "            break  # Use break to stop the generator\n",
    "\n",
    "print(f\"\\n\\nüìä Total tokens collected: {len(collected_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üß© Text Embeddings\n",
    "\n",
    "GPT4All also supports generating text embeddings for semantic search, clustering, and RAG (Retrieval-Augmented Generation) applications.\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts will have similar vector representations.\n",
    "\n",
    "```\n",
    "\"I love dogs\" ‚Üí [0.1, 0.8, -0.3, ...]\n",
    "\"I adore puppies\" ‚Üí [0.12, 0.75, -0.28, ...] ‚Üê Similar!\n",
    "\"The weather is nice\" ‚Üí [-0.5, 0.2, 0.9, ...] ‚Üê Different\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45.9M/45.9M [00:00<00:00, 104MiB/s] \n",
      "Verifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45.9M/45.9M [00:00<00:00, 727MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import Embed4All\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedder = Embed4All()\n",
    "print(\"‚úÖ Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Text Embedding Demo\n",
      "\n",
      "==================================================\n",
      "Number of texts: 3\n",
      "Embedding dimension: 384\n",
      "\n",
      "First 5 values of first embedding: [-0.05978590250015259, -0.028882525861263275, -0.004007628187537193, 0.03483599051833153, -0.050448235124349594]\n"
     ]
    }
   ],
   "source": [
    "# Example: Generate embeddings for texts\n",
    "print(\"üìä Text Embedding Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "texts = [\n",
    "    \"Python is a great programming language for data science.\",\n",
    "    \"Data analysis with Python is popular among scientists.\",\n",
    "    \"The weather forecast predicts rain tomorrow.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = [embedder.embed(text) for text in texts]\n",
    "\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"\\nFirst 5 values of first embedding: {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Similarity Demo\n",
      "\n",
      "==================================================\n",
      "Text pairs and their similarities:\n",
      "\n",
      "1. 'Python is a great programming language f...'\n",
      "2. 'Data analysis with Python is popular amo...'\n",
      "3. 'The weather forecast predicts rain tomor...'\n",
      "\n",
      "Similarity (1, 2): 0.7852 ‚úÖ Related!\n",
      "Similarity (1, 3): 0.0182 ‚ùå Different\n",
      "Similarity (2, 3): 0.0314 ‚ùå Different\n"
     ]
    }
   ],
   "source": [
    "# Example: Computing semantic similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "print(\"üîç Semantic Similarity Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compare similarities\n",
    "sim_1_2 = cosine_similarity(embeddings[0], embeddings[1])\n",
    "sim_1_3 = cosine_similarity(embeddings[0], embeddings[2])\n",
    "sim_2_3 = cosine_similarity(embeddings[1], embeddings[2])\n",
    "\n",
    "print(\"Text pairs and their similarities:\\n\")\n",
    "print(f\"1. '{texts[0][:40]}...'\")\n",
    "print(f\"2. '{texts[1][:40]}...'\")\n",
    "print(f\"3. '{texts[2][:40]}...'\\n\")\n",
    "\n",
    "print(f\"Similarity (1, 2): {sim_1_2:.4f} {'‚úÖ Related!' if sim_1_2 > 0.7 else ''}\")\n",
    "print(f\"Similarity (1, 3): {sim_1_3:.4f} {'‚ùå Different' if sim_1_3 < 0.5 else ''}\")\n",
    "print(f\"Similarity (2, 3): {sim_2_3:.4f} {'‚ùå Different' if sim_2_3 < 0.5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üèóÔ∏è Building Practical Applications\n",
    "\n",
    "Let's build some practical applications using GPT4All!\n",
    "\n",
    "### Application 1: Personal Knowledge Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Knowledge Assistant Demo\n",
      "\n",
      "==================================================\n",
      "Q: What is a decorator in Python and when would I use one?\n",
      "\n",
      "A: A decorator in Python is a design pattern that allows you to modify the behavior of a function or class without permanently modifying it, by abstracting away common functionality from similar functions/classes. Decorators are syntactic sugar that makes code more readable and reusable. They can be used for various purposes such as logging, caching, authorization checks, etc.\n",
      "\n",
      "Here's a simple example of how to use a decorator:\n",
      "\n",
      "```python\n",
      "def my_decorator(func):\n",
      "    def wrapper():\n",
      "        print(\"Something is happening before the function is called.\")\n",
      "        func()\n",
      "        print(\"Something is happening after the function is called.\")\n",
      "    return wrapper\n",
      "\n",
      "@my_decorator\n",
      "def say_hello():\n",
      "    print(\"Hello!\")\n",
      "```\n",
      "\n",
      "In this example, `say_hello` is a decorated function. When you call `say_hello()`, it will first print \"Something is happening before the function is called.\", then execute the original function (which prints \"Hello!\"), and finally print \"Something is happening after the function is called.\". \n",
      "\n",
      "The decorator pattern separates cross-cutting concerns from the main functionality of a function or class, making it easier to manage and maintain code. It's useful when you want to add additional behavior around the execution of a function without permanently modifying it.\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeAssistant:\n",
    "    \"\"\"\n",
    "    A simple knowledge assistant that can answer questions\n",
    "    with customizable expertise areas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\", expertise=\"general\"):\n",
    "        self.model = GPT4All(model_name)\n",
    "        self.expertise = expertise\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # System prompts for different expertise areas\n",
    "        self.system_prompts = {\n",
    "            \"general\": \"You are a helpful assistant.\",\n",
    "            \"programming\": \"You are an expert programmer who explains concepts clearly with code examples.\",\n",
    "            \"science\": \"You are a science educator who explains complex topics in simple terms.\",\n",
    "            \"creative\": \"You are a creative writer who helps with storytelling and creative projects.\"\n",
    "        }\n",
    "    \n",
    "    def ask(self, question, max_tokens=300):\n",
    "        \"\"\"\n",
    "        Ask the assistant a question.\n",
    "        \n",
    "        Args:\n",
    "            question: The question to ask\n",
    "            max_tokens: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            The assistant's response\n",
    "        \"\"\"\n",
    "        system_prompt = self.system_prompts.get(self.expertise, self.system_prompts[\"general\"])\n",
    "        \n",
    "        with self.model.chat_session(system_prompt):\n",
    "            response = self.model.generate(question, max_tokens=max_tokens)\n",
    "            \n",
    "        # Store in history\n",
    "        self.conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "# Demo\n",
    "print(\"ü§ñ Knowledge Assistant Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a programming-focused assistant\n",
    "assistant = KnowledgeAssistant(expertise=\"programming\")\n",
    "\n",
    "question = \"What is a decorator in Python and when would I use one?\"\n",
    "print(f\"Q: {question}\\n\")\n",
    "\n",
    "answer = assistant.ask(question)\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Document Q&A System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Document Q&A Demo\n",
      "\n",
      "==================================================\n",
      "‚úÖ Added document: 'About GPT4All'\n",
      "‚úÖ Added document: 'Understanding Embeddings'\n",
      "‚úÖ Added document: 'GGUF Format'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Q: What is GPT4All and who created it?\n",
      "\n",
      "A: GPT4All is an open-source ecosystem for training and deploying efficient, assistant-style large language models that can run locally on consumer CPUs. It was created by Nomic AI.\n",
      "\n",
      "üìñ Sources used: ['About GPT4All', 'GGUF Format']\n"
     ]
    }
   ],
   "source": [
    "class SimpleDocQA:\n",
    "    \"\"\"\n",
    "    A simple document Q&A system using embeddings for retrieval.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedder = Embed4All()\n",
    "        self.model = GPT4All(\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\")\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text, title=\"Untitled\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        embedding = self.embedder.embed(text)\n",
    "        self.documents.append({\"title\": title, \"text\": text})\n",
    "        self.embeddings.append(embedding)\n",
    "        print(f\"‚úÖ Added document: '{title}'\")\n",
    "    \n",
    "    def find_relevant(self, query, top_k=2):\n",
    "        \"\"\"Find the most relevant documents for a query.\"\"\"\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [\n",
    "            cosine_similarity(query_embedding, emb) \n",
    "            for emb in self.embeddings\n",
    "        ]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                **self.documents[i], \n",
    "                \"similarity\": similarities[i]\n",
    "            } \n",
    "            for i in top_indices\n",
    "        ]\n",
    "    \n",
    "    def answer(self, question, max_tokens=300):\n",
    "        \"\"\"Answer a question using retrieved documents.\"\"\"\n",
    "        # Find relevant documents\n",
    "        relevant = self.find_relevant(question)\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"Document: {doc['title']}\\n{doc['text']}\" \n",
    "            for doc in relevant\n",
    "        )\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Based on the following documents, answer the question.\n",
    "                    \n",
    "                    Documents:\n",
    "                    {context}\n",
    "                    \n",
    "                    Question: {question}\n",
    "                    \n",
    "                    Answer:\"\"\"\n",
    "        \n",
    "        with self.model.chat_session():\n",
    "            response = self.model.generate(prompt, max_tokens=max_tokens)\n",
    "        \n",
    "        return response, relevant\n",
    "\n",
    "# Demo\n",
    "print(\"üìö Document Q&A Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create Q&A system\n",
    "qa = SimpleDocQA()\n",
    "\n",
    "# Add some documents\n",
    "qa.add_document(\n",
    "    \"GPT4All is an open-source ecosystem to train and deploy efficient, \" +\n",
    "    \"assistant-style large language models that run locally on consumer CPU. \" +\n",
    "    \"It was created by Nomic AI and supports various models.\",\n",
    "    title=\"About GPT4All\"\n",
    ")\n",
    "\n",
    "qa.add_document(\n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning. \" +\n",
    "    \"Similar texts have similar embeddings. They are used for semantic search, \" +\n",
    "    \"clustering, and retrieval-augmented generation (RAG).\",\n",
    "    title=\"Understanding Embeddings\"\n",
    ")\n",
    "\n",
    "qa.add_document(\n",
    "    \"GGUF is a file format for storing large language models. \" +\n",
    "    \"It supports quantization which reduces model size while maintaining quality. \" +\n",
    "    \"Common quantization levels include Q4_0, Q5_1, and Q8_0.\",\n",
    "    title=\"GGUF Format\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "question = \"What is GPT4All and who created it?\"\n",
    "print(f\"\\nQ: {question}\\n\")\n",
    "\n",
    "answer, sources = qa.answer(question)\n",
    "print(f\"A: {answer}\\n\")\n",
    "print(f\"üìñ Sources used: {[s['title'] for s in sources]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 3: Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Text Summarizer Demo\n",
      "\n",
      "==================================================\n",
      "Original Text:\n",
      "\n",
      "Artificial Intelligence (AI) has transformed numerous industries over the past decade.\n",
      "In healthcare, AI systems can now detect diseases from medical images with accuracy \n",
      "matching or exceeding human experts. Financial institutions use AI for fraud detection,\n",
      "analyzing millions of transactions in real-time. The automotive industry has embraced\n",
      "AI for developing self-driving vehicles, with companies investing billions in research.\n",
      "However, these advancements also raise important ethical questions about job displacement,\n",
      "privacy, and algorithmic bias that society must address.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "üìå Bullet Point Summary:\n",
      "* AI has transformed multiple industries including healthcare, finance, and automotive\n",
      "* In healthcare, AI can accurately detect diseases from medical images like humans\n",
      "* Financial institutions use AI for real-time fraud detection in millions of transactions\n",
      "* Automotive industry invests billions in research for self-driving vehicles development\n",
      "* Raises ethical questions about job displacement, privacy, and algorithmic bias\n"
     ]
    }
   ],
   "source": [
    "class TextSummarizer:\n",
    "    \"\"\"\n",
    "    A simple text summarizer with adjustable summary length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\"):\n",
    "        self.model = GPT4All(model_name)\n",
    "    \n",
    "    def summarize(self, text, style=\"concise\", max_tokens=150):\n",
    "        \"\"\"\n",
    "        Summarize the given text.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to summarize\n",
    "            style: 'concise', 'detailed', or 'bullet'\n",
    "            max_tokens: Maximum length of summary\n",
    "            \n",
    "        Returns:\n",
    "            Summary of the text\n",
    "        \"\"\"\n",
    "        style_instructions = {\n",
    "            \"concise\": \"Provide a brief, one-paragraph summary.\",\n",
    "            \"detailed\": \"Provide a comprehensive summary covering all key points.\",\n",
    "            \"bullet\": \"Provide a summary as bullet points highlighting key information.\"\n",
    "        }\n",
    "        \n",
    "        instruction = style_instructions.get(style, style_instructions[\"concise\"])\n",
    "        \n",
    "        prompt = f\"\"\"{instruction}\n",
    "                    \n",
    "                    Text to summarize:\n",
    "                    {text}\n",
    "                    \n",
    "                    Summary:\"\"\"\n",
    "        \n",
    "        with self.model.chat_session():\n",
    "            summary = self.model.generate(prompt, max_tokens=max_tokens)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Demo\n",
    "print(\"üìù Text Summarizer Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed numerous industries over the past decade.\n",
    "In healthcare, AI systems can now detect diseases from medical images with accuracy \n",
    "matching or exceeding human experts. Financial institutions use AI for fraud detection,\n",
    "analyzing millions of transactions in real-time. The automotive industry has embraced\n",
    "AI for developing self-driving vehicles, with companies investing billions in research.\n",
    "However, these advancements also raise important ethical questions about job displacement,\n",
    "privacy, and algorithmic bias that society must address.\n",
    "\"\"\"\n",
    "\n",
    "summarizer = TextSummarizer()\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "print(\"\\nüìå Bullet Point Summary:\")\n",
    "bullet_summary = summarizer.summarize(sample_text, style=\"bullet\")\n",
    "print(bullet_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üí° Best Practices and Tips\n",
    "\n",
    "### Memory Management\n",
    "\n",
    "GPT4All models run on CPU by default, so memory usage is important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Best Practices Demo\n",
      "\n",
      "==================================================\n",
      "Q1: What is Python?\n",
      "A1: Python is a high-level, interpreted programming language that was created by Guido van Rossum and fi...\n",
      "\n",
      "Q2: What is JavaScript?\n",
      "A2: JavaScript (JS) is a high-level, dynamic, and versatile programming language used for creating inter...\n",
      "\n",
      "‚úÖ Model unloaded\n"
     ]
    }
   ],
   "source": [
    "# Tip: Use context managers for proper resource cleanup\n",
    "print(\"üí° Best Practices Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Good practice: Load model once, use multiple times\n",
    "model = GPT4All(\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\")\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(questions, 1):\n",
    "    with model.chat_session():\n",
    "        response = model.generate(q, max_tokens=50)\n",
    "        print(f\"Q{i}: {q}\")\n",
    "        print(f\"A{i}: {response[:100]}...\\n\")\n",
    "\n",
    "# Clean up when done\n",
    "del model\n",
    "print(\"‚úÖ Model unloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Safe Model Loading Demo\n",
      "\n",
      "==================================================\n",
      "Attempting to load: Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
      "‚úÖ Successfully loaded Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# Robust model loading with error handling\n",
    "def safe_load_model(model_name, fallback_model=\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\"):\n",
    "    \"\"\"\n",
    "    Safely load a GPT4All model with fallback option.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Primary model to load\n",
    "        fallback_model: Model to use if primary fails\n",
    "        \n",
    "    Returns:\n",
    "        GPT4All model instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to load: {model_name}\")\n",
    "        model = GPT4All(model_name)\n",
    "        print(f\"‚úÖ Successfully loaded {model_name}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {model_name}: {str(e)}\")\n",
    "        print(f\"üîÑ Falling back to {fallback_model}\")\n",
    "        try:\n",
    "            model = GPT4All(fallback_model)\n",
    "            print(f\"‚úÖ Successfully loaded fallback model\")\n",
    "            return model\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fatal: Could not load any model: {str(e2)}\")\n",
    "            raise\n",
    "\n",
    "# Example usage\n",
    "print(\"üõ°Ô∏è Safe Model Loading Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# This will use fallback if primary isn't available\n",
    "model = safe_load_model(\"Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "In this chapter, we've learned:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **GPT4All** enables running LLMs locally without cloud dependencies\n",
    "2. **GGUF format** provides efficient model storage with quantization\n",
    "3. **Chat sessions** vs **direct generation** serve different purposes\n",
    "4. **Generation parameters** (temperature, top_k, top_p) control output\n",
    "5. **Streaming** enables real-time response display\n",
    "6. **Embeddings** power semantic search and RAG applications\n",
    "\n",
    "### Practical Applications Built\n",
    "\n",
    "- ü§ñ Knowledge Assistant with customizable expertise\n",
    "- üìö Document Q&A System with semantic retrieval\n",
    "- üìù Multi-style Text Summarizer\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore larger models for better quality (if hardware permits)\n",
    "- Build RAG applications with local document stores\n",
    "- Integrate with frameworks like LangChain for advanced workflows\n",
    "- Deploy applications using Gradio or Streamlit interfaces\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [GPT4All Official Documentation](https://docs.gpt4all.io/)\n",
    "- [GPT4All GitHub Repository](https://github.com/nomic-ai/gpt4all)\n",
    "- [GGUF Model Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)\n",
    "- [Nomic Embeddings](https://docs.nomic.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
