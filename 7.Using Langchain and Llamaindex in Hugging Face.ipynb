{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Chapter 7: Using Langchain and Llamaindex in Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand Large Language Models (LLMs)** - Learn what LLMs are and their key characteristics\n",
    "2. **Build LangChain Applications** - Create LLM applications using prompt templates and chains\n",
    "3. **Implement Conversational AI** - Maintain context and history in LLM conversations\n",
    "4. **Connect LLMs to Private Data** - Use LlamaIndex for Retrieval-Augmented Generation (RAG)\n",
    "5. **Build Interactive Interfaces** - Create web frontends using Gradio\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. [Introduction to Large Language Models (LLMs)](#1-introduction-to-large-language-models)\n",
    "2. [Getting Started with LangChain](#2-getting-started-with-langchain)\n",
    "3. [Building Conversational Chains](#3-building-conversational-chains)\n",
    "4. [Using Different LLM Providers](#4-using-different-llm-providers)\n",
    "5. [Introduction to LlamaIndex and RAG](#5-introduction-to-llamaindex-and-rag)\n",
    "6. [Building a Document Q&A System](#6-building-a-document-qa-system)\n",
    "7. [Creating Interactive Interfaces with Gradio](#7-creating-interactive-interfaces-with-gradio)\n",
    "8. [Summary and Best Practices](#8-summary-and-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Environment Setup\n",
    "\n",
    "First, let's install all the required packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install required packages\n",
    "# !pip install -q langchain langchain-huggingface langchain-core langchain-community\n",
    "# !pip install -q llama-index llama-index-core llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-llms-huggingface llama-index-readers-file\n",
    "# !pip install -q transformers torch gradio huggingface_hub accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API tokens (Replace with your actual tokens)\n",
    "import os\n",
    "\n",
    "# You can get your Hugging Face token at: https://huggingface.co/settings/tokens\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'hf_---------------------'\n",
    "\n",
    "# Optional: For OpenAI integration (get key at: https://platform.openai.com/api-keys)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Large Language Models (LLMs) <a id=\"1-introduction-to-large-language-models\"></a>\n",
    "\n",
    "### üß† What is an LLM?\n",
    "\n",
    "A **Large Language Model (LLM)** is a type of AI model designed to understand and generate human-like text based on patterns learned from massive amounts of training data.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Size & Scale** | Trained on billions of tokens from books, articles, websites, videos |\n",
    "| **Pretraining** | Learns statistical relationships between words and sentences |\n",
    "| **Fine-tuning** | Specialized for specific tasks like classification, translation, etc. |\n",
    "\n",
    "### üìä Scale Comparison of Popular LLMs:\n",
    "\n",
    "| Model | Parameters | Training Tokens |\n",
    "|-------|------------|----------------|\n",
    "| GPT-3 (Davinci) | 175 billion | 499 billion |\n",
    "| Llama 3.1 | 8B - 405B | 15 trillion |\n",
    "| Mistral 7B | 7 billion | ~1 trillion |\n",
    "\n",
    "### üî§ Understanding Tokenization\n",
    "\n",
    "LLMs don't process text character by character. Instead, they use **tokens** - chunks of text that are processed as single units.\n",
    "\n",
    "**Example:** The word \"artificial\" might be split into:\n",
    "- `art` + `ificial` (2 tokens)\n",
    "\n",
    "This is called **subword tokenization** (e.g., Byte-Pair Encoding or BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Tokenization Examples:\n",
      "\n",
      "Text: 'Artificial Intelligence is transforming the world.'\n",
      "Tokens (8): ['Art', 'ificial', 'ƒ†Intelligence', 'ƒ†is', 'ƒ†transforming', 'ƒ†the', 'ƒ†world', '.']\n",
      "------------------------------------------------------------\n",
      "Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Tokens (10): ['The', 'ƒ†quick', 'ƒ†brown', 'ƒ†fox', 'ƒ†jumps', 'ƒ†over', 'ƒ†the', 'ƒ†lazy', 'ƒ†dog', '.']\n",
      "------------------------------------------------------------\n",
      "Text: 'Supercalifragilisticexpialadocious'\n",
      "Tokens (11): ['Super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'ad', 'ocious']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Visualizing how tokenization works\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Example sentences\n",
    "texts = [\n",
    "    \"Artificial Intelligence is transforming the world.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Supercalifragilisticexpialadocious\"  # A very long word!\n",
    "]\n",
    "\n",
    "print(\"üîç Tokenization Examples:\\n\")\n",
    "for text in texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Getting Started with LangChain <a id=\"2-getting-started-with-langchain\"></a>\n",
    "\n",
    "### üîó What is LangChain?\n",
    "\n",
    "**LangChain** is a framework designed to simplify the creation of applications using LLMs. Think of it as a way to \"chain\" together different components:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Prompt Template ‚îÇ -> ‚îÇ     LLM     ‚îÇ -> ‚îÇ  Output Parser   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Prompt Templates** - Structure instructions for LLMs\n",
    "- **LLMs** - The language models (GPT, Llama, Zephyr, etc.)\n",
    "- **Chains** - Connect components together\n",
    "- **Memory** - Maintain conversation context\n",
    "- **Agents** - Make decisions based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Creating Prompt Templates\n",
    "\n",
    "A **Prompt Template** structures the instruction given to the model. It's like a fill-in-the-blank form that gets completed with user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Prompt Template Created:\n",
      "input_variables=['question'] input_types={} partial_variables={} template='\\nYou are a helpful expert assistant.\\n\\nQuestion: {question}\\n\\nPlease provide a clear and concise answer:\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Simple Q&A template\n",
    "qa_template = '''\n",
    "You are a helpful expert assistant.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear and concise answer:\n",
    "'''\n",
    "\n",
    "# Create the prompt template\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=qa_template,\n",
    "    input_variables=['question']  # These variables will be filled in\n",
    ")\n",
    "\n",
    "# Preview the prompt template\n",
    "print(\"üìã Prompt Template Created:\")\n",
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Filled Prompt:\n",
      "\n",
      "You are a helpful expert assistant.\n",
      "\n",
      "Question: What causes the Northern Lights?\n",
      "\n",
      "Please provide a clear and concise answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the template gets filled\n",
    "sample_question = \"What causes the Northern Lights?\"\n",
    "filled_prompt = qa_prompt.format(question=sample_question)\n",
    "\n",
    "print(\"‚ú® Filled Prompt:\")\n",
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Connecting to Hugging Face LLMs\n",
    "\n",
    "Now let's connect our prompt template to an actual LLM hosted on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized with stop sequences!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Initialize with stop sequences and max tokens limit\n",
    "llm_endpoint = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    task=\"conversational\",\n",
    "    max_new_tokens=256,  # Limits response length\n",
    "    stop_sequences=[\"User:\", \"Human:\", \"\\n\\nUser\", \"\\n\\nHuman\", \"[/INST]\"],\n",
    "    temperature=0.7,\n",
    "    huggingfacehub_api_token=os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    ")\n",
    "\n",
    "# Wrap in ChatHuggingFace for proper chat formatting\n",
    "llm = ChatHuggingFace(llm=llm_endpoint)\n",
    "\n",
    "print(\"‚úÖ LLM initialized with stop sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚õìÔ∏è Creating Your First LLM Chain\n",
    "\n",
    "A **chain** connects the prompt template, LLM, and output parser together using the pipe (`|`) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The sky looks blue because the Earth's atmosphere scatters short wavelength light (blue light) more than it does long wavelength light (red light), causing blue light to be reflected back to our eyes while red light passes through and appears as less visible. This phenomenon is known as scattering, and it's due to the presence of tiny particles in the atmosphere, such as dust, water vapor, and air molecules. When sunlight enters the Earth's atmosphere, it interacts with these particles and scatters in all directions, causing the blue light to be more dispersed than other colors, resulting in the appearance of a blue sky. This is what creates the blue color we see when we look up at the sky. The amount of scattering depends on the angle of the sun and the amount of particles in the air, which can vary throughout the day and in different weather conditions, creating different shades of blue or even orange or pink skies during sunrise and sunset. However, during pollution or smoggy conditions, the atmosphere can appear hazy or gray due to high levels of particulate matter, resulting in a less blue sky. This effect is called the Rayleigh scattering. This is why the sun appears red\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create your chain \n",
    "llm_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test it \n",
    "response = llm_chain.invoke({\"question\": \"Why is the sky blue?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Building Conversational Chains <a id=\"3-building-conversational-chains\"></a>\n",
    "\n",
    "### üó£Ô∏è The Problem with Stateless LLMs\n",
    "\n",
    "By default, LLMs don't remember previous interactions. Each query is treated independently.\n",
    "\n",
    "**Example Problem:**\n",
    "- User: \"Who invented the telephone?\"\n",
    "- AI: \"Alexander Graham Bell\"\n",
    "- User: \"When was he born?\" \n",
    "- AI: ‚ùå Doesn't know who \"he\" refers to!\n",
    "\n",
    "### üíæ Solution 1: Manual History Management\n",
    "\n",
    "We can modify our prompt template to include conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversation chain created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Use ChatPromptTemplate instead of string-based PromptTemplate\n",
    "conversation_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions concisely and accurately.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Create chain\n",
    "conversation_chain = conversation_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Conversation chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Space Exploration Conversation:\n",
      "\n",
      "üë§ You: Who was the first person to walk on the moon?\n",
      "ü§ñ Assistant: Neil Armstrong was the first person to walk on the moon. He was a part of the Apollo 11 mission that landed on the moon on July 20, 1969. He took his famous first steps on the lunar surface, declaring, \"That's one small step for man, one giant leap for mankind.\"\n",
      "------------------------------------------------------------\n",
      "üë§ You: Which mission was that part of?\n",
      "ü§ñ Assistant: The Apollo 11 mission.\n",
      "------------------------------------------------------------\n",
      "üë§ You: How long did the entire journey take?\n",
      "ü§ñ Assistant: The entire Apollo 11 mission took 11 days, 3 hours, and 38 minutes, from launch to splashdown. The moon landing itself took approximately 2.5 hours.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Store history as message objects (not raw strings)\n",
    "message_history = []\n",
    "\n",
    "conversation_flow = [\n",
    "    \"Who was the first person to walk on the moon?\",\n",
    "    \"Which mission was that part of?\",\n",
    "    \"How long did the entire journey take?\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Space Exploration Conversation:\\n\")\n",
    "\n",
    "for question in conversation_flow:\n",
    "    print(f\"üë§ You: {question}\")\n",
    "    \n",
    "    # Invoke with proper history format\n",
    "    response = conversation_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"history\": message_history\n",
    "    })\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    response = response.strip()\n",
    "    \n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    \n",
    "    # Add to history as proper message objects\n",
    "    message_history.append(HumanMessage(content=question))\n",
    "    message_history.append(AIMessage(content=response))\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ Solution 2: Using RunnableWithMessageHistory\n",
    "\n",
    "LangChain provides a built-in class for managing conversation history more elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ History-aware chain created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Store for session histories\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Create a chain with message history\n",
    "with_history_chain = RunnableWithMessageHistory(\n",
    "    qa_prompt | llm | StrOutputParser(),\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ History-aware chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "\n",
      "Thomas Edison invented the practical light bulb in 1879. While others had attempted electric lighting before him, his version was the first to be commercially successful and widely adopted.\n"
     ]
    }
   ],
   "source": [
    "# To use the chain, you need to provide a session_id in the config\n",
    "# The session_id groups messages together - same ID = same conversation\n",
    "\n",
    "# Example 1: Single question\n",
    "response = with_history_chain.invoke(\n",
    "    {\"question\": \"Who invented the light bulb?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user123\"}}\n",
    ")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Conversation with Memory:\n",
      "\n",
      "üë§ You: Who was the first person in space?\n",
      "ü§ñ Assistant: The first person in space was Yuri Gagarin, a Soviet cosmonaut, who completed the first human spaceflight on April 12, 1961. He was launched into orbit aboard the Vostok 1 spacecraft as part of the Vostok 1 mission.\n",
      "--------------------------------------------------\n",
      "üë§ You: What country was he from?\n",
      "ü§ñ Assistant: Yuri Gagarin was a Soviet cosmonaut who completed the first human spaceflight on April 12, 1961. He was from the Soviet Union (now Russia).\n",
      "--------------------------------------------------\n",
      "üë§ You: When did this happen?\n",
      "ü§ñ Assistant: The first human spaceflight occurred on April 12, 1961, and the first person in space was Yuri Gagarin, a Soviet cosmonaut from the Soviet Union (now Russia).\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Multi-turn conversation (same session_id = remembers context)\n",
    "\n",
    "session = \"space_chat\"  # Use a consistent session ID for related questions\n",
    "\n",
    "questions = [\n",
    "    \"Who was the first person in space?\",\n",
    "    \"What country was he from?\",    # \"he\" refers to previous answer\n",
    "    \"When did this happen?\"          # \"this\" refers to the space journey\n",
    "]\n",
    "\n",
    "print(\"üöÄ Conversation with Memory:\\n\")\n",
    "for q in questions:\n",
    "    print(f\"üë§ You: {q}\")\n",
    "    \n",
    "    response = with_history_chain.invoke(\n",
    "        {\"question\": q},\n",
    "        config={\"configurable\": {\"session_id\": session}}\n",
    "    )\n",
    "    \n",
    "    print(f\"ü§ñ Assistant: {response.strip()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú Conversation History:\n",
      "[HumanMessage(content='Who was the first person in space?', additional_kwargs={}, response_metadata={}), AIMessage(content='The first person in space was Yuri Gagarin, a Soviet cosmonaut, who completed the first human spaceflight on April 12, 1961. He was launched into orbit aboard the Vostok 1 spacecraft as part of the Vostok 1 mission.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='What country was he from?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yuri Gagarin was a Soviet cosmonaut who completed the first human spaceflight on April 12, 1961. He was from the Soviet Union (now Russia).', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='When did this happen?', additional_kwargs={}, response_metadata={}), AIMessage(content=' \\n\\nThe first human spaceflight occurred on April 12, 1961, and the first person in space was Yuri Gagarin, a Soviet cosmonaut from the Soviet Union (now Russia).', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Check the stored history\n",
    "print(\"\\nüìú Conversation History:\")\n",
    "print(store[\"space_chat\"].messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New topic answer: \n",
      "\n",
      "Photosynthesis is a biological process that occurs in green plants, algae, and some bacteria, through which they convert light energy into chemical energy in the form of glucose and oxygen. During photosynthesis, chlorophyll in plant cells captures light energy and converts carbon dioxide and water into oxygen and glucose in the presence of water and chlorophyll. This process is essential for the growth and survival of most living organisms, as it provides energy for plants to sustain life and releases oxygen into the atmosphere, which is essential for all living beings to breathe. This process takes place in the chloroplasts of plant cells and is essential for the ecosystem as it produces food for herbivores and other organisms, and serves as the base of the food chain. It is a vital process for the maintenance of the earth's atmosphere, as it contributes to the oxygen-nitrogen cycle and helps to maintain the balance of carbon dioxide levels.\n",
      "\n",
      "Photosynthesis is important for the production of oxygen in the earth's atmosphere, which is necessary for the survival of all aerobic organisms, as it is the primary source of oxygen in the\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Start a NEW conversation (different session_id)\n",
    "response = with_history_chain.invoke(\n",
    "    {\"question\": \"What is photosynthesis?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"biology_chat\"}}  # New session!\n",
    ")\n",
    "print(f\"New topic answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Using Different LLM Providers <a id=\"4-using-different-llm-providers\"></a>\n",
    "\n",
    "LangChain supports multiple LLM providers. Let's explore some options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Translation Examples:\n",
      "\n",
      "English: Hello, how are you today?\n",
      "Spanish: Hola, ¬øc√≥mo est√°s hoy?\n",
      "--------------------------------------------------\n",
      "English: The weather is beautiful.\n",
      "French: Le temps est beau.\n",
      "--------------------------------------------------\n",
      "English: Thank you very much!\n",
      "Japanese: „ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "translation_llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    task=\"conversational\",\n",
    "    max_new_tokens=128,\n",
    "    stop_sequences=[\"\\n\\n\", \"Text:\", \"English:\"],\n",
    "    temperature=0.3,  # Lower temperature for more accurate translations\n",
    "    huggingfacehub_api_token=os.environ.get('HUGGINGFACEHUB_API_TOKEN')\n",
    ")\n",
    "\n",
    "# Wrap for proper chat handling\n",
    "translation_chat = ChatHuggingFace(llm=translation_llm)\n",
    "\n",
    "# Create a translation chain\n",
    "translation_template = '''Translate the following text to {target_language}. \n",
    "Only provide the translation, nothing else.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Translation:'''\n",
    "\n",
    "translation_prompt = PromptTemplate(\n",
    "    template=translation_template,\n",
    "    input_variables=['text', 'target_language']\n",
    ")\n",
    "\n",
    "translation_chain = translation_prompt | translation_chat | StrOutputParser()\n",
    "\n",
    "# Test translations\n",
    "translations = [\n",
    "    {\"text\": \"Hello, how are you today?\", \"target_language\": \"Spanish\"},\n",
    "    {\"text\": \"The weather is beautiful.\", \"target_language\": \"French\"},\n",
    "    {\"text\": \"Thank you very much!\", \"target_language\": \"Japanese\"}\n",
    "]\n",
    "\n",
    "print(\"üåç Translation Examples:\\n\")\n",
    "for t in translations:\n",
    "    result = translation_chain.invoke(t)\n",
    "    # Clean up the result\n",
    "    result = result.strip().split('\\n')[0]  # Take only first line\n",
    "    print(f\"English: {t['text']}\")\n",
    "    print(f\"{t['target_language']}: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Creative Use Case: Code Explanation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Code Explanation:\n",
      "\n",
      "\n",
      "This code snippet is defining a function called `fibonacci` that takes an argument `n`. This function uses recursive calls to calculate the nth number of the Fibonacci sequence. The Fibonacci sequence is a mathematical sequence where each number is the sum of the previous two numbers. For example, the first two numbers are 0 and 1. The function first checks if `n` is less than or equal to 1. If it is, it returns `n` since the base case is when the number is 0 or 1. If `n` is greater than 1, the function calls itself recursively to get the (n-1)th and (n-2)th numbers and adds them together to get the `n`th number. So, for instance, if we call `fibonacci(5)`, it will call itself recursively for `n-1` (which is `4`) and get the 4th and 3rd numbers, add them up, and return the 5th number. This continues until we reach the base case (0 or 1) and return the final answer. This function will keep calling itself to calculate all the numbers\n"
     ]
    }
   ],
   "source": [
    "# Create a code explainer chain\n",
    "code_explainer_template = '''\n",
    "You are a programming tutor. Explain the following code snippet in simple terms.\n",
    "Target audience: {audience}\n",
    "\n",
    "Code:\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "'''\n",
    "\n",
    "code_prompt = PromptTemplate(\n",
    "    template=code_explainer_template,\n",
    "    input_variables=['code', 'language', 'audience']\n",
    ")\n",
    "\n",
    "code_chain = code_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Example code snippets to explain\n",
    "python_code = '''\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "'''\n",
    "\n",
    "print(\"üíª Code Explanation:\\n\")\n",
    "explanation = code_chain.invoke({\n",
    "    'code': python_code,\n",
    "    'language': 'python',\n",
    "    'audience': 'beginner programmers'\n",
    "})\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Introduction to LlamaIndex and RAG <a id=\"5-introduction-to-llamaindex-and-rag\"></a>\n",
    "\n",
    "### üìö What is LlamaIndex?\n",
    "\n",
    "**LlamaIndex** is a data framework that enables LLM-based applications to ingest, structure, and access private or domain-specific data.\n",
    "\n",
    "### üîÑ What is RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "**RAG** enhances LLM performance by integrating them with an external retrieval system:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Question   ‚îÇ ->  ‚îÇ Retrieve Docs ‚îÇ ->  ‚îÇ LLM Answer  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                     ‚îÇ  Vector Store ‚îÇ\n",
    "                     ‚îÇ (Your Data!)  ‚îÇ\n",
    "                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Document Loading** | Importing your private documents (PDFs, text, etc.) |\n",
    "| **Vector Embedding** | Converting text to numerical representations |\n",
    "| **Indexing** | Organizing embeddings for efficient retrieval |\n",
    "| **Querying** | Finding relevant context to answer questions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Creating Sample Documents\n",
    "\n",
    "Let's create some sample documents to demonstrate LlamaIndex capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created: company_policies.txt\n",
      "‚úÖ Created: benefits_guide.txt\n",
      "‚úÖ Created: it_guidelines.txt\n",
      "\n",
      "üìÇ Sample documents created in: sample_documents/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory for sample documents\n",
    "sample_docs_dir = \"sample_documents\"\n",
    "os.makedirs(sample_docs_dir, exist_ok=True)\n",
    "\n",
    "# Create sample company handbook documents\n",
    "documents_content = {\n",
    "    \"company_policies.txt\": \"\"\"\n",
    "TechCorp Employee Handbook - Company Policies\n",
    "\n",
    "VACATION POLICY:\n",
    "- All full-time employees receive 20 days of paid vacation per year.\n",
    "- Vacation days are accrued at a rate of 1.67 days per month.\n",
    "- Unused vacation days can be carried over (maximum of 5 days).\n",
    "- Vacation requests must be submitted at least 2 weeks in advance.\n",
    "\n",
    "REMOTE WORK POLICY:\n",
    "- Employees can work remotely up to 3 days per week.\n",
    "- Remote work days should be coordinated with your team.\n",
    "- A stable internet connection is required for remote work.\n",
    "- Core hours are 10 AM to 3 PM in your local timezone.\n",
    "\n",
    "SICK LEAVE:\n",
    "- 10 paid sick days per year.\n",
    "- For absences longer than 3 days, a doctor's note is required.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"benefits_guide.txt\": \"\"\"\n",
    "TechCorp Employee Handbook - Benefits Guide\n",
    "\n",
    "HEALTH INSURANCE:\n",
    "- Comprehensive medical, dental, and vision coverage.\n",
    "- Company covers 90% of premium costs for employees.\n",
    "- Family plans available with 70% company contribution.\n",
    "- HSA option with $1,500 annual company contribution.\n",
    "\n",
    "RETIREMENT BENEFITS:\n",
    "- 401(k) plan with 6% company match.\n",
    "- Immediate vesting for company contributions.\n",
    "- Access to financial planning services.\n",
    "\n",
    "PROFESSIONAL DEVELOPMENT:\n",
    "- $2,500 annual budget for conferences and courses.\n",
    "- Access to online learning platforms (Coursera, LinkedIn Learning).\n",
    "- Mentorship program available.\n",
    "- Tuition reimbursement up to $5,000/year for approved programs.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"it_guidelines.txt\": \"\"\"\n",
    "TechCorp Employee Handbook - IT Security Guidelines\n",
    "\n",
    "PASSWORD REQUIREMENTS:\n",
    "- Minimum 12 characters with uppercase, lowercase, numbers, and symbols.\n",
    "- Passwords must be changed every 90 days.\n",
    "- Multi-factor authentication (MFA) is mandatory.\n",
    "- Never share passwords with anyone, including IT staff.\n",
    "\n",
    "DATA SECURITY:\n",
    "- All company data must be stored on approved cloud services.\n",
    "- Personal USB drives are prohibited for company data.\n",
    "- Encryption is required for all laptops and mobile devices.\n",
    "- Report suspicious emails to security@techcorp.com.\n",
    "\n",
    "SOFTWARE INSTALLATION:\n",
    "- Only approved software may be installed on company devices.\n",
    "- Submit software requests through the IT portal.\n",
    "- Open source software requires security review.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Write documents to files\n",
    "for filename, content in documents_content.items():\n",
    "    filepath = os.path.join(sample_docs_dir, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"‚úÖ Created: {filename}\")\n",
    "\n",
    "print(f\"\\nüìÇ Sample documents created in: {sample_docs_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building a Document Q&A System <a id=\"6-building-a-document-qa-system\"></a>\n",
    "\n",
    "Now let's build a system that can answer questions about our company documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• Step 1: Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 3 documents:\n",
      "   - benefits_guide.txt\n",
      "   - company_policies.txt\n",
      "   - it_guidelines.txt\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Load all documents from the directory\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=\"./sample_documents\",\n",
    "    recursive=True,  # Include subdirectories\n",
    "    required_exts=[\".txt\"]  # Only load text files\n",
    ")\n",
    "\n",
    "documents = loader.load_data()\n",
    "\n",
    "print(f\"üìÑ Loaded {len(documents)} documents:\")\n",
    "for doc in documents:\n",
    "    print(f\"   - {doc.metadata.get('file_name', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ Step 2: Creating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:10:44,951 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b152fa3b771e42409e8c34af4f21c0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9108761d525a4d34ada2253b6b0ea9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0e830a8ee246b385e8382e6cbeca50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad85d1f8e85d4fb092f6e98f28abf4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af967f93d0094e53839100e1da287be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dda7606ae740cd91fc32d20bac11a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3274ece7cc784716a019efe294a105f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c24a43a34c49909746b2be0454585b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac636474c35e4bc69b33bf5ae9c42738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b02b3c4eeb4e7886c42ba4648b88d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46419326f5a54a7da4088a717c4e2b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:10:47,148 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Creating vector embeddings...\n",
      "‚úÖ Vector index created and saved!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "print(\"üî¢ Creating vector embeddings...\")\n",
    "\n",
    "# Create index from documents\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embedding_model\n",
    ")\n",
    "\n",
    "# Persist the index\n",
    "index.storage_context.persist(persist_dir=\"./index_storage\")\n",
    "\n",
    "print(\"‚úÖ Vector index created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 3: Setting Up the Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:11:19,569 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-01-05 12:11:20,742 - WARNING - Supplied context_window 3900 is greater than the model's max input size 2048. Disable this warning by setting a lower context_window.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Local LLM loaded!\n",
      "‚úÖ Query engine ready!\n",
      "‚ùì What is the company match for 401k?\n",
      "üí° 6% of company contributions\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "llm_llama = HuggingFaceLLM(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\n",
    "        \"dtype\": torch.float16,\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.3,\n",
    "        \"do_sample\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Local LLM loaded!\")\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine(llm=llm_llama)\n",
    "print(\"‚úÖ Query engine ready!\")\n",
    "\n",
    "# Test\n",
    "question = \"What is the company match for 401k?\"\n",
    "response = query_engine.query(question)\n",
    "print(f\"‚ùì {question}\")\n",
    "print(f\"üí° {response.response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Creating Interactive Interfaces with Gradio <a id=\"7-creating-interactive-interfaces-with-gradio\"></a>\n",
    "\n",
    "### ü§ñ Building a Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 12:11:35,368 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_engine = index.as_chat_engine(llm=llm_llama)\n",
    "\n",
    "def chat_response(message, history):\n",
    "    response = chat_engine.chat(message)\n",
    "    return response.response\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_response,\n",
    "    title=\"TechCorp HR Assistant\",\n",
    "    description=\"Ask me about company policies!\"\n",
    ")\n",
    "\n",
    "# # Uncomment To launch: \n",
    "# demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary and Best Practices <a id=\"8-summary-and-best-practices\"></a>\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **LangChain** is great for complex logic and connecting components.\n",
    "2. **LlamaIndex** is specialized for indexing and retrieving from your data.\n",
    "3. **RAG** allows LLMs to answer questions about information they weren't trained on.\n",
    "4. **Gradio** provides a professional-looking UI with very little code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
