{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî§ Tokenization in Hugging Face\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "Tokenization is the **foundational step** in Natural Language Processing (NLP) that converts human-readable text into a format that machine learning models can understand. In this notebook, you'll learn:\n",
    "\n",
    "1. **What is Tokenization?** - Breaking text into smaller, meaningful units\n",
    "2. **Types of Tokenization** - Word-level, Subword-level, and Character-level\n",
    "3. **Hugging Face Tokenizers** - Using AutoTokenizer for various models\n",
    "4. **Understanding Token Components** - input_ids, attention_mask, token_type_ids\n",
    "5. **Practical Dataset Tokenization** - Preparing data for model training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Tokenization Matters\n",
    "\n",
    "| Purpose | Description |\n",
    "|---------|-------------|\n",
    "| **Text Preprocessing** | Simplifies handling of punctuation, casing, and special characters |\n",
    "| **Numerical Representation** | Converts text to token IDs for model consumption |\n",
    "| **Memory Efficiency** | Subword tokenization reduces vocabulary size while maintaining coverage |\n",
    "| **Foundation for NLP Tasks** | Essential for NER, translation, summarization, and more |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if needed)\n",
    "# !pip install transformers datasets torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Part 1: Types of Tokenization Methods\n",
    "\n",
    "There are three main approaches to breaking text into tokens, each with its own strengths and use cases.\n",
    "\n",
    "### 1Ô∏è‚É£ Word-Level Tokenization\n",
    "\n",
    "The simplest approach - splits text by whitespace and punctuation into complete words.\n",
    "\n",
    "**Pros:**\n",
    "- Simple and intuitive\n",
    "- Each token represents a complete word\n",
    "\n",
    "**Cons:**\n",
    "- Struggles with out-of-vocabulary (OOV) words\n",
    "- Requires large vocabulary for diverse languages\n",
    "- Doesn't capture word morphology (prefixes, suffixes)\n",
    "\n",
    "**Models using this:** Word2Vec, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Text: 'Artificial Intelligence is transforming healthcare and education'\n",
      "üî¢ Number of Tokens: 7\n",
      "üìã Tokens: ['Artificial', 'Intelligence', 'is', 'transforming', 'healthcare', 'and', 'education']\n"
     ]
    }
   ],
   "source": [
    "# ‚ú® Example: Simple Word-Level Tokenization\n",
    "\n",
    "def simple_word_tokenize(text):\n",
    "    \"\"\"Basic word-level tokenization using split.\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "# Let's try it with a tech-related sentence\n",
    "sample_text = \"Artificial Intelligence is transforming healthcare and education\"\n",
    "word_tokens = simple_word_tokenize(sample_text)\n",
    "\n",
    "print(f\"üìù Original Text: '{sample_text}'\")\n",
    "print(f\"üî¢ Number of Tokens: {len(word_tokens)}\")\n",
    "print(f\"üìã Tokens: {word_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking tokens against vocabulary:\n",
      "   'artificial': ‚úÖ Known\n",
      "   'intelligence': ‚úÖ Known\n",
      "   'is': ‚úÖ Known\n",
      "   'revolutionizing': ‚ùå OOV (Unknown)\n",
      "   'biotechnology': ‚ùå OOV (Unknown)\n",
      "\n",
      "üí° Words like 'revolutionizing' and 'biotechnology' would be unknown!\n"
     ]
    }
   ],
   "source": [
    "# üö´ Problem with Word-Level: Out-of-Vocabulary Words\n",
    "\n",
    "# Imagine a fixed vocabulary\n",
    "vocabulary = {\"artificial\", \"intelligence\", \"is\", \"good\", \"for\", \"society\"}\n",
    "\n",
    "text = \"Artificial Intelligence is revolutionizing biotechnology\"\n",
    "tokens = simple_word_tokenize(text.lower())\n",
    "\n",
    "print(\"üîç Checking tokens against vocabulary:\")\n",
    "for token in tokens:\n",
    "    status = \"‚úÖ Known\" if token in vocabulary else \"‚ùå OOV (Unknown)\"\n",
    "    print(f\"   '{token}': {status}\")\n",
    "\n",
    "print(\"\\nüí° Words like 'revolutionizing' and 'biotechnology' would be unknown!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Subword-Level Tokenization\n",
    "\n",
    "The **gold standard** for modern NLP! Breaks words into smaller meaningful units.\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Handles OOV words by breaking them into known subwords\n",
    "- ‚úÖ Captures morphology (prefixes, suffixes, roots)\n",
    "- ‚úÖ Compact vocabulary with wide coverage\n",
    "- ‚úÖ Works well across languages\n",
    "\n",
    "**Algorithms:**\n",
    "- **BPE (Byte-Pair Encoding)**: Used by GPT-2, GPT-3, RoBERTa\n",
    "- **WordPiece**: Used by BERT, DistilBERT\n",
    "- **SentencePiece**: Used by T5, ALBERT, XLNet\n",
    "\n",
    "**Models using this:** BERT, GPT, T5, RoBERTa, and most modern transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89c89d2de4a4df8b9560b61ff3a5ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fec2258a1654c26b99d266170dac6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8ce161eeb54e85ba1ad02bc32bff5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f41a81b55640d7bab34147d34fe209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Subword Tokenization Examples:\n",
      "==================================================\n",
      "\n",
      "üìù Word: 'cryptocurrency'\n",
      "   Subwords: ['crypt', '##oc', '##ur', '##ren', '##cy']\n",
      "   Count: 5 tokens\n",
      "\n",
      "üìù Word: 'unbelievable'\n",
      "   Subwords: ['unbelievable']\n",
      "   Count: 1 tokens\n",
      "\n",
      "üìù Word: 'internationalization'\n",
      "   Subwords: ['international', '##ization']\n",
      "   Count: 2 tokens\n",
      "\n",
      "üìù Word: 'neuroscientist'\n",
      "   Subwords: ['ne', '##uro', '##sc', '##ient', '##ist']\n",
      "   Count: 5 tokens\n"
     ]
    }
   ],
   "source": [
    "# ‚ú® Example: Subword Tokenization with BERT\n",
    "\n",
    "# Load BERT's WordPiece tokenizer\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Let's see how it handles complex words\n",
    "complex_words = [\n",
    "    \"cryptocurrency\",\n",
    "    \"unbelievable\",\n",
    "    \"internationalization\",\n",
    "    \"neuroscientist\"\n",
    "]\n",
    "\n",
    "print(\"üî¨ Subword Tokenization Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for word in complex_words:\n",
    "    tokens = bert_tokenizer.tokenize(word)\n",
    "    print(f\"\\nüìù Word: '{word}'\")\n",
    "    print(f\"   Subwords: {tokens}\")\n",
    "    print(f\"   Count: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original: 'The microprocessor revolutionized computing'\n",
      "\n",
      "üîç Tokens: ['the', 'micro', '##pro', '##ces', '##sor', 'revolution', '##ized', 'computing']\n",
      "\n",
      "üìö Explanation:\n",
      "   ‚Ä¢ '##' prefix indicates the token is a CONTINUATION of the previous word\n",
      "   ‚Ä¢ Tokens without '##' are either standalone words or word beginnings\n",
      "   ‚Ä¢ This helps the model understand word boundaries\n"
     ]
    }
   ],
   "source": [
    "# üí° Understanding the '##' Symbol in WordPiece\n",
    "\n",
    "text = \"The microprocessor revolutionized computing\"\n",
    "tokens = bert_tokenizer.tokenize(text)\n",
    "\n",
    "print(f\"üìù Original: '{text}'\")\n",
    "print(f\"\\nüîç Tokens: {tokens}\")\n",
    "\n",
    "print(\"\\nüìö Explanation:\")\n",
    "print(\"   ‚Ä¢ '##' prefix indicates the token is a CONTINUATION of the previous word\")\n",
    "print(\"   ‚Ä¢ Tokens without '##' are either standalone words or word beginnings\")\n",
    "print(\"   ‚Ä¢ This helps the model understand word boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Character-Level Tokenization\n",
    "\n",
    "Breaks text into individual characters. Especially useful for:\n",
    "- Languages without clear word boundaries (Chinese, Japanese, Korean)\n",
    "- Handling typos and misspellings\n",
    "- Very fine-grained text analysis\n",
    "\n",
    "**Pros:**\n",
    "- No OOV problem (limited character set)\n",
    "- Perfect for logographic languages\n",
    "\n",
    "**Cons:**\n",
    "- Very long sequences\n",
    "- Harder for models to learn word meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Character-Level Tokenization:\n",
      "\n",
      "üìù English: 'Hello AI'\n",
      "   Characters: ['H', 'e', 'l', 'l', 'o', ' ', 'A', 'I']\n",
      "   Count: 8 characters\n",
      "\n",
      "üìù Chinese: '‰∫∫Â∑•Êô∫ËÉΩ'\n",
      "   Characters: ['‰∫∫', 'Â∑•', 'Êô∫', 'ËÉΩ']\n",
      "   Count: 4 characters\n",
      "\n",
      "üí° Each Chinese character is a meaningful unit!\n"
     ]
    }
   ],
   "source": [
    "# ‚ú® Example: Character-Level Tokenization\n",
    "\n",
    "def char_tokenize(text):\n",
    "    \"\"\"Character-level tokenization.\"\"\"\n",
    "    return list(text)\n",
    "\n",
    "# English example\n",
    "english_text = \"Hello AI\"\n",
    "english_chars = char_tokenize(english_text)\n",
    "\n",
    "print(\"üî§ Character-Level Tokenization:\")\n",
    "print(f\"\\nüìù English: '{english_text}'\")\n",
    "print(f\"   Characters: {english_chars}\")\n",
    "print(f\"   Count: {len(english_chars)} characters\")\n",
    "\n",
    "# Chinese example (if your system supports it)\n",
    "chinese_text = \"‰∫∫Â∑•Êô∫ËÉΩ\"  # \"Artificial Intelligence\" in Chinese\n",
    "chinese_chars = char_tokenize(chinese_text)\n",
    "\n",
    "print(f\"\\nüìù Chinese: '{chinese_text}'\")\n",
    "print(f\"   Characters: {chinese_chars}\")\n",
    "print(f\"   Count: {len(chinese_chars)} characters\")\n",
    "print(\"\\nüí° Each Chinese character is a meaningful unit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Comparison Summary\n",
    "\n",
    "| Method | Vocabulary Size | OOV Handling | Sequence Length | Best For |\n",
    "|--------|----------------|--------------|-----------------|----------|\n",
    "| Word-Level | Very Large | Poor | Short | Simple tasks, older models |\n",
    "| **Subword** | Medium | **Excellent** | Medium | **Modern Transformers** |\n",
    "| Character | Small | None (No OOV) | Very Long | CJK languages, typo handling |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Part 2: Hugging Face AutoTokenizer\n",
    "\n",
    "The `AutoTokenizer` class automatically loads the correct tokenizer for any pre-trained model. This is the **recommended way** to work with tokenizers in Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tokenizers:\n",
      "   ‚Ä¢ BERT: BertTokenizerFast\n",
      "   ‚Ä¢ GPT-2: GPT2TokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# üîÑ Loading Different Tokenizers\n",
    "\n",
    "# BERT tokenizer (WordPiece)\n",
    "bert_tok = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# GPT-2 tokenizer (BPE)\n",
    "gpt2_tok = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"‚úÖ Loaded tokenizers:\")\n",
    "print(f\"   ‚Ä¢ BERT: {type(bert_tok).__name__}\")\n",
    "print(f\"   ‚Ä¢ GPT-2: {type(gpt2_tok).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Input: 'Machine learning enables computers to learn from data'\n",
      "\n",
      "üî¨ Tokenization Comparison:\n",
      "============================================================\n",
      "\n",
      "ü§ñ BERT (WordPiece):\n",
      "   ['machine', 'learning', 'enables', 'computers', 'to', 'learn', 'from', 'data']\n",
      "   Token count: 8\n",
      "\n",
      "ü§ñ GPT-2 (BPE):\n",
      "   ['Machine', 'ƒ†learning', 'ƒ†enables', 'ƒ†computers', 'ƒ†to', 'ƒ†learn', 'ƒ†from', 'ƒ†data']\n",
      "   Token count: 8\n"
     ]
    }
   ],
   "source": [
    "# üîç Comparing How Different Tokenizers Handle the Same Text\n",
    "\n",
    "sample = \"Machine learning enables computers to learn from data\"\n",
    "\n",
    "print(f\"üìù Input: '{sample}'\\n\")\n",
    "print(\"üî¨ Tokenization Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# BERT\n",
    "bert_tokens = bert_tok.tokenize(sample)\n",
    "print(f\"\\nü§ñ BERT (WordPiece):\")\n",
    "print(f\"   {bert_tokens}\")\n",
    "print(f\"   Token count: {len(bert_tokens)}\")\n",
    "\n",
    "# GPT-2\n",
    "gpt2_tokens = gpt2_tok.tokenize(sample)\n",
    "print(f\"\\nü§ñ GPT-2 (BPE):\")\n",
    "print(f\"   {gpt2_tokens}\")\n",
    "print(f\"   Token count: {len(gpt2_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Part 3: Understanding Tokenizer Outputs\n",
    "\n",
    "When you tokenize text with Hugging Face, you get three important components:\n",
    "\n",
    "1. **`input_ids`** - Numerical IDs representing each token\n",
    "2. **`attention_mask`** - Binary mask showing real tokens (1) vs padding (0)\n",
    "3. **`token_type_ids`** - Segment IDs for sentence pairs (for BERT-like models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Text: 'Deep learning powers modern AI applications'\n",
      "\n",
      "============================================================\n",
      "\n",
      "üì¶ Tokenizer Output:\n",
      "\n",
      "1Ô∏è‚É£ input_ids (Token IDs):\n",
      "   [101, 2784, 4083, 4204, 2715, 9932, 5097, 102, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "2Ô∏è‚É£ attention_mask:\n",
      "   [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "3Ô∏è‚É£ token_type_ids:\n",
      "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# üìä Full Tokenization Example\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Deep learning powers modern AI applications\"\n",
    "\n",
    "# Full tokenization with all outputs\n",
    "encoded = tokenizer(\n",
    "    text,\n",
    "    padding='max_length',    # Pad to max_length\n",
    "    max_length=15,           # Maximum sequence length\n",
    "    truncation=True,         # Truncate if too long\n",
    "    return_tensors=None      # Return Python lists\n",
    ")\n",
    "\n",
    "print(f\"üìù Original Text: '{text}'\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüì¶ Tokenizer Output:\")\n",
    "print(f\"\\n1Ô∏è‚É£ input_ids (Token IDs):\")\n",
    "print(f\"   {encoded['input_ids']}\")\n",
    "print(f\"\\n2Ô∏è‚É£ attention_mask:\")\n",
    "print(f\"   {encoded['attention_mask']}\")\n",
    "print(f\"\\n3Ô∏è‚É£ token_type_ids:\")\n",
    "print(f\"   {encoded['token_type_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Token ID to Token Mapping:\n",
      "\n",
      "Token ID     Token           Attention    Description\n",
      "------------------------------------------------------------\n",
      "101          [CLS]           1            ‚Üê Start token\n",
      "2784         deep            1            \n",
      "4083         learning        1            \n",
      "4204         powers          1            \n",
      "2715         modern          1            \n",
      "9932         ai              1            \n",
      "5097         applications    1            \n",
      "102          [SEP]           1            ‚Üê Separator/End token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n",
      "0            [PAD]           0            ‚Üê Padding token\n"
     ]
    }
   ],
   "source": [
    "# üîç Decoding: Converting Token IDs Back to Text\n",
    "\n",
    "# Get the tokens from IDs\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "\n",
    "print(\"üîÑ Token ID to Token Mapping:\\n\")\n",
    "print(f\"{'Token ID':<12} {'Token':<15} {'Attention':<12} {'Description'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for token_id, token, attn in zip(encoded['input_ids'], tokens, encoded['attention_mask']):\n",
    "    if token == '[CLS]':\n",
    "        desc = \"‚Üê Start token\"\n",
    "    elif token == '[SEP]':\n",
    "        desc = \"‚Üê Separator/End token\"\n",
    "    elif token == '[PAD]':\n",
    "        desc = \"‚Üê Padding token\"\n",
    "    elif token.startswith('##'):\n",
    "        desc = \"‚Üê Subword continuation\"\n",
    "    else:\n",
    "        desc = \"\"\n",
    "    \n",
    "    print(f\"{token_id:<12} {token:<15} {attn:<12} {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Special Tokens in BERT Tokenizer:\n",
      "\n",
      "[CLS] (ID: 101)\n",
      "   ‚îî‚îÄ‚îÄ Classification token - marks the beginning of input\n",
      "\n",
      "[SEP] (ID: 102)\n",
      "   ‚îî‚îÄ‚îÄ Separator token - marks end of sentence or separates sentence pairs\n",
      "\n",
      "[PAD] (ID: 0)\n",
      "   ‚îî‚îÄ‚îÄ Padding token - fills sequences to uniform length\n",
      "\n",
      "[UNK] (ID: 100)\n",
      "   ‚îî‚îÄ‚îÄ Unknown token - represents out-of-vocabulary tokens\n",
      "\n",
      "[MASK] (ID: 103)\n",
      "   ‚îî‚îÄ‚îÄ Mask token - used for masked language modeling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üí° Understanding Special Tokens\n",
    "\n",
    "print(\"üè∑Ô∏è Special Tokens in BERT Tokenizer:\\n\")\n",
    "\n",
    "special_tokens = {\n",
    "    '[CLS]': 'Classification token - marks the beginning of input',\n",
    "    '[SEP]': 'Separator token - marks end of sentence or separates sentence pairs',\n",
    "    '[PAD]': 'Padding token - fills sequences to uniform length',\n",
    "    '[UNK]': 'Unknown token - represents out-of-vocabulary tokens',\n",
    "    '[MASK]': 'Mask token - used for masked language modeling'\n",
    "}\n",
    "\n",
    "for token, description in special_tokens.items():\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"{token} (ID: {token_id})\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ {description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Understanding Attention Mask\n",
    "\n",
    "The attention mask tells the model:\n",
    "- **1** = \"Pay attention to this token\" (real content)\n",
    "- **0** = \"Ignore this token\" (padding)\n",
    "\n",
    "This is crucial for:\n",
    "- Batch processing with variable-length sequences\n",
    "- Preventing the model from learning from padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Attention Mask Comparison:\n",
      "\n",
      "Short text: 'AI is amazing'\n",
      "Tokens:     ['[CLS]', 'ai', 'is', 'amazing', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Attention:  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Real tokens: 5 | Padding: 15\n",
      "\n",
      "Long text: 'Artificial intelligence transforms how we interact with technology daily'\n",
      "Tokens:     ['[CLS]', 'artificial', 'intelligence', 'transforms', 'how', 'we', 'interact', 'with', 'technology', 'daily', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Attention:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Real tokens: 11 | Padding: 9\n"
     ]
    }
   ],
   "source": [
    "# üìä Visualizing Attention Mask\n",
    "\n",
    "short_text = \"AI is amazing\"\n",
    "long_text = \"Artificial intelligence transforms how we interact with technology daily\"\n",
    "\n",
    "# Tokenize both with same max_length\n",
    "short_encoded = tokenizer(short_text, padding='max_length', max_length=20, truncation=True)\n",
    "long_encoded = tokenizer(long_text, padding='max_length', max_length=20, truncation=True)\n",
    "\n",
    "print(\"üìä Attention Mask Comparison:\\n\")\n",
    "\n",
    "print(f\"Short text: '{short_text}'\")\n",
    "print(f\"Tokens:     {tokenizer.convert_ids_to_tokens(short_encoded['input_ids'])}\")\n",
    "print(f\"Attention:  {short_encoded['attention_mask']}\")\n",
    "print(f\"Real tokens: {sum(short_encoded['attention_mask'])} | Padding: {len(short_encoded['attention_mask']) - sum(short_encoded['attention_mask'])}\")\n",
    "\n",
    "print(f\"\\nLong text: '{long_text}'\")\n",
    "print(f\"Tokens:     {tokenizer.convert_ids_to_tokens(long_encoded['input_ids'])}\")\n",
    "print(f\"Attention:  {long_encoded['attention_mask']}\")\n",
    "print(f\"Real tokens: {sum(long_encoded['attention_mask'])} | Padding: {len(long_encoded['attention_mask']) - sum(long_encoded['attention_mask'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Understanding Token Type IDs\n",
    "\n",
    "Token type IDs are used when the input consists of **multiple segments** (e.g., question-answering tasks):\n",
    "- **0** = First segment (e.g., question)\n",
    "- **1** = Second segment (e.g., context/answer)\n",
    "\n",
    "For single-sentence inputs, all token type IDs are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Sentence Pair Tokenization:\n",
      "\n",
      "Question: 'What is machine learning?'\n",
      "Answer: 'Machine learning is a subset of AI that enables computers to learn from data.'\n",
      "\n",
      "Token                Type ID    Segment\n",
      "---------------------------------------------\n",
      "[CLS]                0          Special (Question)\n",
      "what                 0          Question\n",
      "is                   0          Question\n",
      "machine              0          Question\n",
      "learning             0          Question\n",
      "?                    0          Question\n",
      "[SEP]                0          Special (Question)\n",
      "machine              1          Answer\n",
      "learning             1          Answer\n",
      "is                   1          Answer\n",
      "a                    1          Answer\n",
      "subset               1          Answer\n",
      "of                   1          Answer\n",
      "ai                   1          Answer\n",
      "that                 1          Answer\n",
      "enables              1          Answer\n",
      "computers            1          Answer\n",
      "to                   1          Answer\n",
      "learn                1          Answer\n",
      "from                 1          Answer\n",
      "data                 1          Answer\n",
      ".                    1          Answer\n",
      "[SEP]                1          Special (Answer)\n"
     ]
    }
   ],
   "source": [
    "# üîó Token Type IDs with Sentence Pairs\n",
    "\n",
    "question = \"What is machine learning?\"\n",
    "answer = \"Machine learning is a subset of AI that enables computers to learn from data.\"\n",
    "\n",
    "# Tokenize as a sentence pair\n",
    "pair_encoded = tokenizer(\n",
    "    question, \n",
    "    answer, \n",
    "    padding='max_length', \n",
    "    max_length=35,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(pair_encoded['input_ids'])\n",
    "\n",
    "print(\"üîó Sentence Pair Tokenization:\\n\")\n",
    "print(f\"Question: '{question}'\")\n",
    "print(f\"Answer: '{answer}'\\n\")\n",
    "\n",
    "print(f\"{'Token':<20} {'Type ID':<10} {'Segment'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for token, type_id in zip(tokens, pair_encoded['token_type_ids']):\n",
    "    if token == '[PAD]':\n",
    "        continue\n",
    "    segment = \"Question\" if type_id == 0 else \"Answer\"\n",
    "    if token in ['[CLS]', '[SEP]']:\n",
    "        segment = f\"Special ({segment})\"\n",
    "    print(f\"{token:<20} {type_id:<10} {segment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Part 4: Tokenizing Datasets for Training\n",
    "\n",
    "Let's apply everything we've learned to tokenize a real dataset, preparing it for model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60a9f68ddf54ef1bff1589f544b7a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a97c2cd6c414175a4fb02ca6adae398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split/train-00000-of-00001.parquet:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9974738e744f73958e0aa167dde626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split/validation-00000-of-00001.parquet:   0%|          | 0.00/127k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e936ddbf4046c9ad9e705a85bc1426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split/test-00000-of-00001.parquet:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041a2c5ab8ed4ccaa21e6ba72b03ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4673750e9d45446687f1bcc29a312540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98278fad5b594dfeb8d29be2b0f7feeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset Info:\n",
      "   ‚Ä¢ Number of samples: 1000\n",
      "   ‚Ä¢ Features: {'text': Value('string'), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])}\n",
      "\n",
      "üìù Sample entries:\n",
      "   1. 'i didnt feel humiliated...' ‚Üí Label: 0\n",
      "   2. 'i can go from feeling so hopeless to so damned hopeful just ...' ‚Üí Label: 0\n",
      "   3. 'im grabbing a minute to post i feel greedy wrong...' ‚Üí Label: 3\n"
     ]
    }
   ],
   "source": [
    "# üì• Load a Sample Dataset\n",
    "\n",
    "# Using the 'emotion' dataset - classifying text into emotions\n",
    "dataset = load_dataset('dair-ai/emotion', split='train[:1000]')  # First 1000 samples\n",
    "\n",
    "print(\"üìä Dataset Info:\")\n",
    "print(f\"   ‚Ä¢ Number of samples: {len(dataset)}\")\n",
    "print(f\"   ‚Ä¢ Features: {dataset.features}\")\n",
    "print(f\"\\nüìù Sample entries:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {i+1}. '{dataset[i]['text'][:60]}...' ‚Üí Label: {dataset[i]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization function defined!\n",
      "\n",
      "üìã Parameters:\n",
      "   ‚Ä¢ truncation=True: Cut sequences longer than max_length\n",
      "   ‚Ä¢ padding='max_length': Pad shorter sequences\n",
      "   ‚Ä¢ max_length=128: Maximum sequence length\n"
     ]
    }
   ],
   "source": [
    "# üîß Define Tokenization Function\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize a batch of examples.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Tokenization function defined!\")\n",
    "print(\"\\nüìã Parameters:\")\n",
    "print(\"   ‚Ä¢ truncation=True: Cut sequences longer than max_length\")\n",
    "print(\"   ‚Ä¢ padding='max_length': Pad shorter sequences\")\n",
    "print(\"   ‚Ä¢ max_length=128: Maximum sequence length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fad2bcdc09433199c9c26c2b1c8265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset tokenized successfully!\n",
      "\n",
      "üìä Tokenized Dataset Features:\n",
      "   {'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8'))}\n",
      "\n",
      "üìã New columns added:\n",
      "   ‚Ä¢ input_ids: Token IDs\n",
      "   ‚Ä¢ attention_mask: Real token indicators\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° Apply Tokenization to Dataset\n",
    "\n",
    "# Using the map() function for efficient batch processing\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,           # Process in batches for efficiency\n",
    "    remove_columns=['text'] # Remove original text column\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset tokenized successfully!\")\n",
    "print(f\"\\nüìä Tokenized Dataset Features:\")\n",
    "print(f\"   {tokenized_dataset.features}\")\n",
    "print(f\"\\nüìã New columns added:\")\n",
    "print(\"   ‚Ä¢ input_ids: Token IDs\")\n",
    "print(\"   ‚Ä¢ attention_mask: Real token indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Tokenized Sample #0:\n",
      "\n",
      "Label: 0\n",
      "\n",
      "Input IDs (first 30): [101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask (first 30): [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "üîÑ Decoded text: 'i didnt feel humiliated'\n"
     ]
    }
   ],
   "source": [
    "# üîç Examine a Tokenized Sample\n",
    "\n",
    "sample_idx = 0\n",
    "sample = tokenized_dataset[sample_idx]\n",
    "\n",
    "print(f\"üìã Tokenized Sample #{sample_idx}:\\n\")\n",
    "print(f\"Label: {sample['label']}\")\n",
    "print(f\"\\nInput IDs (first 30): {sample['input_ids'][:30]}\")\n",
    "print(f\"Attention Mask (first 30): {sample['attention_mask'][:30]}\")\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"\\nüîÑ Decoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Token Distribution Statistics:\n",
      "\n",
      "   ‚Ä¢ Average tokens per sample: 22.9\n",
      "   ‚Ä¢ Maximum tokens: 70\n",
      "   ‚Ä¢ Minimum tokens: 5\n",
      "   ‚Ä¢ Max length setting: 128\n",
      "\n",
      "üí° Insight: Samples are shorter than max_length - consider reducing max_length for efficiency.\n"
     ]
    }
   ],
   "source": [
    "# üìä Analyze Token Distribution\n",
    "\n",
    "# Count actual tokens (excluding padding)\n",
    "token_counts = [sum(sample['attention_mask']) for sample in tokenized_dataset]\n",
    "\n",
    "avg_tokens = sum(token_counts) / len(token_counts)\n",
    "max_tokens = max(token_counts)\n",
    "min_tokens = min(token_counts)\n",
    "\n",
    "print(\"üìä Token Distribution Statistics:\\n\")\n",
    "print(f\"   ‚Ä¢ Average tokens per sample: {avg_tokens:.1f}\")\n",
    "print(f\"   ‚Ä¢ Maximum tokens: {max_tokens}\")\n",
    "print(f\"   ‚Ä¢ Minimum tokens: {min_tokens}\")\n",
    "print(f\"   ‚Ä¢ Max length setting: 128\")\n",
    "print(f\"\\nüí° Insight: {'Most samples use full length!' if avg_tokens > 100 else 'Samples are shorter than max_length - consider reducing max_length for efficiency.'}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Part 5: Advanced Tokenization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Dynamic Padding Results:\n",
      "\n",
      "Shape of input_ids: torch.Size([3, 14])\n",
      "(batch_size=3, sequence_length=14)\n",
      "\n",
      "üí° Sequence length is determined by the longest text in the batch!\n"
     ]
    }
   ],
   "source": [
    "# üéØ Dynamic Padding (More Efficient)\n",
    "\n",
    "# Instead of padding to max_length, pad to longest in batch\n",
    "texts = [\n",
    "    \"Short text\",\n",
    "    \"This is a medium length sentence\",\n",
    "    \"This is a much longer sentence that contains more words and information\"\n",
    "]\n",
    "\n",
    "# With dynamic padding\n",
    "dynamic_encoded = tokenizer(\n",
    "    texts,\n",
    "    padding=True,  # Pad to longest in batch\n",
    "    truncation=True,\n",
    "    return_tensors='pt'  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"üéØ Dynamic Padding Results:\\n\")\n",
    "print(f\"Shape of input_ids: {dynamic_encoded['input_ids'].shape}\")\n",
    "print(f\"(batch_size=3, sequence_length={dynamic_encoded['input_ids'].shape[1]})\")\n",
    "print(\"\\nüí° Sequence length is determined by the longest text in the batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÄ Batch Sentence Pair Encoding:\n",
      "\n",
      "Input IDs shape: torch.Size([2, 15])\n",
      "   ‚Üí (batch_size=2, sequence_length=15)\n",
      "\n",
      "üìù First Pair:\n",
      "   Premise: 'The weather is sunny today'\n",
      "   Hypothesis: 'It's a beautiful day outside'\n",
      "\n",
      "   Tokens: ['[CLS]', 'the', 'weather', 'is', 'sunny', 'today', '[SEP]', 'it', \"'\", 's', 'a', 'beautiful', 'day', 'outside', '[SEP]']\n",
      "\n",
      "   Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "   ‚Üí 0 = Premise tokens, 1 = Hypothesis tokens\n",
      "\n",
      "üìù Second Pair:\n",
      "   Premise: 'Python is a programming language'\n",
      "   Hypothesis: 'Python is used for web development'\n",
      "\n",
      "   Tokens: ['[CLS]', 'python', 'is', 'a', 'programming', 'language', '[SEP]', 'python', 'is', 'used', 'for', 'web', 'development', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# üîÄ Handling Multiple Text Pairs\n",
    "\n",
    "# Useful for NLI, QA, and sentence similarity tasks\n",
    "premises = [\n",
    "    \"The weather is sunny today\",\n",
    "    \"Python is a programming language\"\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    \"It's a beautiful day outside\",\n",
    "    \"Python is used for web development\"\n",
    "]\n",
    "\n",
    "# Use BERT tokenizer for this example (it returns token_type_ids)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "pair_encoded = bert_tokenizer(\n",
    "    premises,\n",
    "    hypotheses,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"üîÄ Batch Sentence Pair Encoding:\\n\")\n",
    "print(f\"Input IDs shape: {pair_encoded['input_ids'].shape}\")\n",
    "print(f\"   ‚Üí (batch_size={pair_encoded['input_ids'].shape[0]}, sequence_length={pair_encoded['input_ids'].shape[1]})\")\n",
    "\n",
    "# Show first pair details\n",
    "print(f\"\\nüìù First Pair:\")\n",
    "print(f\"   Premise: '{premises[0]}'\")\n",
    "print(f\"   Hypothesis: '{hypotheses[0]}'\")\n",
    "print(f\"\\n   Tokens: {bert_tokenizer.convert_ids_to_tokens(pair_encoded['input_ids'][0].tolist())}\")\n",
    "\n",
    "# Show token_type_ids to distinguish premise from hypothesis\n",
    "print(f\"\\n   Token Type IDs: {pair_encoded['token_type_ids'][0].tolist()}\")\n",
    "print(\"   ‚Üí 0 = Premise tokens, 1 = Hypothesis tokens\")\n",
    "\n",
    "# Show second pair for comparison\n",
    "print(f\"\\nüìù Second Pair:\")\n",
    "print(f\"   Premise: '{premises[1]}'\")\n",
    "print(f\"   Hypothesis: '{hypotheses[1]}'\")\n",
    "print(f\"\\n   Tokens: {bert_tokenizer.convert_ids_to_tokens(pair_encoded['input_ids'][1].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### 1. Tokenization Types\n",
    "- **Word-level**: Simple but limited vocabulary and OOV handling\n",
    "- **Subword-level** (BPE, WordPiece): Best of both worlds - used by modern transformers\n",
    "- **Character-level**: Good for CJK languages and typo handling\n",
    "\n",
    "### 2. Tokenizer Outputs\n",
    "- **`input_ids`**: Numerical token representations\n",
    "- **`attention_mask`**: 1 for real tokens, 0 for padding\n",
    "- **`token_type_ids`**: Segment identifiers for sentence pairs\n",
    "\n",
    "### 3. Special Tokens\n",
    "- **[CLS]**: Start of sequence (classification)\n",
    "- **[SEP]**: Separator/End of sequence\n",
    "- **[PAD]**: Padding for batch processing\n",
    "- **[MASK]**: For masked language modeling\n",
    "\n",
    "### 4. Best Practices\n",
    "- Use `AutoTokenizer` for model-compatible tokenization\n",
    "- Apply `truncation=True` and set appropriate `max_length`\n",
    "- Use `batched=True` with `dataset.map()` for efficiency\n",
    "- Consider dynamic padding for variable-length sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/tokenizers/)\n",
    "- [Understanding WordPiece](https://huggingface.co/learn/nlp-course/chapter6/6)\n",
    "- [BPE Algorithm Explained](https://huggingface.co/learn/nlp-course/chapter6/5)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
