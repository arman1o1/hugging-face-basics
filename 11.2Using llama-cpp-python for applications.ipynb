{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Chapter 11.2: Using llama.cpp for Applications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**llama.cpp** is the foundational C/C++ library that powers many local LLM applications. The `llama-cpp-python` package provides Python bindings that enable developers to run Large Language Models locally with maximum efficiency.\n",
    "\n",
    "### Why Use llama-cpp-python?\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Performance** | Highly optimized C++ backend with CPU/GPU support |\n",
    "| **OpenAI Compatible** | Returns responses in OpenAI API format |\n",
    "| **Flexibility** | Fine-grained control over model parameters |\n",
    "| **GGUF Format** | Native support for quantized models |\n",
    "| **Multi-Modal** | Supports vision models (LLaVA, etc.) |\n",
    "| **GPU Acceleration** | CUDA, Metal, ROCm, and Vulkan support |\n",
    "\n",
    "### What We'll Cover\n",
    "\n",
    "1. üõ†Ô∏è Installation and Setup\n",
    "2. üöÄ Loading Models\n",
    "3. üí¨ Text Completion vs Chat Completion\n",
    "4. üéõÔ∏è Generation Parameters\n",
    "5. üìä Streaming Responses\n",
    "6. üß© Text Embeddings\n",
    "7. üèóÔ∏è Building Practical Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üõ†Ô∏è Installation and Setup\n",
    "\n",
    "Installing `llama-cpp-python` can be done via pip. For optimal performance, you may want to compile with specific backend support.\n",
    "\n",
    "### Basic Installation (CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Basic CPU installation\n",
    "# !pip install llama-cpp-python -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with GPU Support\n",
    "\n",
    "For GPU acceleration, you need to compile with specific flags:\n",
    "\n",
    "```bash\n",
    "# NVIDIA CUDA\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir\n",
    "\n",
    "# Apple Metal (macOS)\n",
    "CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir\n",
    "\n",
    "# AMD ROCm\n",
    "CMAKE_ARGS=\"-DGGML_HIPBLAS=on\" pip install llama-cpp-python --force-reinstall --no-cache-dir\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-cpp-python version: 0.3.16\n",
      "‚úÖ llama-cpp-python imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Verify installation\n",
    "import llama_cpp\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(f\"llama-cpp-python version: {version('llama-cpp-python')}\")\n",
    "print(\"‚úÖ llama-cpp-python imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üöÄ Loading Models\n",
    "\n",
    "llama-cpp-python uses GGUF format models. You can load models from:\n",
    "- Local file path\n",
    "- Hugging Face Hub (automatic download)\n",
    "\n",
    "### Available Models on Hugging Face\n",
    "\n",
    "| Model | Size | Description |\n",
    "|-------|------|-------------|\n",
    "| `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` | ~4GB | High-quality Mistral model |\n",
    "| `TheBloke/Llama-2-7B-Chat-GGUF` | ~4GB | Meta's Llama 2 Chat |\n",
    "| `microsoft/Phi-3-mini-4k-instruct-gguf` | ~2GB | Microsoft's efficient Phi-3 |\n",
    "| `TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF` | ~600MB | Tiny but capable model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Hugging Face Hub...\n",
      "This may take a few minutes on first run...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9457e728f8814fda8e57fac86ccb590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./mistral-7b-instruct-v0.2.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Download and load model from Hugging Face Hub\n",
    "# This will download the model on first run (~4GB)\n",
    "print(\"Loading model from Hugging Face Hub...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "    filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    n_ctx=4096,  # Mistral supports 32k context, use what you need\n",
    "    n_threads=4,  # Number of CPU threads\n",
    "    verbose=False  # Set to True for debug info\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from Local Path\n",
    "\n",
    "If you have a GGUF model file locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° Key initialization parameters:\n",
      "   - model_path: Path to GGUF model file\n",
      "   - n_ctx: Context window size (tokens)\n",
      "   - n_gpu_layers: Number of layers to offload to GPU (-1 for all)\n",
      "   - n_threads: CPU threads for computation\n",
      "   - seed: Random seed for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading from local path (commented to avoid errors if file doesn't exist)\n",
    "# llm = Llama(\n",
    "#     model_path=\"./models/my-model.gguf\",\n",
    "#     n_ctx=2048,          # Context window\n",
    "#     n_gpu_layers=-1,     # Use all GPU layers (if GPU available)\n",
    "#     n_threads=4,         # CPU threads for generation\n",
    "#     seed=42,             # For reproducibility\n",
    "#     verbose=False\n",
    "# )\n",
    "\n",
    "print(\"üí° Key initialization parameters:\")\n",
    "print(\"   - model_path: Path to GGUF model file\")\n",
    "print(\"   - n_ctx: Context window size (tokens)\")\n",
    "print(\"   - n_gpu_layers: Number of layers to offload to GPU (-1 for all)\")\n",
    "print(\"   - n_threads: CPU threads for computation\")\n",
    "print(\"   - seed: Random seed for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üí¨ Text Completion vs Chat Completion\n",
    "\n",
    "llama-cpp-python provides two main ways to generate text:\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| `__call__()` / `create_completion()` | Raw text completion | Text continuation, code completion |\n",
    "| `create_chat_completion()` | Chat-style with messages | Chatbots, Q&A systems |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text Completion\n",
    "\n",
    "Text completion treats your input as a prompt to continue. Returns OpenAI-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Text Completion Demo\n",
      "\n",
      "==================================================\n",
      "Prompt: The benefits of exercise include\n",
      "\n",
      "Completion:  weight loss, improved cardiovascular health, better mood, and increased energy levels. However, many people struggle to find the motivation to get started or to stick with an exercise routine. Here are some tips to help you get moving and make exercise a regular part of your life:\n",
      "\n",
      "üìä Token usage: {'prompt_tokens': 6, 'completion_tokens': 58, 'total_tokens': 64}\n"
     ]
    }
   ],
   "source": [
    "# Text completion example\n",
    "print(\"üìù Text Completion Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = \"The benefits of exercise include\"\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\\n\"],  # Stop at double newline\n",
    "    echo=False      # Don't include prompt in output\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Completion: {output['choices'][0]['text']}\")\n",
    "print(f\"\\nüìä Token usage: {output['usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Chat Completion\n",
    "\n",
    "Chat completion uses a list of messages with roles (system, user, assistant) for conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat Completion Demo\n",
      "\n",
      "==================================================\n",
      "User: How do I make scrambled eggs?\n",
      "\n",
      "Assistant:  Making scrambled eggs is a simple and quick process. Here's a step-by-step guide:\n",
      "\n",
      "Ingredients:\n",
      "- 2 to 4 eggs\n",
      "- Salt, to taste\n",
      "- Pepper, to taste\n",
      "- 2 tablespoons of butter or oil\n",
      "- Optional: milk or cream, shredded cheese, herbs, or vegetables\n",
      "\n",
      "Instructions:\n",
      "1. Crack the eggs into a bowl. Add a pinch of salt and pepper to taste. If desired, you can also add milk or cream to make the eggs softer.\n",
      "2. Whisk the eggs with a fork until the yolks and whites are fully combined. Set aside.\n",
      "3. Heat a non-stick frying pan over medium-low heat. Add the butter or oil and let it melt.\n",
      "4. Pour the beaten eggs into the pan. Let them cook undisturbed for a few seconds until the edges start\n"
     ]
    }
   ],
   "source": [
    "# Chat completion example\n",
    "print(\"üí¨ Chat Completion Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful cooking assistant. Give concise recipes.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I make scrambled eggs?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"User: {messages[1]['content']}\\n\")\n",
    "print(f\"Assistant: {response['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Multi-Turn Conversation Demo\n",
      "\n",
      "==================================================\n",
      "User: What is the Pythagorean theorem?\n",
      "\n",
      "Assistant:  The Pythagorean theorem is a mathematical relationship between the sides of a right-angled triangle. It states that the square of the length of the hypotenuse (the side opposite the right angle) is equal to the sum of the squares of the lengths of the other two sides. In mathematical notation, if a and b are the lengths of the legs (the two shorter sides), and c is the length of the hypotenuse, then the theorem can be written as:\n",
      "\n",
      "a¬≤ + b¬≤ = c¬≤\n",
      "\n",
      "This theorem has been known since ancient times and is named after the Greek mathematician Pythagoras, who is credited with its discovery. It is one of the\n",
      "\n",
      "--------------------------------------------------\n",
      "User: Can you give me an example with numbers?\n",
      "\n",
      "Assistant:  Certainly! Let's consider a right-angled triangle with legs of length 3 and 4 units. We can use the Pythagorean theorem to find the length of the hypotenuse, c:\n",
      "\n",
      "a¬≤ = 3¬≤ = 9\n",
      "b¬≤ = 4¬≤ = 16\n",
      "\n",
      "Now, we can find the length of the hypotenuse by adding the squares of the lengths of the legs and taking the square root of the result:\n",
      "\n",
      "c¬≤ = a¬≤ + b¬≤\n",
      "c¬≤ = 9 + 16\n",
      "c¬≤ = 25\n",
      "\n",
      "c = ‚àö25 = 5\n",
      "\n",
      "So, in this example, the\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversation\n",
    "print(\"üîÑ Multi-Turn Conversation Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the Pythagorean theorem?\"},\n",
    "]\n",
    "\n",
    "# First response\n",
    "response1 = llm.create_chat_completion(messages=conversation, max_tokens=150)\n",
    "assistant_reply = response1['choices'][0]['message']['content']\n",
    "print(f\"User: What is the Pythagorean theorem?\\n\")\n",
    "print(f\"Assistant: {assistant_reply}\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Add assistant response to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you give me an example with numbers?\"})\n",
    "\n",
    "# Follow-up\n",
    "response2 = llm.create_chat_completion(messages=conversation, max_tokens=150)\n",
    "print(f\"User: Can you give me an example with numbers?\\n\")\n",
    "print(f\"Assistant: {response2['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üéõÔ∏è Generation Parameters\n",
    "\n",
    "Control the model's output with these key parameters:\n",
    "\n",
    "| Parameter | Description | Default | Range |\n",
    "|-----------|-------------|---------|---------|\n",
    "| `max_tokens` | Maximum tokens to generate | 16 | 1-n_ctx |\n",
    "| `temperature` | Randomness (higher = more creative) | 0.8 | 0.0-2.0 |\n",
    "| `top_k` | Consider top k tokens | 40 | 1-100 |\n",
    "| `top_p` | Nucleus sampling threshold | 0.95 | 0.0-1.0 |\n",
    "| `repeat_penalty` | Penalize repeated tokens | 1.1 | 1.0-2.0 |\n",
    "| `stop` | Stop sequences | None | list of strings |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå°Ô∏è Temperature Comparison Demo\n",
      "\n",
      "==================================================\n",
      "Low Temperature (0.1) - Focused:\n",
      "   \"Sip. Savor. Connect: Unleash the Power of a Single Cup at Our Cozy Coffee Sanctuary\" \n",
      "\n",
      "\n",
      "\n",
      "Medium Temperature (0.7) - Balanced:\n",
      "   \"Savor the Moment: Where Every Sip is a Story, Every Brew is a Masterpiece, and Every Visit Becomes a Treas\n",
      "\n",
      "High Temperature (1.2) - Creative:\n",
      "   \"Wake Up, Sip In, and Unravel: Where Every Sip Tells a Story at Our Cozy Coffee Sanctuary\n"
     ]
    }
   ],
   "source": [
    "# Temperature comparison\n",
    "print(\"üå°Ô∏è Temperature Comparison Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = \"Write a creative tagline for a coffee shop:\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# Low temperature - more focused\n",
    "response_low = llm.create_chat_completion(\n",
    "    messages=messages, max_tokens=30, temperature=0.1\n",
    ")\n",
    "print(f\"Low Temperature (0.1) - Focused:\")\n",
    "print(f\"  {response_low['choices'][0]['message']['content']}\\n\")\n",
    "\n",
    "# Medium temperature\n",
    "response_med = llm.create_chat_completion(\n",
    "    messages=messages, max_tokens=30, temperature=0.7\n",
    ")\n",
    "print(f\"Medium Temperature (0.7) - Balanced:\")\n",
    "print(f\"  {response_med['choices'][0]['message']['content']}\\n\")\n",
    "\n",
    "# High temperature - more creative\n",
    "response_high = llm.create_chat_completion(\n",
    "    messages=messages, max_tokens=30, temperature=1.2\n",
    ")\n",
    "print(f\"High Temperature (1.2) - Creative:\")\n",
    "print(f\"  {response_high['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Stop Sequences Demo\n",
      "\n",
      "==================================================\n",
      "List the first 3 planets:\n",
      "1. Mercury\n",
      "2. Venus\n",
      "3. Earth\n",
      "These are the first three planets from the Sun in our solar system. Mercury is the closest planet to the Sun, and Venus is the second closest. Earth is the third planet from the Sun.\n"
     ]
    }
   ],
   "source": [
    "# Using stop sequences\n",
    "print(\"üõë Stop Sequences Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate a list and stop when numbering ends\n",
    "output = llm(\n",
    "    \"List the first 3 planets:\\n1.\",\n",
    "    max_tokens=100,\n",
    "    stop=[\"4.\", \"\\n\\n\"],  # Stop before 4th item or double newline\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìä Streaming Responses\n",
    "\n",
    "Streaming allows you to receive tokens as they are generated, providing a better user experience for longer responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming Text Completion Demo\n",
      "\n",
      "==================================================\n",
      "Generating: \n",
      "\n",
      "Quantum computing is a type of computing that uses quantum bits, or qubits, instead of classical bits to process information. Qubits can exist in a superposition of states, meaning they can represent multiple values at once, allowing for exponentially greater computational power compared to classical computers. Quantum algorithms, such as Shor's algorithm for factorization and Grover's algorithm for searching unsorted databases, can solve problems that are intractable for classical computers. However, quantum computing is still in its infancy, and building and maintaining stable qubits is a significant challenge. Nonetheless, the potential applications for quantum computers in fields such as cryptography, optimization, and machine learning make it a promising area\n",
      "\n",
      "‚úÖ Generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming text completion\n",
    "print(\"üåä Streaming Text Completion Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Generating: \", end=\"\")\n",
    "\n",
    "stream = llm(\n",
    "    \"Explain quantum computing in one paragraph:\",\n",
    "    max_tokens=150,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    text = chunk['choices'][0]['text']\n",
    "    print(text, end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Streaming Chat Completion Demo\n",
      "\n",
      "==================================================\n",
      "Assistant:  Why did the Java developer quit his job? Because he didn't feel appreciated. But seriously, why did he quit? Because there was no 'app-reciation'! ü§™ #Java #ProgrammingJokes #Appreciation #AppreciationDay\n",
      "\n",
      "‚úÖ Generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "print(\"üåä Streaming Chat Completion Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Assistant: \", end=\"\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a short joke about programming.\"}\n",
    "]\n",
    "\n",
    "stream = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=100,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'content' in delta:\n",
    "        print(delta['content'], end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Early Stopping with Streaming Demo\n",
      "\n",
      "==================================================\n",
      "Generating (max 200 chars):  a robot who, upon completing a task, gains sentience and realizes he is alive.\n",
      "\n",
      "Once upon a time in the not-so-distant future, there was a research facility nestled deep in the heart of the Siberian wild... [STOPPED]\n",
      "\n",
      "\n",
      "üìä Total characters collected: 204\n"
     ]
    }
   ],
   "source": [
    "# Early stopping with streaming\n",
    "print(\"üõë Early Stopping with Streaming Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "collected_text = \"\"\n",
    "max_chars = 200\n",
    "\n",
    "print(f\"Generating (max {max_chars} chars): \", end=\"\")\n",
    "\n",
    "stream = llm(\n",
    "    \"Write a story about a robot:\",\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    text = chunk['choices'][0]['text']\n",
    "    collected_text += text\n",
    "    print(text, end='', flush=True)\n",
    "    \n",
    "    if len(collected_text) >= max_chars:\n",
    "        print(\"... [STOPPED]\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n\\nüìä Total characters collected: {len(collected_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üß© Text Embeddings\n",
    "\n",
    "llama-cpp-python can also generate text embeddings for semantic search and RAG applications.\n",
    "\n",
    "> **Note:** You must initialize the model with `embedding=True` to use embedding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93349ba31af4b84849bd039f74a4c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "./nomic-embed-text-v1.5.Q4_K_M.gguf:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a small, purpose-built embedding model\n",
    "embed_llm = Llama.from_pretrained(\n",
    "    repo_id=\"nomic-ai/nomic-embed-text-v1.5-GGUF\",\n",
    "    filename=\"nomic-embed-text-v1.5.Q4_K_M.gguf\",\n",
    "    embedding=True,\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")\n",
    "# Only ~100MB and produces 768-dim embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Text Embedding Demo\n",
      "\n",
      "==================================================\n",
      "Number of texts: 3\n",
      "Embedding dimension: 768\n",
      "\n",
      "First 5 values: [0.4224206805229187, 1.4838993549346924, -3.341722011566162, -0.27156567573547363, 0.9094995260238647]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"üìä Text Embedding Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"AI and ML are transforming technology industries.\",\n",
    "    \"The weather is sunny today.\"\n",
    "]\n",
    "\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    result = embed_llm.create_embedding(text)\n",
    "    embeddings.append(result['data'][0]['embedding'])\n",
    "\n",
    "print(f\"Number of texts: {len(texts)}\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"\\nFirst 5 values: {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Semantic Similarity Demo\n",
      "\n",
      "==================================================\n",
      "Text pairs and their similarities:\n",
      "\n",
      "1. 'Machine learning is a subset of artifici...'\n",
      "2. 'AI and ML are transforming technology in...'\n",
      "3. 'The weather is sunny today....'\n",
      "\n",
      "Similarity (1, 2): 0.7631 ‚úÖ Related!\n",
      "Similarity (1, 3): 0.4964 \n",
      "Similarity (2, 3): 0.5067 \n"
     ]
    }
   ],
   "source": [
    "# Semantic similarity\n",
    "print(\"üîç Semantic Similarity Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sim_1_2 = cosine_similarity(embeddings[0], embeddings[1])\n",
    "sim_1_3 = cosine_similarity(embeddings[0], embeddings[2])\n",
    "sim_2_3 = cosine_similarity(embeddings[1], embeddings[2])\n",
    "\n",
    "print(\"Text pairs and their similarities:\\n\")\n",
    "print(f\"1. '{texts[0][:40]}...'\")\n",
    "print(f\"2. '{texts[1][:40]}...'\")\n",
    "print(f\"3. '{texts[2][:40]}...'\\n\")\n",
    "\n",
    "print(f\"Similarity (1, 2): {sim_1_2:.4f} {'‚úÖ Related!' if sim_1_2 > 0.5 else ''}\")\n",
    "print(f\"Similarity (1, 3): {sim_1_3:.4f} {'‚ùå Different' if sim_1_3 < 0.3 else ''}\")\n",
    "print(f\"Similarity (2, 3): {sim_2_3:.4f} {'‚ùå Different' if sim_2_3 < 0.3 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üèóÔ∏è Building Practical Applications\n",
    "\n",
    "Let's build some practical applications using llama-cpp-python!\n",
    "\n",
    "### Application 1: Conversational Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Conversational Assistant Demo\n",
      "\n",
      "==================================================\n",
      "User: What is a Python list?\n",
      "\n",
      "Assistant:  A Python list is a collection of ordered and mutable items. In other words, a list is a data structure that can hold a sequence of values of any data type, including integers, floats, strings, lists, tuples, and dictionaries. The order of the items in a list is important, and the items can be changed or modified after the list has been created.\n",
      "\n",
      "Here's an example of creating a list in Python:\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "colors = ['red', 'green', 'blue']\n",
      "mixed = ['apple', 2, 'banana', 4.5, 'orange']\n",
      "```\n",
      "\n",
      "You can access the items in a list by their index. The first item has index 0, the second item has index 1, and so on. For example, to access the third item in the\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "User: How do I add items to it?\n",
      "\n",
      "Assistant:  To add an item to a list in Python, you can use the `append()` method or the `extend()` method, depending on whether you want to add a single item or a list of items, respectively.\n",
      "\n",
      "Here's an example of adding a single item using the `append()` method:\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3]\n",
      "my_list.append(4)\n",
      "print(my_list) # Output: [1, 2, 3, 4]\n",
      "```\n",
      "\n",
      "And here's an example of adding multiple items using the `extend()` method:\n",
      "\n",
      "```python\n",
      "my_list = [1, 2, 3]\n",
      "my_list.extend([4, 5])\n",
      "print(my_list) # Output: [1, 2, 3, 4, 5]\n",
      "```\n",
      "\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "class ConversationalAssistant:\n",
    "    \"\"\"\n",
    "    A conversational AI assistant with memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, system_prompt=\"You are a helpful assistant.\"):\n",
    "        self.llm = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "    \n",
    "    def chat(self, user_message, max_tokens=200, stream=False):\n",
    "        \"\"\"\n",
    "        Send a message and get a response.\n",
    "        \n",
    "        Args:\n",
    "            user_message: The user's message\n",
    "            max_tokens: Maximum response length\n",
    "            stream: Whether to stream the response\n",
    "            \n",
    "        Returns:\n",
    "            The assistant's response\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        if stream:\n",
    "            # Streaming response\n",
    "            response_stream = self.llm.create_chat_completion(\n",
    "                messages=self.conversation_history,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            full_response = \"\"\n",
    "            for chunk in response_stream:\n",
    "                delta = chunk['choices'][0]['delta']\n",
    "                if 'content' in delta:\n",
    "                    full_response += delta['content']\n",
    "                    print(delta['content'], end='', flush=True)\n",
    "            print()  # Newline after streaming\n",
    "            \n",
    "            response = full_response\n",
    "        else:\n",
    "            # Non-streaming response\n",
    "            result = self.llm.create_chat_completion(\n",
    "                messages=self.conversation_history,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            response = result['choices'][0]['message']['content']\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Reset conversation history.\"\"\"\n",
    "        self.conversation_history = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ]\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "# Demo\n",
    "print(\"ü§ñ Conversational Assistant Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "assistant = ConversationalAssistant(\n",
    "    llm,\n",
    "    system_prompt=\"You are a helpful coding tutor. Give concise explanations.\"\n",
    ")\n",
    "\n",
    "# First message\n",
    "print(\"User: What is a Python list?\\n\")\n",
    "print(\"Assistant: \", end=\"\")\n",
    "response1 = assistant.chat(\"What is a Python list?\", stream=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "# Follow-up (model remembers context)\n",
    "print(\"\\nUser: How do I add items to it?\\n\")\n",
    "print(\"Assistant: \", end=\"\")\n",
    "response2 = assistant.chat(\"How do I add items to it?\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Simple RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Simple RAG Demo\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
      "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added: 'Python History'\n",
      "‚úÖ Added: 'JavaScript History'\n",
      "‚úÖ Added: 'ML Definition'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Q: Who created Python and when?\n",
      "\n",
      "A:  Python was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "üìñ Sources: ['Python History', 'JavaScript History']\n"
     ]
    }
   ],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"\n",
    "    A simple Retrieval-Augmented Generation system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm, embed_llm):\n",
    "        self.llm = llm\n",
    "        self.embed_llm = embed_llm\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text, title=\"Untitled\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        result = self.embed_llm.create_embedding(text)\n",
    "        embedding = result['data'][0]['embedding']\n",
    "        \n",
    "        self.documents.append({\"title\": title, \"text\": text})\n",
    "        self.embeddings.append(embedding)\n",
    "        print(f\"‚úÖ Added: '{title}'\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=2):\n",
    "        \"\"\"Retrieve most relevant documents for a query.\"\"\"\n",
    "        query_result = self.embed_llm.create_embedding(query)\n",
    "        query_embedding = query_result['data'][0]['embedding']\n",
    "        \n",
    "        similarities = [\n",
    "            cosine_similarity(query_embedding, emb)\n",
    "            for emb in self.embeddings\n",
    "        ]\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {**self.documents[i], \"score\": similarities[i]}\n",
    "            for i in top_indices\n",
    "        ]\n",
    "    \n",
    "    def answer(self, question, max_tokens=200):\n",
    "        \"\"\"Answer a question using retrieved documents.\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        relevant = self.retrieve(question)\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[{doc['title']}]: {doc['text']}\"\n",
    "            for doc in relevant\n",
    "        )\n",
    "        \n",
    "        # Create prompt\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Answer questions based only on the provided context. Be concise.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        return response['choices'][0]['message']['content'], relevant\n",
    "\n",
    "# Demo\n",
    "print(\"üìö Simple RAG Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "rag = SimpleRAG(llm, embed_llm)\n",
    "\n",
    "# Add documents\n",
    "rag.add_document(\n",
    "    \"Python was created by Guido van Rossum and first released in 1991. \"\n",
    "    \"It emphasizes code readability and simplicity.\",\n",
    "    title=\"Python History\"\n",
    ")\n",
    "\n",
    "rag.add_document(\n",
    "    \"JavaScript was created by Brendan Eich in 1995 for Netscape Navigator. \"\n",
    "    \"It is the primary language for web browsers.\",\n",
    "    title=\"JavaScript History\"\n",
    ")\n",
    "\n",
    "rag.add_document(\n",
    "    \"Machine learning is a subset of AI that enables computers to learn \"\n",
    "    \"from data without being explicitly programmed.\",\n",
    "    title=\"ML Definition\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Ask question\n",
    "question = \"Who created Python and when?\"\n",
    "print(f\"\\nQ: {question}\\n\")\n",
    "\n",
    "answer, sources = rag.answer(question)\n",
    "print(f\"A: {answer}\\n\")\n",
    "print(f\"üìñ Sources: {[s['title'] for s in sources]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 3: Code Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Code Explainer Demo\n",
      "\n",
      "==================================================\n",
      "Code:\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Explanation:\n",
      " This Python code defines a function named `fibonacci` that takes an integer `n` as an argument. The function is designed to calculate the `n`th number in the Fibonacci sequence.\n",
      "\n",
      "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, starting from 0 and 1. So the sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, and so on.\n",
      "\n",
      "The function starts with an `if` statement that checks if the input `n` is less than or equal to 1. If it is, the function simply returns the value of `n` because the first two numbers in the Fibonacci sequence are 0 and 1.\n",
      "\n",
      "If the input `n` is greater than 1, the function calls itself recursively twice: once with the argument `n-1` and once with the argument `n-2`. The function then returns the sum of the results of these two recursive calls. This is how the function calculates the `n`th number in the Fibonacci sequence.\n",
      "\n",
      "For example, if you call the function with an argument of 5, the function will first check if 5 is less than or equal to 1. Since it's not, the function will then\n"
     ]
    }
   ],
   "source": [
    "class CodeExplainer:\n",
    "    \"\"\"\n",
    "    A tool to explain code snippets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def explain(self, code, language=\"python\", detail_level=\"simple\"):\n",
    "        \"\"\"\n",
    "        Explain a code snippet.\n",
    "        \n",
    "        Args:\n",
    "            code: The code to explain\n",
    "            language: Programming language\n",
    "            detail_level: 'simple', 'detailed', or 'line-by-line'\n",
    "            \n",
    "        Returns:\n",
    "            Explanation of the code\n",
    "        \"\"\"\n",
    "        instructions = {\n",
    "            \"simple\": \"Provide a brief, high-level explanation.\",\n",
    "            \"detailed\": \"Provide a comprehensive explanation covering all aspects.\",\n",
    "            \"line-by-line\": \"Explain each line of the code.\"\n",
    "        }\n",
    "        \n",
    "        instruction = instructions.get(detail_level, instructions[\"simple\"])\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a {language} expert. {instruction}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Explain this {language} code:\\n\\n```{language}\\n{code}\\n```\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "# Demo\n",
    "print(\"üíª Code Explainer Demo\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "explainer = CodeExplainer(llm)\n",
    "\n",
    "code = '''def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)'''\n",
    "\n",
    "print(f\"Code:\\n{code}\\n\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nExplanation:\")\n",
    "explanation = explainer.explain(code, detail_level=\"simple\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üí° Best Practices and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üí° Best Practices and Tips\n",
    "\n",
    "### üß† Memory Management\n",
    "\n",
    "| Practice | Description |\n",
    "|----------|-------------|\n",
    "| Load once, reuse | Load models once and reuse for multiple queries |\n",
    "| Clean up | Use `del model` to free memory when done |\n",
    "| Quantization | Choose appropriate level (`Q4_K_M` is a good balance) |\n",
    "\n",
    "### ‚ö° Performance Optimization\n",
    "\n",
    "| Setting | Recommendation |\n",
    "|---------|---------------|\n",
    "| `n_gpu_layers` | Set to `-1` for full GPU acceleration |\n",
    "| `n_threads` | Match to your CPU core count |\n",
    "| `n_ctx` | Smaller values use less memory but limit context |\n",
    "\n",
    "### üéØ Output Quality\n",
    "\n",
    "| Task Type | Temperature | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| Factual | 0.1 - 0.4 | Q&A, coding, data extraction |\n",
    "| Balanced | 0.5 - 0.7 | General conversation |\n",
    "| Creative | 0.8 - 1.2 | Storytelling, brainstorming |\n",
    "\n",
    "> **Tip:** Always use appropriate `stop` sequences to prevent unwanted generation.\n",
    "\n",
    "### üîß Debugging Tips\n",
    "\n",
    "- Set `verbose=True` to see model info and selected chat format\n",
    "- Check `finish_reason` in responses (`stop`, `length`, etc.)\n",
    "- Monitor `usage` field for token counts\n",
    "- Use `echo=True` in completions to see the full prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up...\n",
      "‚úÖ Models unloaded!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "print(\"üßπ Cleaning up...\")\n",
    "del llm\n",
    "del embed_llm\n",
    "print(\"‚úÖ Models unloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **llama-cpp-python** provides Python bindings for the efficient llama.cpp C++ library\n",
    "2. **GGUF format** enables quantized models for efficient local inference\n",
    "3. **Text completion** vs **Chat completion** serve different use cases\n",
    "4. **Streaming** provides real-time token generation\n",
    "5. **Embeddings** enable semantic search and RAG applications\n",
    "\n",
    "### Applications Built\n",
    "\n",
    "- ü§ñ Conversational Assistant with memory\n",
    "- üìö Simple RAG System with document retrieval\n",
    "- üíª Code Explainer with customizable detail levels\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try larger models for better quality\n",
    "- Enable GPU acceleration for faster inference\n",
    "- Explore function calling for tool use\n",
    "- Build an OpenAI-compatible API server\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [llama-cpp-python Documentation](https://llama-cpp-python.readthedocs.io/)\n",
    "- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)\n",
    "- [Hugging Face GGUF Models](https://huggingface.co/models?library=gguf)\n",
    "- [GGUF Format Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
