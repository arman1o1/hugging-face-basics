{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ó Chapter 1: Introduction to Hugging Face\n",
        "\n",
        "Welcome to this comprehensive tutorial on **Hugging Face** - the central hub for open-source AI! This notebook will guide you through the fundamental concepts and tools that make Hugging Face the go-to platform for machine learning practitioners.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö What You'll Learn\n",
        "\n",
        "| Section | Topic | Description |\n",
        "|---------|-------|-------------|\n",
        "| 1 | **Hugging Face Ecosystem** | Understanding the platform and its philosophy |\n",
        "| 2 | **Transformers Library** | The powerful Python package for ML models |\n",
        "| 3 | **Pipeline API** | Easy-to-use interface for common ML tasks |\n",
        "| 4 | **Model Hub** | Exploring and using pre-trained models |\n",
        "| 5 | **Inference API** | Testing models via HTTP requests |\n",
        "| 6 | **Gradio** | Building web interfaces for ML models |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Environment Setup\n",
        "\n",
        "First, let's install the necessary libraries. Uncomment and run the cell below if you haven't installed them yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install required packages\n",
        "# %pip install transformers torch accelerate gradio requests pillow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üñ•Ô∏è Using device: GPU (CUDA)\n",
            "ü§ó Transformers library ready!\n"
          ]
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core libraries\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"üñ•Ô∏è Using device: {'GPU (CUDA)' if device == 0 else 'CPU'}\")\n",
        "print(f\"ü§ó Transformers library ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ The Hugging Face Ecosystem\n",
        "\n",
        "### What is Hugging Face?\n",
        "\n",
        "**Hugging Face** is a company and community that has become the central hub for open-source AI. Think of it as the **\"GitHub for Machine Learning\"** - a place where:\n",
        "\n",
        "- ü§ñ **Models** are shared and discovered\n",
        "- üìä **Datasets** are hosted and versioned  \n",
        "- üöÄ **Applications** are deployed (via Spaces)\n",
        "- ü§ù **Collaboration** happens between ML practitioners worldwide\n",
        "\n",
        "### Core Libraries\n",
        "\n",
        "Hugging Face maintains several key libraries:\n",
        "\n",
        "| Library | Purpose |\n",
        "|---------|--------|\n",
        "| `transformers` | State-of-the-art pretrained models |\n",
        "| `diffusers` | Diffusion models for image generation |\n",
        "| `datasets` | Easy access to thousands of datasets |\n",
        "| `accelerate` | Distributed training made simple |\n",
        "| `tokenizers` | Fast tokenization for NLP |\n",
        "\n",
        "### Philosophy: Open Source AI\n",
        "\n",
        "> **\"Why reinvent the wheel when you can stand on the shoulders of giants?\"**\n",
        "\n",
        "Instead of training models from scratch (which requires massive compute and data), Hugging Face enables you to:\n",
        "1. **Find** pre-trained models for your task\n",
        "2. **Fine-tune** them on your specific data\n",
        "3. **Deploy** them easily to production\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ The Transformers Library\n",
        "\n",
        "The `transformers` library is the flagship product of Hugging Face. It provides:\n",
        "\n",
        "- üéØ **Easy access** to 200,000+ pre-trained models\n",
        "- üîß **Unified API** across different model architectures\n",
        "- üìù **Support for NLP, Vision, Audio, and Multimodal tasks**\n",
        "\n",
        "### The Magic of `pipeline()`\n",
        "\n",
        "The `pipeline()` function is the **simplest way to use any model** for inference. It handles:\n",
        "\n",
        "1. **Preprocessing** - Converting your input to model-ready format\n",
        "2. **Model Inference** - Running the actual prediction\n",
        "3. **Postprocessing** - Converting model output to human-readable format\n",
        "\n",
        "Let's explore different pipeline tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Hands-On: Pipeline API Examples\n",
        "\n",
        "### 3.1 Sentiment Analysis üí≠\n",
        "\n",
        "Let's analyze the sentiment of product reviews for a fictional coffee shop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bffda1bcfb04e299c7404588972a636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe970501437411fa60b8240339806ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d85cac1fa36c43749f72a8536fdfe849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4fcea60cab848cfab839f125f749c4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚òï Coffee Shop Review Analysis\n",
            "==================================================\n",
            "\n",
            "üòä POSITIVE (confidence: 99.98%)\n",
            "   Review: \"The espresso was absolutely divine! Best coffee I'...\"\n",
            "\n",
            "üòû NEGATIVE (confidence: 99.98%)\n",
            "   Review: \"Waited 20 minutes for a simple latte. Very disappo...\"\n",
            "\n",
            "üòä POSITIVE (confidence: 99.99%)\n",
            "   Review: \"Cozy atmosphere and friendly staff. The pastries a...\"\n",
            "\n",
            "üòû NEGATIVE (confidence: 99.98%)\n",
            "   Review: \"Overpriced and overcrowded. Not worth the hype.\"\n",
            "\n",
            "üòä POSITIVE (confidence: 99.99%)\n",
            "   Review: \"My new favorite spot! The cold brew is perfection.\"\n"
          ]
        }
      ],
      "source": [
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Sample coffee shop reviews\n",
        "reviews = [\n",
        "    \"The espresso was absolutely divine! Best coffee I've had in years.\",\n",
        "    \"Waited 20 minutes for a simple latte. Very disappointing service.\",\n",
        "    \"Cozy atmosphere and friendly staff. The pastries are okay.\",\n",
        "    \"Overpriced and overcrowded. Not worth the hype.\",\n",
        "    \"My new favorite spot! The cold brew is perfection.\"\n",
        "]\n",
        "\n",
        "print(\"‚òï Coffee Shop Review Analysis\\n\" + \"=\"*50)\n",
        "for review in reviews:\n",
        "    result = sentiment_analyzer(review)[0]\n",
        "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòû\"\n",
        "    print(f\"\\n{emoji} {result['label']} (confidence: {result['score']:.2%})\")\n",
        "    print(f\"   Review: \\\"{review[:50]}...\\\"\" if len(review) > 50 else f\"   Review: \\\"{review}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Named Entity Recognition (NER) üè∑Ô∏è\n",
        "\n",
        "Let's extract entities from news headlines about technology companies!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "334bcb25b4ca43d0a66f4725356ebf79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7780ca5ce66d4a51a886aa83ea2f2253",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfbe270384014b349e896465e8f726ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15435fc02eb747a0b3990c85ae5c7271",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì∞ Tech News Entity Extraction\n",
            "==================================================\n",
            "\n",
            "üìå Headline: \"Microsoft CEO Satya Nadella announces new AI partnership in Seattle\"\n",
            "   Entities found:\n",
            "   ‚Ä¢ Microsoft [ORG] (confidence: 99.88%)\n",
            "   ‚Ä¢ Satya Nadella [PER] (confidence: 99.76%)\n",
            "   ‚Ä¢ Seattle [LOC] (confidence: 99.91%)\n",
            "\n",
            "üìå Headline: \"Apple's Tim Cook visits Tokyo to meet with Japanese suppliers\"\n",
            "   Entities found:\n",
            "   ‚Ä¢ Apple [ORG] (confidence: 99.92%)\n",
            "   ‚Ä¢ Tim Cook [PER] (confidence: 99.96%)\n",
            "   ‚Ä¢ Tokyo [LOC] (confidence: 99.97%)\n",
            "   ‚Ä¢ Japanese [MISC] (confidence: 99.90%)\n",
            "\n",
            "üìå Headline: \"Google DeepMind researchers publish breakthrough paper in Nature\"\n",
            "   Entities found:\n",
            "   ‚Ä¢ Google DeepMind [ORG] (confidence: 93.74%)\n",
            "   ‚Ä¢ Nature [ORG] (confidence: 98.89%)\n"
          ]
        }
      ],
      "source": [
        "# Create a NER pipeline\n",
        "ner_pipeline = pipeline(\n",
        "    task=\"ner\",\n",
        "    aggregation_strategy=\"simple\",  # Group tokens belonging to same entity\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Sample tech news headlines\n",
        "headlines = [\n",
        "    \"Microsoft CEO Satya Nadella announces new AI partnership in Seattle\",\n",
        "    \"Apple's Tim Cook visits Tokyo to meet with Japanese suppliers\",\n",
        "    \"Google DeepMind researchers publish breakthrough paper in Nature\"\n",
        "]\n",
        "\n",
        "print(\"üì∞ Tech News Entity Extraction\\n\" + \"=\"*50)\n",
        "for headline in headlines:\n",
        "    print(f\"\\nüìå Headline: \\\"{headline}\\\"\")\n",
        "    entities = ner_pipeline(headline)\n",
        "    print(\"   Entities found:\")\n",
        "    for entity in entities:\n",
        "        entity_type = entity['entity_group']\n",
        "        entity_text = entity['word']\n",
        "        score = entity['score']\n",
        "        print(f\"   ‚Ä¢ {entity_text} [{entity_type}] (confidence: {score:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Text Generation ‚úçÔ∏è\n",
        "\n",
        "Let's generate creative story continuations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de74bd06faaf4d14a80b523f743ca46f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cacc462a32da4866add297b6464f9d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74e887ab33944f7bb2617758bdf41229",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3223da3dad0a4938af6610888dc71a31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e388845b4fe433c98e87f3b07a1aaca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2943a996395747d7ab8a82ec8aa20dec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53e83d950f004584a684477e02fc400a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Creative Story Generation\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üé≠ Prompt: \"In a world where robots learned to dream,\"\n",
            "   Generated: In a world where robots learned to dream, it's quite hard to see how AI can do anything for humans. But if the human mind could be controlled by an autonomous robot, robots could be made to dream.\n",
            "\n",
            "In fact, the dream machine is being built using a very real robot, called a human-computer interface. And it's using the same basic principles that led to the first real human dream: The human brain is in fact an artificial one.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "It's a robotic version of the dream machine, which is based on IBM's Watson. And it's built using a very similar algorithm as the one we used to build the human dream machine.\n",
            "\n",
            "While the technology is still in its infancy, the technology is also being used in a number of other industries, including medical diagnostics, artificial intelligence, and robotics. According to a new report from the Stanford University Business School, the number of new jobs created in the United Kingdom this year is projected to double this fiscal year.\n",
            "\n",
            "For example, the first jobs created in the UK in the month of May are expected to be expected to be in healthcare, and a further 36,000 tech jobs are expected to be created in the next five years.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "Advertisement\n",
            "\n",
            "This brings into line the fact\n",
            "\n",
            "üé≠ Prompt: \"The ancient library held a secret that\"\n",
            "   Generated: The ancient library held a secret that was never revealed to the Church. When St. John the Baptist was summoned by Jesus to the Council of Nicea, the new king of the Jews, St. John was there, with St. Paul, with St. Joseph and with the Apostles, and the only man in the church who had dared to speak of this secret. However, when the time came for him there was no time for him to speak.\" -St. Matthew, Gospel of Thomas, 4:21\n",
            "\n",
            "The Bible reveals that Joseph was the son of David, the leader of the Jews, and that he was the son of John, the prophet. In fact, the \"King of the Jews\" is David, the patriarch of the Jews.\n",
            "\n",
            "The Bible also says that Joseph was the son of Thomas, the brother of Joseph the \"King of the Jews\".\n",
            "\n",
            "In other words, David and Joseph were the first line of succession of the Jews to Jerusalem.\n",
            "\n",
            "Joseph's \"Son of the Jews\" was the first of Joseph's brothers.\n",
            "\n",
            "Joseph and David are the first line of succession of the Romans to Jerusalem.\n",
            "\n",
            "The Bible states that the Jews were the first of the Gentiles to become the \"kingdom of the Jews\".\n",
            "\n",
            "The Gent\n"
          ]
        }
      ],
      "source": [
        "# Create a text generation pipeline\n",
        "text_generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"gpt2\",  # Using GPT-2 for text generation\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Story prompts\n",
        "prompts = [\n",
        "    \"In a world where robots learned to dream,\",\n",
        "    \"The ancient library held a secret that\",\n",
        "]\n",
        "\n",
        "print(\"üìñ Creative Story Generation\\n\" + \"=\"*50)\n",
        "for prompt in prompts:\n",
        "    result = text_generator(\n",
        "        prompt,\n",
        "        max_length=60,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.8,  # Higher = more creative\n",
        "        do_sample=True\n",
        "    )\n",
        "    print(f\"\\nüé≠ Prompt: \\\"{prompt}\\\"\")\n",
        "    print(f\"   Generated: {result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Question Answering ü§î\n",
        "\n",
        "Let's build a context-based Q&A system about a Space Mission!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baaf39db039e40cfb1d64d5f529c1718",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7ccf83d4ce4359a318f611c13603a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc55c4269c344ca5bffe7185c9469baa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c38ce1f91b3d43bb9cd950adbe40fdc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "895e2d307d3b42cf8548fa37e537e845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Mars Mission Q&A\n",
            "==================================================\n",
            "Context: The Artemis VII mission launched on March 15, 2030, from the Kennedy Space Center.\n",
            "The crew consisted of four astronauts: Commander Sarah Chen, Pilot Marcus Williams,\n",
            "Mission Specialist Dr. Elena Rodr...\n",
            "\n",
            "‚ùì Q: When did the mission launch?\n",
            "‚úÖ A: March 15, 2030 (confidence: 98.34%)\n",
            "\n",
            "‚ùì Q: Who was the commander?\n",
            "‚úÖ A: Sarah Chen (confidence: 96.87%)\n",
            "\n",
            "‚ùì Q: What was the name of the Mars habitat?\n",
            "‚úÖ A: New Olympus (confidence: 91.66%)\n",
            "\n",
            "‚ùì Q: How much did the mission cost?\n",
            "‚úÖ A: $85 billion (confidence: 85.75%)\n",
            "\n",
            "‚ùì Q: How long was the journey?\n",
            "‚úÖ A: 7 months (confidence: 66.26%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a question-answering pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    task=\"question-answering\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Context about a fictional Mars mission\n",
        "context = \"\"\"\n",
        "The Artemis VII mission launched on March 15, 2030, from the Kennedy Space Center.\n",
        "The crew consisted of four astronauts: Commander Sarah Chen, Pilot Marcus Williams,\n",
        "Mission Specialist Dr. Elena Rodriguez, and Flight Engineer James Okonkwo.\n",
        "The journey to Mars took approximately 7 months. Upon arrival, they established\n",
        "the first permanent human habitat on Mars, named 'New Olympus'. The mission was\n",
        "funded by a consortium of 12 countries with a budget of $85 billion.\n",
        "\"\"\"\n",
        "\n",
        "# Questions to ask\n",
        "questions = [\n",
        "    \"When did the mission launch?\",\n",
        "    \"Who was the commander?\",\n",
        "    \"What was the name of the Mars habitat?\",\n",
        "    \"How much did the mission cost?\",\n",
        "    \"How long was the journey?\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ Mars Mission Q&A\\n\" + \"=\"*50)\n",
        "print(f\"Context: {context.strip()[:200]}...\\n\")\n",
        "\n",
        "for question in questions:\n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    print(f\"‚ùì Q: {question}\")\n",
        "    print(f\"‚úÖ A: {result['answer']} (confidence: {result['score']:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Text Summarization üìù\n",
        "\n",
        "Let's summarize a long article about climate change!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5085a6a8fb542e787a06ee2b5e3631d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf6bfd5ba464851bbe8f5bafa03ce08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "433c4309f8074edcaea659b689c10a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84efb863af08483499737b11ab66338c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a99ead4979dc4f999776906c29415d6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54d4978a8e52428f8eff46a3b2319d7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç Article Summarization\n",
            "==================================================\n",
            "Original length: 188 words\n",
            "\n",
            "üìã Summary (33 words):\n",
            "    Global investment in clean energy reached $500 billion in 2024, surpassing fossil fuel investments for the first time in history . Renewable energy could provide 80% of global electricity needs by 2050 .\n"
          ]
        }
      ],
      "source": [
        "# Create a summarization pipeline\n",
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# A lengthy article about renewable energy\n",
        "article = \"\"\"\n",
        "The transition to renewable energy sources has accelerated dramatically in the past decade.\n",
        "Solar and wind power have become the cheapest sources of new electricity generation in most\n",
        "parts of the world. According to recent reports, global investment in clean energy reached\n",
        "$500 billion in 2024, surpassing fossil fuel investments for the first time in history.\n",
        "\n",
        "Countries like Germany, Denmark, and Costa Rica are leading the charge, with renewable\n",
        "sources now providing over 50% of their electricity needs. China, despite being the world's\n",
        "largest carbon emitter, has also become the biggest investor in solar and wind technology,\n",
        "manufacturing over 80% of the world's solar panels.\n",
        "\n",
        "The challenges remain significant, however. Energy storage technology needs further development\n",
        "to handle the intermittent nature of renewable sources. Grid infrastructure in many countries\n",
        "requires substantial upgrades to accommodate distributed power generation. Additionally,\n",
        "the transition must be managed carefully to protect workers in traditional energy sectors.\n",
        "\n",
        "Experts predict that with continued investment and policy support, renewable energy could\n",
        "provide 80% of global electricity needs by 2050, significantly reducing carbon emissions\n",
        "and helping to limit global warming to 1.5 degrees Celsius above pre-industrial levels.\n",
        "\"\"\"\n",
        "\n",
        "print(\"üåç Article Summarization\\n\" + \"=\"*50)\n",
        "print(f\"Original length: {len(article.split())} words\\n\")\n",
        "\n",
        "summary = summarizer(article, max_length=80, min_length=30, do_sample=False)\n",
        "\n",
        "print(f\"üìã Summary ({len(summary[0]['summary_text'].split())} words):\")\n",
        "print(f\"   {summary[0]['summary_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Zero-Shot Classification üéØ\n",
        "\n",
        "Classify text into categories without training! Let's categorize customer support tickets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb5e22e0ef19497288dd936d478732b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f715df9700a482cae1e8153fd416307",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7efd83bee5244b88ae1833b0f97ccd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "959cf31b7e8646ce8a53bca7339fbb33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a955ed0f13e84184ad90545ad3729459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d071f415d1f54bea9ef2f07820e3987f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé´ Customer Support Ticket Classification\n",
            "==================================================\n",
            "\n",
            "üìù Ticket: \"My package hasn't arrived yet and it's been 2 week...\"\n",
            "   üè∑Ô∏è Category: Shipping Issue (confidence: 71.90%)\n",
            "\n",
            "üìù Ticket: \"How do I change my password? I forgot my login cre...\"\n",
            "   üè∑Ô∏è Category: Account Help (confidence: 64.56%)\n",
            "\n",
            "üìù Ticket: \"I'd like a refund for order #12345, the item was d...\"\n",
            "   üè∑Ô∏è Category: Refund Request (confidence: 76.91%)\n",
            "\n",
            "üìù Ticket: \"Your app keeps crashing on my iPhone, please fix t...\"\n",
            "   üè∑Ô∏è Category: Technical Support (confidence: 61.31%)\n",
            "\n",
            "üìù Ticket: \"Can you recommend a product similar to what I boug...\"\n",
            "   üè∑Ô∏è Category: Product Inquiry (confidence: 71.30%)\n"
          ]
        }
      ],
      "source": [
        "# Create a zero-shot classification pipeline\n",
        "classifier = pipeline(\n",
        "    task=\"zero-shot-classification\",\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Customer support tickets\n",
        "tickets = [\n",
        "    \"My package hasn't arrived yet and it's been 2 weeks!\",\n",
        "    \"How do I change my password? I forgot my login credentials.\",\n",
        "    \"I'd like a refund for order #12345, the item was damaged.\",\n",
        "    \"Your app keeps crashing on my iPhone, please fix this bug.\",\n",
        "    \"Can you recommend a product similar to what I bought last month?\"\n",
        "]\n",
        "\n",
        "# Possible categories\n",
        "categories = [\"Shipping Issue\", \"Account Help\", \"Refund Request\", \"Technical Support\", \"Product Inquiry\"]\n",
        "\n",
        "print(\"üé´ Customer Support Ticket Classification\\n\" + \"=\"*50)\n",
        "for ticket in tickets:\n",
        "    result = classifier(ticket, candidate_labels=categories)\n",
        "    top_label = result['labels'][0]\n",
        "    top_score = result['scores'][0]\n",
        "    print(f\"\\nüìù Ticket: \\\"{ticket[:50]}...\\\"\" if len(ticket) > 50 else f\"\\nüìù Ticket: \\\"{ticket}\\\"\")\n",
        "    print(f\"   üè∑Ô∏è Category: {top_label} (confidence: {top_score:.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Exploring the Model Hub\n",
        "\n",
        "The **Hugging Face Model Hub** hosts over 200,000+ pre-trained models covering:\n",
        "\n",
        "### Supported Modalities\n",
        "\n",
        "| Modality | Example Tasks |\n",
        "|----------|---------------|\n",
        "| üìù **Text (NLP)** | Translation, Summarization, Q&A, Classification |\n",
        "| üñºÔ∏è **Vision** | Image Classification, Object Detection, Segmentation |\n",
        "| üéµ **Audio** | Speech Recognition, Audio Classification |\n",
        "| üé≠ **Multimodal** | Image Captioning, Visual Q&A, Text-to-Image |\n",
        "\n",
        "### How to Find Models\n",
        "\n",
        "1. Visit [huggingface.co/models](https://huggingface.co/models)\n",
        "2. Filter by task, library, language, or license\n",
        "3. Check the **Model Card** for usage instructions\n",
        "4. Look at downloads and likes for popularity indicators\n",
        "\n",
        "### Using a Specific Model\n",
        "\n",
        "You can specify any model from the Hub in your pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50b08453d0464d83a64ca90d8e0c2d87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f37953c1e744683846a100ee2190dc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/669M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "655618268a9242f4ad4dae65b627a3dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6532f587d6b4eacab71b86d9121ecc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d56fdfa3d91949fdbb4b6d257cfe7aca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç Multilingual Sentiment Analysis\n",
            "==================================================\n",
            "\n",
            "English: \"This product is amazing!\"\n",
            "   Rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 stars)\n",
            "\n",
            "French: \"Ce produit est fantastique!\"\n",
            "   Rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 stars)\n",
            "\n",
            "German: \"Dieses Produkt ist schrecklich.\"\n",
            "   Rating: ‚≠ê (1/5 stars)\n",
            "\n",
            "Spanish: \"Este producto es excelente!\"\n",
            "   Rating: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5 stars)\n"
          ]
        }
      ],
      "source": [
        "# Using a specific model for multilingual sentiment\n",
        "# This model supports 8 languages!\n",
        "\n",
        "multilingual_sentiment = pipeline(\n",
        "    task=\"sentiment-analysis\",\n",
        "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",  # Specific model from Hub\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Reviews in different languages\n",
        "multilingual_reviews = [\n",
        "    (\"This product is amazing!\", \"English\"),\n",
        "    (\"Ce produit est fantastique!\", \"French\"),\n",
        "    (\"Dieses Produkt ist schrecklich.\", \"German\"),\n",
        "    (\"Este producto es excelente!\", \"Spanish\"),\n",
        "]\n",
        "\n",
        "print(\"üåç Multilingual Sentiment Analysis\\n\" + \"=\"*50)\n",
        "for text, lang in multilingual_reviews:\n",
        "    result = multilingual_sentiment(text)[0]\n",
        "    stars = int(result['label'].split()[0])  # Extract star rating\n",
        "    print(f\"\\n{lang}: \\\"{text}\\\"\")\n",
        "    print(f\"   Rating: {'‚≠ê' * stars} ({stars}/5 stars)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ The Inference API\n",
        "\n",
        "The **Hosted Inference API** allows you to use models without downloading them locally. This is perfect for:\n",
        "\n",
        "- üß™ Quick prototyping and testing\n",
        "- üíª Limited local compute resources\n",
        "- üöÄ Production deployments (with paid API)\n",
        "\n",
        "### How it Works\n",
        "\n",
        "You send an HTTP request to Hugging Face's servers, and they return the prediction.\n",
        "\n",
        "> ‚ö†Ô∏è **Note**: The free API has rate limits. For production use, consider the [Inference Endpoints](https://huggingface.co/inference-endpoints)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Inference API Usage Example\n",
            "==================================================\n",
            "\n",
            "To use the Inference API:\n",
            "\n",
            "1. Get your API token from https://huggingface.co/settings/tokens\n",
            "\n",
            "2. Make a request:\n",
            "   result = query_huggingface_api(\n",
            "       model_id=\"gpt2\",\n",
            "       inputs=\"Once upon a time\",\n",
            "       api_token=\"your_token_here\"\n",
            "   )\n",
            "\n",
            "3. Process the response\n",
            "\n",
            "Supported endpoints:\n",
            "‚Ä¢ Text Generation: /models/gpt2\n",
            "‚Ä¢ Sentiment: /models/distilbert-base-uncased-finetuned-sst-2-english\n",
            "‚Ä¢ Object Detection: /models/facebook/detr-resnet-50\n",
            "‚Ä¢ And many more!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example: Using the Inference API programmatically\n",
        "# Note: For actual API calls, you'd need an API token from huggingface.co\n",
        "\n",
        "def query_huggingface_api(model_id: str, inputs: str, api_token: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Query the Hugging Face Inference API.\n",
        "    \n",
        "    Args:\n",
        "        model_id: The model identifier (e.g., 'gpt2')\n",
        "        inputs: The input text/data\n",
        "        api_token: Your HF API token (optional for some models)\n",
        "    \n",
        "    Returns:\n",
        "        dict: The model's response\n",
        "    \"\"\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
        "    \n",
        "    headers = {}\n",
        "    if api_token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {api_token}\"\n",
        "    \n",
        "    response = requests.post(API_URL, headers=headers, json={\"inputs\": inputs})\n",
        "    return response.json()\n",
        "\n",
        "# Demonstration (without actual API call to avoid rate limits)\n",
        "print(\"üåê Inference API Usage Example\\n\" + \"=\"*50)\n",
        "print(\"\"\"\n",
        "To use the Inference API:\n",
        "\n",
        "1. Get your API token from https://huggingface.co/settings/tokens\n",
        "\n",
        "2. Make a request:\n",
        "   result = query_huggingface_api(\n",
        "       model_id=\"gpt2\",\n",
        "       inputs=\"Once upon a time\",\n",
        "       api_token=\"your_token_here\"\n",
        "   )\n",
        "\n",
        "3. Process the response\n",
        "\n",
        "Supported endpoints:\n",
        "‚Ä¢ Text Generation: /models/gpt2\n",
        "‚Ä¢ Sentiment: /models/distilbert-base-uncased-finetuned-sst-2-english\n",
        "‚Ä¢ Object Detection: /models/facebook/detr-resnet-50\n",
        "‚Ä¢ And many more!\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Building UIs with Gradio\n",
        "\n",
        "**Gradio** is a Python library that makes it incredibly easy to create web interfaces for machine learning models. With just a few lines of code, you can:\n",
        "\n",
        "- üé® Create beautiful interactive demos\n",
        "- üîó Share your model with a public link\n",
        "- üöÄ Deploy to Hugging Face Spaces\n",
        "\n",
        "### Basic Gradio Concepts\n",
        "\n",
        "| Component | Purpose |\n",
        "|-----------|--------|\n",
        "| `gr.Interface` | Wraps a function with input/output components |\n",
        "| `gr.Textbox` | Text input/output |\n",
        "| `gr.Image` | Image input/output |\n",
        "| `gr.Audio` | Audio input/output |\n",
        "| `gr.Slider` | Numeric input with slider |\n",
        "\n",
        "Let's build some interactive demos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® Gradio Interface Created!\n",
            "Run the cell below to launch the demo.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Simple sentiment analysis interface\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyze the sentiment of input text.\n",
        "    Returns a formatted result string.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Please enter some text to analyze.\"\n",
        "    \n",
        "    result = sentiment_analyzer(text)[0]\n",
        "    emoji = \"üòä\" if result['label'] == 'POSITIVE' else \"üòû\"\n",
        "    \n",
        "    return f\"\"\"\n",
        "    {emoji} **Sentiment: {result['label']}**\n",
        "    \n",
        "    Confidence: {result['score']:.2%}\n",
        "    \n",
        "    Input: \"{text}\"\n",
        "    \"\"\"\n",
        "\n",
        "# Create the Gradio interface\n",
        "sentiment_demo = gr.Interface(\n",
        "    fn=analyze_sentiment,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Enter text to analyze\",\n",
        "        placeholder=\"Type a sentence here...\",\n",
        "        lines=3\n",
        "    ),\n",
        "    outputs=gr.Markdown(label=\"Analysis Result\"),\n",
        "    title=\"üé≠ Sentiment Analyzer\",\n",
        "    description=\"Enter any text and I'll tell you if it's positive or negative!\",\n",
        "    examples=[\n",
        "        [\"I absolutely love this product! Best purchase ever.\"],\n",
        "        [\"This is the worst experience I've ever had.\"],\n",
        "        [\"The weather today is quite pleasant.\"]\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "print(\"üé® Gradio Interface Created!\")\n",
        "print(\"Run the cell below to launch the demo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bd9728b145c34116eb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://bd9728b145c34116eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Launch the sentiment analysis demo\n",
        "# This will create a local web server\n",
        "\n",
        "sentiment_demo.launch(\n",
        "    share=True,  # Set to True to get a public link\n",
        "    inline=True   # Display inline in notebook\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Gradio: Multi-Input Q&A Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üé® Q&A Interface Created!\n"
          ]
        }
      ],
      "source": [
        "def answer_question(context, question):\n",
        "    \"\"\"\n",
        "    Answer a question based on provided context.\n",
        "    \"\"\"\n",
        "    if not context.strip() or not question.strip():\n",
        "        return \"Please provide both context and a question.\"\n",
        "    \n",
        "    result = qa_pipeline(question=question, context=context)\n",
        "    \n",
        "    return f\"\"\"\n",
        "    ### üìù Answer\n",
        "    **{result['answer']}**\n",
        "    \n",
        "    ---\n",
        "    *Confidence: {result['score']:.2%}*\n",
        "    \"\"\"\n",
        "\n",
        "# Create a multi-input interface\n",
        "qa_demo = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"üìö Context\",\n",
        "            placeholder=\"Paste a paragraph of text here...\",\n",
        "            lines=8\n",
        "        ),\n",
        "        gr.Textbox(\n",
        "            label=\"‚ùì Question\",\n",
        "            placeholder=\"Ask a question about the context...\",\n",
        "            lines=2\n",
        "        )\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Answer\"),\n",
        "    title=\"ü§ñ AI Question Answering System\",\n",
        "    description=\"Provide a context paragraph and ask questions about it!\",\n",
        "    examples=[\n",
        "        [\n",
        "            \"Python was created by Guido van Rossum and released in 1991. It emphasizes code readability with its notable use of significant whitespace. Python is dynamically typed and garbage-collected. It supports multiple programming paradigms.\",\n",
        "            \"Who created Python?\"\n",
        "        ],\n",
        "        [\n",
        "            \"The Eiffel Tower is located in Paris, France. It was constructed from 1887 to 1889 as the entrance arch for the 1889 World's Fair. The tower is 330 meters tall and was the tallest man-made structure in the world for 41 years.\",\n",
        "            \"How tall is the Eiffel Tower?\"\n",
        "        ]\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "print(\"üé® Q&A Interface Created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fef19a1af529c3a919.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://fef19a1af529c3a919.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Launch the Q&A demo\n",
        "qa_demo.launch(share=True, inline=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing server running on port: 7860\n",
            "Closing server running on port: 7861\n"
          ]
        }
      ],
      "source": [
        "gr.close_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Summary & Key Takeaways\n",
        "\n",
        "Congratulations! You've completed the introduction to Hugging Face. Here's what you learned:\n",
        "\n",
        "### Core Concepts\n",
        "\n",
        "| Concept | What You Learned |\n",
        "|---------|------------------|\n",
        "| **Hugging Face Hub** | The \"GitHub for ML\" hosting 200,000+ models |\n",
        "| **Transformers Library** | Easy access to pre-trained models |\n",
        "| **Pipeline API** | One-liner model inference |\n",
        "| **Inference API** | Cloud-based model usage |\n",
        "| **Gradio** | Building ML web interfaces |\n",
        "\n",
        "### Pipeline Tasks Explored\n",
        "\n",
        "- ‚úÖ Sentiment Analysis\n",
        "- ‚úÖ Named Entity Recognition\n",
        "- ‚úÖ Text Generation\n",
        "- ‚úÖ Question Answering\n",
        "- ‚úÖ Summarization\n",
        "- ‚úÖ Zero-Shot Classification\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. üîç **Explore the Hub** - Find models for your specific use case\n",
        "2. üõ†Ô∏è **Fine-tune** - Learn to adapt models to your data\n",
        "3. üöÄ **Deploy** - Publish your model on Hugging Face Spaces\n",
        "4. ü§ù **Contribute** - Share your models with the community\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional Resources\n",
        "\n",
        "- üìñ [Hugging Face Documentation](https://huggingface.co/docs)\n",
        "- üéì [Hugging Face Course](https://huggingface.co/course)\n",
        "- ü§ó [Model Hub](https://huggingface.co/models)\n",
        "- üìä [Datasets Hub](https://huggingface.co/datasets)\n",
        "- üöÄ [Spaces](https://huggingface.co/spaces)\n",
        "- üí¨ [Forums](https://discuss.huggingface.co/)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Learning! ü§ó**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
