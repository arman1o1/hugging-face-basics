{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  Chapter 3: NLP Tasks using Hugging Face\n",
                "\n",
                "This notebook will guide you through performing various Natural Language Processing (NLP) tasks using Hugging Face's powerful `pipeline()` function and Transformers library.\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“š What You'll Learn\n",
                "\n",
                "| Section | Topic | Description |\n",
                "|---------|-------|-------------|\n",
                "| 1 | **Transformer Architecture** | Understanding what makes Transformers special |\n",
                "| 2 | **The Pipeline Function** | Simplified API for NLP tasks |\n",
                "| 3 | **Text Classification** | Sentiment analysis & topic classification |\n",
                "| 4 | **Named Entity Recognition** | Extracting entities from text |\n",
                "| 5 | **Text Summarization** | Condensing long text into summaries |\n",
                "| 6 | **Translation** | Converting text between languages |\n",
                "| 7 | **Text Generation** | Creating new text from prompts |\n",
                "| 8 | **Question Answering** | Answering questions from context |\n",
                "| 9 | **Zero-shot Classification** | Classifying without training |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ”§ Setup\n",
                "\n",
                "First, let's install and import the necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to install required packages\n",
                "# %pip install transformers torch accelerate sentencepiece sacremoses -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“¦ Library Versions\n",
                        "========================================\n",
                        "ğŸ¤— Transformers: 4.57.3\n",
                        "ğŸ”¥ PyTorch: 2.9.0+cu126\n",
                        "ğŸš€ Using GPU: Tesla T4\n"
                    ]
                }
            ],
            "source": [
                "# Core imports\n",
                "import transformers\n",
                "from transformers import pipeline\n",
                "import torch\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check versions\n",
                "print(\"ğŸ“¦ Library Versions\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ğŸ¤— Transformers: {transformers.__version__}\")\n",
                "print(f\"ğŸ”¥ PyTorch: {torch.__version__}\")\n",
                "\n",
                "# Device selection\n",
                "if torch.cuda.is_available():\n",
                "    device = 0\n",
                "    print(f\"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    device = -1\n",
                "    print(\"ğŸ’» Using CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1ï¸âƒ£ Understanding the Transformer Architecture\n",
                "\n",
                "The **Transformer** architecture revolutionized NLP in 2017 with the paper \"Attention Is All You Need\". Here's what makes it special:\n",
                "\n",
                "### Key Components\n",
                "\n",
                "| Component | Purpose |\n",
                "|-----------|--------|\n",
                "| **Self-Attention** | Allows each word to \"look at\" all other words in the sentence |\n",
                "| **Multi-Head Attention** | Multiple attention patterns learned simultaneously |\n",
                "| **Positional Encoding** | Adds word position information |\n",
                "| **Feed-Forward Networks** | Processes attention outputs |\n",
                "\n",
                "### Why Transformers Beat RNNs\n",
                "\n",
                "```\n",
                "Traditional RNN:  Word1 â†’ Word2 â†’ Word3 â†’ Word4  (Sequential, slow)\n",
                "Transformer:      Word1 âŸ· Word2 âŸ· Word3 âŸ· Word4  (Parallel, all at once!)\n",
                "```\n",
                "\n",
                "> ğŸ’¡ **Key Insight**: Transformers can process all words simultaneously, making them much faster and better at capturing long-range dependencies.\n",
                "\n",
                "### Transformer Architecture Overview\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚              INPUT TEXT                 â”‚\n",
                "â”‚         \"The food was amazing\"          â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                  â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚        TOKENIZATION + EMBEDDING         â”‚\n",
                "â”‚    Convert words to numerical vectors   â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                  â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚         POSITIONAL ENCODING             â”‚\n",
                "â”‚      Add position information           â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                  â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚      TRANSFORMER ENCODER LAYERS         â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
                "â”‚  â”‚     Multi-Head Self-Attention     â”‚  â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
                "â”‚                  â–¼                      â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
                "â”‚  â”‚     Feed-Forward Network          â”‚  â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                  â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚              OUTPUT                     â”‚\n",
                "â”‚    Sentiment: POSITIVE (0.98)           â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2ï¸âƒ£ The Pipeline Function\n",
                "\n",
                "The `pipeline()` function is Hugging Face's **easiest way** to use pre-trained models. It handles:\n",
                "\n",
                "- âœ… Tokenization (converting text to numbers)\n",
                "- âœ… Model inference (running the neural network)\n",
                "- âœ… Post-processing (converting outputs to readable format)\n",
                "\n",
                "### Available Pipeline Tasks\n",
                "\n",
                "| Task | Description | Example Use Case |\n",
                "|------|-------------|------------------|\n",
                "| `sentiment-analysis` | Positive/negative classification | Product reviews |\n",
                "| `text-classification` | Multi-class classification | Spam detection |\n",
                "| `ner` | Named Entity Recognition | Extract names, locations |\n",
                "| `summarization` | Condense long text | Article summaries |\n",
                "| `translation_xx_to_yy` | Language translation | Multi-language apps |\n",
                "| `text-generation` | Generate new text | Creative writing |\n",
                "| `question-answering` | Answer questions | FAQ bots |\n",
                "| `zero-shot-classification` | Classify without training | Flexible categorization |\n",
                "\n",
                "### Basic Usage Pattern\n",
                "\n",
                "```python\n",
                "from transformers import pipeline\n",
                "\n",
                "# Create a pipeline for any task\n",
                "task_pipeline = pipeline(\"task-name\")\n",
                "\n",
                "# Use it!\n",
                "result = task_pipeline(\"Your input text here\")\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3ï¸âƒ£ Text Classification\n",
                "\n",
                "Text classification assigns categories or labels to text. The most common example is **sentiment analysis**.\n",
                "\n",
                "### 3.1 Sentiment Analysis - Restaurant Reviews\n",
                "\n",
                "Let's analyze customer reviews for a restaurant!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a443caca88a14bd680a73942022122a1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d4e01a7ed73346c295c3aa154675b88b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4d232329174f4ba18cec7601a7b65e1e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "14ba115f8ef74a03b1beac3b58b7a83c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ• Restaurant Review Sentiment Analysis\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ˜Š POSITIVE (100.0%)\n",
                        "   \"The pasta was absolutely divine! Best Italian food I've had ...\"\n",
                        "\n",
                        "ğŸ˜ NEGATIVE (100.0%)\n",
                        "   \"Service was extremely slow and the waiter was rude. Very dis...\"\n",
                        "\n",
                        "ğŸ˜ NEGATIVE (99.9%)\n",
                        "   \"Decent food but nothing special. Prices are a bit too high f...\"\n",
                        "\n",
                        "ğŸ˜Š POSITIVE (100.0%)\n",
                        "   \"Amazing ambiance and the chef's special was out of this worl...\"\n",
                        "\n",
                        "ğŸ˜ NEGATIVE (99.8%)\n",
                        "   \"Food poisoning after eating here. Never going back. Avoid at...\"\n"
                    ]
                }
            ],
            "source": [
                "# Create a sentiment analysis pipeline\n",
                "sentiment_analyzer = pipeline(\n",
                "    \"sentiment-analysis\",\n",
                "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Restaurant reviews to analyze\n",
                "restaurant_reviews = [\n",
                "    \"The pasta was absolutely divine! Best Italian food I've had in years.\",\n",
                "    \"Service was extremely slow and the waiter was rude. Very disappointed.\",\n",
                "    \"Decent food but nothing special. Prices are a bit too high for what you get.\",\n",
                "    \"Amazing ambiance and the chef's special was out of this world!\",\n",
                "    \"Food poisoning after eating here. Never going back. Avoid at all costs!\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ• Restaurant Review Sentiment Analysis\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for review in restaurant_reviews:\n",
                "    result = sentiment_analyzer(review)[0]\n",
                "    emoji = \"ğŸ˜Š\" if result['label'] == 'POSITIVE' else \"ğŸ˜\"\n",
                "    print(f\"\\n{emoji} {result['label']} ({result['score']:.1%})\")\n",
                "    print(f\"   \\\"{review[:60]}...\\\"\" if len(review) > 60 else f\"   \\\"{review}\\\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Batch Processing for Efficiency\n",
                "\n",
                "When analyzing many texts, batch processing is much faster!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš¡ Batch Processing Performance\n",
                        "========================================\n",
                        "ğŸ“Š Processing 50 reviews\n",
                        "\n",
                        "â±ï¸ One-by-one: 0.23s\n",
                        "â±ï¸ Batch:      0.25s\n",
                        "ğŸš€ Speedup:    0.9x faster!\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "# More reviews for batch processing demo\n",
                "many_reviews = restaurant_reviews * 10  # 50 reviews\n",
                "\n",
                "# Single processing\n",
                "start = time.time()\n",
                "single_results = [sentiment_analyzer(r) for r in many_reviews]\n",
                "single_time = time.time() - start\n",
                "\n",
                "# Batch processing\n",
                "start = time.time()\n",
                "batch_results = sentiment_analyzer(many_reviews)\n",
                "batch_time = time.time() - start\n",
                "\n",
                "print(\"âš¡ Batch Processing Performance\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ğŸ“Š Processing {len(many_reviews)} reviews\")\n",
                "print(f\"\\nâ±ï¸ One-by-one: {single_time:.2f}s\")\n",
                "print(f\"â±ï¸ Batch:      {batch_time:.2f}s\")\n",
                "print(f\"ğŸš€ Speedup:    {single_time/batch_time:.1f}x faster!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Multi-class Classification - Tech News Topics\n",
                "\n",
                "Let's classify technology news headlines into different categories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“° Tech News Topic Classification\n",
                        "============================================================\n",
                        "ğŸ·ï¸ Categories: AI & Hardware, Cryptocurrency, Cybersecurity, Space Technology, Social Media\n",
                        "\n",
                        "ğŸ“Œ AI & Hardware (92.6%)\n",
                        "   Apple announces new AI-powered chip for next-generation MacBooks\n",
                        "\n",
                        "ğŸ“Œ Cryptocurrency (98.8%)\n",
                        "   Bitcoin surges past $100,000 as institutional investors pile in\n",
                        "\n",
                        "ğŸ“Œ Cybersecurity (96.2%)\n",
                        "   Security researchers discover critical vulnerability in popular browser\n",
                        "\n",
                        "ğŸ“Œ Space Technology (91.4%)\n",
                        "   SpaceX successfully launches 60 more Starlink satellites into orbit\n",
                        "\n",
                        "ğŸ“Œ Social Media (98.5%)\n",
                        "   New study shows social media algorithms impact teen mental health\n"
                    ]
                }
            ],
            "source": [
                "# Use zero-shot classification for topic classification\n",
                "# This allows us to define our own categories without training!\n",
                "topic_classifier = pipeline(\n",
                "    \"zero-shot-classification\",\n",
                "    model=\"facebook/bart-large-mnli\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "tech_headlines = [\n",
                "    \"Apple announces new AI-powered chip for next-generation MacBooks\",\n",
                "    \"Bitcoin surges past $100,000 as institutional investors pile in\",\n",
                "    \"Security researchers discover critical vulnerability in popular browser\",\n",
                "    \"SpaceX successfully launches 60 more Starlink satellites into orbit\",\n",
                "    \"New study shows social media algorithms impact teen mental health\"\n",
                "]\n",
                "\n",
                "# Define our custom topic categories\n",
                "topic_labels = [\"AI & Hardware\", \"Cryptocurrency\", \"Cybersecurity\", \"Space Technology\", \"Social Media\"]\n",
                "\n",
                "print(\"ğŸ“° Tech News Topic Classification\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"ğŸ·ï¸ Categories: {', '.join(topic_labels)}\")\n",
                "\n",
                "for headline in tech_headlines:\n",
                "    result = topic_classifier(headline, topic_labels)\n",
                "    top_label = result['labels'][0]\n",
                "    top_score = result['scores'][0]\n",
                "    print(f\"\\nğŸ“Œ {top_label} ({top_score:.1%})\")\n",
                "    print(f\"   {headline}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4ï¸âƒ£ Named Entity Recognition (NER)\n",
                "\n",
                "NER identifies and classifies named entities (people, organizations, locations, etc.) in text.\n",
                "\n",
                "### Entity Types\n",
                "\n",
                "| Tag | Meaning | Example |\n",
                "|-----|---------|----------|\n",
                "| PER | Person | \"Elon Musk\" |\n",
                "| ORG | Organization | \"Google\" |\n",
                "| LOC | Location | \"San Francisco\" |\n",
                "| MISC | Miscellaneous | \"iPhone\" |\n",
                "\n",
                "### 4.1 Extracting Entities from Travel Blogs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
                        "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœˆï¸ Named Entity Recognition - Travel Blog\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“ Text: Last summer, I traveled from New York to Paris on Air France. \n",
                        "My guide, Marie Dubois, showed me the Eiffel Tower and the Louvre Museum. \n",
                        "We had the most amazing croissants at a cafÃ© near Notre-Dame Cathedral.\n",
                        "I also met fellow travelers from Tokyo and Sydney at our hostel.\n",
                        "\n",
                        "ğŸ·ï¸ Extracted Entities:\n",
                        "----------------------------------------\n",
                        "\n",
                        "ğŸ“ LOC: Eiffel Tower, Notre - Dame Cathedral, Tokyo, Louvre Museum, Paris, Sydney, New York\n",
                        "\n",
                        "ğŸ¢ ORG: Air France\n",
                        "\n",
                        "ğŸ‘¤ PER: Marie Dubois\n"
                    ]
                }
            ],
            "source": [
                "# Create NER pipeline\n",
                "ner_pipeline = pipeline(\n",
                "    \"ner\",\n",
                "    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
                "    aggregation_strategy=\"simple\",  # Groups tokens into words\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Travel blog excerpt\n",
                "travel_blog = \"\"\"\n",
                "Last summer, I traveled from New York to Paris on Air France. \n",
                "My guide, Marie Dubois, showed me the Eiffel Tower and the Louvre Museum. \n",
                "We had the most amazing croissants at a cafÃ© near Notre-Dame Cathedral.\n",
                "I also met fellow travelers from Tokyo and Sydney at our hostel.\n",
                "\"\"\"\n",
                "\n",
                "print(\"âœˆï¸ Named Entity Recognition - Travel Blog\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ“ Text: {travel_blog.strip()}\")\n",
                "print(\"\\nğŸ·ï¸ Extracted Entities:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "entities = ner_pipeline(travel_blog)\n",
                "\n",
                "# Group by entity type\n",
                "entity_groups = {}\n",
                "for entity in entities:\n",
                "    label = entity['entity_group']\n",
                "    if label not in entity_groups:\n",
                "        entity_groups[label] = []\n",
                "    entity_groups[label].append(entity['word'])\n",
                "\n",
                "# Pretty print\n",
                "icons = {'PER': 'ğŸ‘¤', 'LOC': 'ğŸ“', 'ORG': 'ğŸ¢', 'MISC': 'ğŸ“¦'}\n",
                "for label, words in entity_groups.items():\n",
                "    icon = icons.get(label, 'ğŸ·ï¸')\n",
                "    print(f\"\\n{icon} {label}: {', '.join(set(words))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Business News Entity Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ’¼ Business News Entity Extraction\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“° Microsoft CEO Satya Nadella announced a partnership with OpenAI in Seattle.\n",
                        "   ğŸ¢ Microsoft (ORG, 99.9%)\n",
                        "   ğŸ‘¤ Satya Nadella (PER, 99.8%)\n",
                        "   ğŸ¢ OpenAI (ORG, 99.7%)\n",
                        "   ğŸ“ Seattle (LOC, 99.9%)\n",
                        "----------------------------------------\n",
                        "\n",
                        "ğŸ“° Amazon is opening a new headquarters in Arlington, Virginia.\n",
                        "   ğŸ¢ Amazon (ORG, 99.9%)\n",
                        "   ğŸ“ Arlington (LOC, 99.7%)\n",
                        "   ğŸ“ Virginia (LOC, 99.9%)\n",
                        "----------------------------------------\n",
                        "\n",
                        "ğŸ“° Warren Buffett's Berkshire Hathaway increased its stake in Apple Inc.\n",
                        "   ğŸ‘¤ Warren Buffett (PER, 98.3%)\n",
                        "   ğŸ¢ Berkshire Hathaway (ORG, 99.9%)\n",
                        "   ğŸ¢ Apple Inc (ORG, 99.9%)\n",
                        "----------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "# Business news excerpts\n",
                "business_news = [\n",
                "    \"Microsoft CEO Satya Nadella announced a partnership with OpenAI in Seattle.\",\n",
                "    \"Amazon is opening a new headquarters in Arlington, Virginia.\",\n",
                "    \"Warren Buffett's Berkshire Hathaway increased its stake in Apple Inc.\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ’¼ Business News Entity Extraction\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for news in business_news:\n",
                "    print(f\"\\nğŸ“° {news}\")\n",
                "    entities = ner_pipeline(news)\n",
                "    \n",
                "    if entities:\n",
                "        for entity in entities:\n",
                "            icon = icons.get(entity['entity_group'], 'ğŸ·ï¸')\n",
                "            print(f\"   {icon} {entity['word']} ({entity['entity_group']}, {entity['score']:.1%})\")\n",
                "    print(\"-\" * 40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5ï¸âƒ£ Text Summarization\n",
                "\n",
                "Text summarization condenses long documents into shorter versions while preserving key information.\n",
                "\n",
                "### Types of Summarization\n",
                "\n",
                "| Type | Description |\n",
                "|------|-------------|\n",
                "| **Extractive** | Selects and concatenates important sentences |\n",
                "| **Abstractive** | Generates new text that captures the meaning |\n",
                "\n",
                "### 5.1 Summarizing Scientific Abstracts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c7ff1f31d8324ebbb029e01d7bc1f7d4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b0f188efd85a4d39ae7ce0f57b6fe86f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5b4d76cbc9c640a9b8172e6b10890f84",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d3ad595082804c0fb64f72eef0372628",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "11d24c46f4424dae92f8b790d20a5959",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "557d8ce2216246ceb1ad3c8ae4c8d910",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”¬ Scientific Text Summarization\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“„ Original (128 words):\n",
                        "Climate change represents one of the most significant challenges facing humanity in the 21st century. \n",
                        "Rising global temperatures, driven primarily by increased greenhouse gas emissions from human activities, \n",
                        "are causing widespread environmental changes. These include melting polar ice caps, rising sea levels, \n",
                        "more frequent and intense weather events, and shifts in ecosystem distributions. Scientists project \n",
                        "that without significant reductions in carbon emissions, global temperatures could rise by 2-4 degrees \n",
                        "Celsius by the end of the century. This would result in catastrophic consequences for both natural \n",
                        "systems and human societies, including food and water scarcity, mass migrations, and economic disruption. \n",
                        "Addressing climate change requires coordinated global action, including transitioning to renewable energy \n",
                        "sources, improving energy efficiency, protecting and restoring forests, and developing new technologies \n",
                        "for carbon capture and storage.\n",
                        "\n",
                        "ğŸ“ Summary (46 words):\n",
                        "Climate change represents one of the most significant challenges facing humanity in the 21st century. Without significant reductions in carbon emissions, global temperatures could rise by 2-4 degrees by the end of the century. Addressing climate change requires coordinated global action, including transitioning to renewable energy.\n",
                        "\n",
                        "ğŸ“Š Compression: 36% of original\n"
                    ]
                }
            ],
            "source": [
                "# Create summarization pipeline\n",
                "summarizer = pipeline(\n",
                "    \"summarization\",\n",
                "    model=\"facebook/bart-large-cnn\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Scientific abstract about climate change\n",
                "scientific_text = \"\"\"\n",
                "Climate change represents one of the most significant challenges facing humanity in the 21st century. \n",
                "Rising global temperatures, driven primarily by increased greenhouse gas emissions from human activities, \n",
                "are causing widespread environmental changes. These include melting polar ice caps, rising sea levels, \n",
                "more frequent and intense weather events, and shifts in ecosystem distributions. Scientists project \n",
                "that without significant reductions in carbon emissions, global temperatures could rise by 2-4 degrees \n",
                "Celsius by the end of the century. This would result in catastrophic consequences for both natural \n",
                "systems and human societies, including food and water scarcity, mass migrations, and economic disruption. \n",
                "Addressing climate change requires coordinated global action, including transitioning to renewable energy \n",
                "sources, improving energy efficiency, protecting and restoring forests, and developing new technologies \n",
                "for carbon capture and storage.\n",
                "\"\"\"\n",
                "\n",
                "print(\"ğŸ”¬ Scientific Text Summarization\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ“„ Original ({len(scientific_text.split())} words):\")\n",
                "print(scientific_text.strip())\n",
                "\n",
                "# Generate summary\n",
                "summary = summarizer(\n",
                "    scientific_text, \n",
                "    max_length=80, \n",
                "    min_length=30, \n",
                "    do_sample=False\n",
                ")[0]['summary_text']\n",
                "\n",
                "print(f\"\\nğŸ“ Summary ({len(summary.split())} words):\")\n",
                "print(summary)\n",
                "print(f\"\\nğŸ“Š Compression: {len(summary.split())/len(scientific_text.split())*100:.0f}% of original\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Summarizing with Different Lengths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ™ï¸ Podcast Summary - Different Lengths\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“Œ Short Summary (43 words):\n",
                        "   A leading researcher in machine learning joined us to discuss the future of artificial intelligence. We looked at emerging applications in healthcare, where AI is being used to diagnose diseases earlier and personalize treatments. The key takeaway is that AI is a powerful\n",
                        "\n",
                        "ğŸ“Œ Medium Summary (51 words):\n",
                        "   A leading researcher in machine learning joined us to discuss the future of artificial intelligence. We looked at emerging applications in healthcare, where AI is being used to diagnose diseases earlier, personalize treatments, and accelerate drug discovery. The key takeaway is that AI is a powerful tool that requires careful stewardship.\n",
                        "\n",
                        "ğŸ“Œ Detailed Summary (73 words):\n",
                        "   A leading researcher in machine learning joins us to discuss the future of artificial intelligence. We look at how large language models are changing the way we interact with computers. We also explore ethical considerations around AI, including bias in training data, privacy concerns, and the potential displacement of workers. Finally, we look at emerging applications in healthcare, where AI is being used to diagnose diseases earlier, personalize treatments, and accelerate drug discovery.\n"
                    ]
                }
            ],
            "source": [
                "# Podcast transcript excerpt\n",
                "podcast_text = \"\"\"\n",
                "Today we're discussing the future of artificial intelligence with our special guest, \n",
                "a leading researcher in machine learning. We covered three main topics: first, how \n",
                "large language models are changing the way we interact with computers, making technology \n",
                "more accessible to everyone through natural language interfaces. Second, we explored \n",
                "the ethical considerations around AI, including bias in training data, privacy concerns, \n",
                "and the potential displacement of workers. Our guest emphasized the importance of \n",
                "developing AI responsibly and ensuring that benefits are distributed equitably across \n",
                "society. Finally, we looked at emerging applications in healthcare, where AI is being \n",
                "used to diagnose diseases earlier, personalize treatments, and accelerate drug discovery. \n",
                "The key takeaway is that AI is a powerful tool that requires careful stewardship.\n",
                "\"\"\"\n",
                "\n",
                "print(\"ğŸ™ï¸ Podcast Summary - Different Lengths\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Different summary lengths\n",
                "lengths = [(30, 50, \"Short\"), (50, 80, \"Medium\"), (80, 120, \"Detailed\")]\n",
                "\n",
                "for min_len, max_len, label in lengths:\n",
                "    summary = summarizer(\n",
                "        podcast_text,\n",
                "        max_length=max_len,\n",
                "        min_length=min_len,\n",
                "        do_sample=False\n",
                "    )[0]['summary_text']\n",
                "    \n",
                "    print(f\"\\nğŸ“Œ {label} Summary ({len(summary.split())} words):\")\n",
                "    print(f\"   {summary}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6ï¸âƒ£ Translation\n",
                "\n",
                "Translation converts text from one language to another using specialized models.\n",
                "\n",
                "### 6.1 Translating Cooking Recipes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4a84de2f99f84b8bb551621ed27c6c43",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0e559a61e8f34621889494599ab15c2b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6e7f5e8c9e3949beb60071c019ca544d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "bad6d8be5cbb4c95b56ae38abcee6126",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1074060eb4fd4e4699fb61ec69ef655b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e3d913f45b9343acb07399f8775f4769",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "40b5baf96a744ad8b9a6241bbf0ba443",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "357597c18308485ba5c397291d4a8749",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ° Recipe Translation (English â†’ French)\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ Preheat the oven to 180 degrees Celsius.\n",
                        "ğŸ‡«ğŸ‡· PrÃ©chauffer le four Ã  180 degrÃ©s Celsius.\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ Mix flour, sugar, and eggs in a large bowl.\n",
                        "ğŸ‡«ğŸ‡· MÃ©langer la farine, le sucre et les Å“ufs dans un grand bol.\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ Pour the batter into a greased baking pan.\n",
                        "ğŸ‡«ğŸ‡· Verser la pÃ¢te dans une casserole graissÃ©e.\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ Bake for 25 minutes until golden brown.\n",
                        "ğŸ‡«ğŸ‡· Cuire au four pendant 25 minutes jusqu'Ã  ce qu'il soit dorÃ©.\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ Let it cool before serving with fresh cream.\n",
                        "ğŸ‡«ğŸ‡· Laisser refroidir avant de servir avec de la crÃ¨me fraÃ®che.\n"
                    ]
                }
            ],
            "source": [
                "# English to French translator\n",
                "translator_en_fr = pipeline(\n",
                "    \"translation_en_to_fr\",\n",
                "    model=\"Helsinki-NLP/opus-mt-en-fr\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Recipe instructions\n",
                "recipe_steps = [\n",
                "    \"Preheat the oven to 180 degrees Celsius.\",\n",
                "    \"Mix flour, sugar, and eggs in a large bowl.\",\n",
                "    \"Pour the batter into a greased baking pan.\",\n",
                "    \"Bake for 25 minutes until golden brown.\",\n",
                "    \"Let it cool before serving with fresh cream.\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ° Recipe Translation (English â†’ French)\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for step in recipe_steps:\n",
                "    translated = translator_en_fr(step)[0]['translation_text']\n",
                "    print(f\"\\nğŸ‡¬ğŸ‡§ {step}\")\n",
                "    print(f\"ğŸ‡«ğŸ‡· {translated}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Multi-Language Greeting Generator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9c9c2721f00e4094901a57aa18e1166f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "36e70974b5774e7bae5b063318481cae",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c94af50ebfb24a988d155eb055b1bd28",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1b31c52050924f47943b131acfc826e7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/298M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b4fade77b51f4785a672afd2b619c1e6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3b91fe8d4f294cf0881efc0a91924426",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "703e7e3df9d342d78483820ea47acacb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "01ebbb5dce63485cbbe0b365d3778810",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "97ca6b4cc4ce48d0adc2be8bf47ce827",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f78dacb6fdd0426fb1e6c45fdb74a727",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c810ef6b215646899db63e37c25d194f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2814c44fd53446788040e273cf0dcd71",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a66beacaab164db298c0db9ae24d6e38",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f0cf62554b744ba9bc573029ef6a3ea8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e57e23e84fc4488faebafcb98e6d214d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "267b50adbfed4397ab8cf408f2ed7522",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸŒ Multi-Language Greeting\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ‡¬ğŸ‡§ English: Welcome to our restaurant! We hope you enjoy your meal today.\n",
                        "\n",
                        "ğŸ‡«ğŸ‡· French: Bienvenue dans notre restaurant! Nous espÃ©rons que vous apprÃ©cierez votre repas aujourd'hui.\n",
                        "\n",
                        "ğŸ‡©ğŸ‡ª German: Willkommen in unserem Restaurant! Wir hoffen, dass Sie Ihre Mahlzeit heute genieÃŸen.\n",
                        "\n",
                        "ğŸ‡ªğŸ‡¸ Spanish: Â¡Bienvenidos a nuestro restaurante! Esperamos que disfruten de su comida hoy.\n"
                    ]
                }
            ],
            "source": [
                "# Create translators for different languages\n",
                "translators = {\n",
                "    \"French\": pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=device),\n",
                "    \"German\": pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\", device=device),\n",
                "    \"Spanish\": pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=device)\n",
                "}\n",
                "\n",
                "greeting = \"Welcome to our restaurant! We hope you enjoy your meal today.\"\n",
                "\n",
                "print(\"ğŸŒ Multi-Language Greeting\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ‡¬ğŸ‡§ English: {greeting}\")\n",
                "\n",
                "flags = {\"French\": \"ğŸ‡«ğŸ‡·\", \"German\": \"ğŸ‡©ğŸ‡ª\", \"Spanish\": \"ğŸ‡ªğŸ‡¸\"}\n",
                "\n",
                "for language, translator in translators.items():\n",
                "    translated = translator(greeting)[0]['translation_text']\n",
                "    print(f\"\\n{flags[language]} {language}: {translated}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7ï¸âƒ£ Text Generation\n",
                "\n",
                "Text generation creates new text based on a prompt. This is the technology behind ChatGPT and similar tools.\n",
                "\n",
                "### 7.1 Creative Story Continuation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "00ec5093b3d8481d904ec3db8500b9ef",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3b468ee82f7341f997b483e563da1a50",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "df4a6f3d6ea54219b02824ea9d5272de",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "67dea430fc9f4fc7b19b6b13f75f3b4a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7b3e30cd674e4b07bcee24f2780d11ee",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e61dd9644eb5430e8637dd392b42d291",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8e05950575cb4f74a5d5a7becb11dcd2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n",
                        "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
                        "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“– Creative Story Continuation\n",
                        "============================================================\n",
                        "\n",
                        "âœï¸ Prompt: The old lighthouse keeper looked out at the stormy sea and noticed something strange. A mysterious light appeared on the horizon, growing brighter with each passing moment.\n",
                        "\n",
                        "ğŸ“š Generated Continuation:\n",
                        " It was this light that he expected when it appeared. It was the power of the sea. There was nothing here that he could not see, but then he thought. The lighthouse keeper had seen the light with his own eyes. He could not deny it was there. He was sure that he had seen it before before, but what he had was a dark and dangerous place. He had seen all the strange lights that people had seen here.\n",
                        "\n",
                        "As the lighthouse keeper looked at the lighthouse, he could see nothing. He could not help but sigh. He was so worried. He had called for help. He had even called for something. They were safe. He had to go.\n",
                        "\n",
                        "He felt the sunlight on his hand. It was warm and gentle, but there was something missing from it. He felt a little bit shaky, but he did it. He felt his hand on his wrist. As the sun sank low behind the lighthouse keeper, everything around him was glowing and glowing brightly. Nothing was moving. It seemed that there was a place where the light could not be seen anymore. He looked around his own home. There were no signs of life. There were no walls and stairs. All he could see was dust and grass.\n",
                        "\n",
                        "A few\n"
                    ]
                }
            ],
            "source": [
                "# Create text generation pipeline\n",
                "generator = pipeline(\n",
                "    \"text-generation\",\n",
                "    model=\"gpt2\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "story_prompt = \"The old lighthouse keeper looked out at the stormy sea and noticed something strange. A mysterious light appeared on the horizon, growing brighter with each passing moment.\"\n",
                "\n",
                "print(\"ğŸ“– Creative Story Continuation\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nâœï¸ Prompt: {story_prompt}\")\n",
                "print(\"\\nğŸ“š Generated Continuation:\")\n",
                "\n",
                "# Generate continuation\n",
                "generated = generator(\n",
                "    story_prompt,\n",
                "    max_length=150,\n",
                "    num_return_sequences=1,\n",
                "    temperature=0.8,\n",
                "    do_sample=True,\n",
                "    pad_token_id=generator.tokenizer.eos_token_id\n",
                ")[0]['generated_text']\n",
                "\n",
                "# Show only the new part\n",
                "continuation = generated[len(story_prompt):]\n",
                "print(continuation)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Temperature Effects on Generation\n",
                "\n",
                "The `temperature` parameter controls creativity:\n",
                "- **Low (0.1-0.5)**: More predictable, focused\n",
                "- **High (0.8-1.5)**: More creative, diverse"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸŒ¡ï¸ Temperature Effects on Text Generation\n",
                        "============================================================\n",
                        "\n",
                        "âœï¸ Prompt: \"The future of technology is\"\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸŒ¡ï¸ Temperature 0.3 (Conservative):\n",
                        "   The future of technology is not yet clear, but it is clear that the future of the Internet is not yet finished.\n",
                        "\n",
                        "The Internet is a great thing. It is a great way to connect people and create a better world. It is a great way to connect people and create a better world. It is a great way to connect people and create a better world.\n",
                        "\n",
                        "The Internet is a great thing. It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing.\n",
                        "\n",
                        "It is a great thing. It is a great thing. It is a great thing\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸŒ¡ï¸ Temperature 0.7 (Balanced):\n",
                        "   The future of technology is increasingly important, with the potential to revolutionise the way we live and work. Technology makes things better, cheaper and more efficient. As we move into the next decade, we can expect to see more and more connected devices, and more and more connected devices.\n",
                        "\n",
                        "With the advent of smart phones, we will be able to interact with each other, interact with the world around us and even communicate in ways that we can't normally do, such as by sending and receiving messages. The use of this technology has the potential to revolutionise the way we work and we are just beginning to see it happen.\n",
                        "\n",
                        "Smart phones have helped to democratise how we work, and have brought about the change we need. In light of the latest developments, we need to ensure that we are using smart phones in the right ways.\n",
                        "\n",
                        "The world is changing and we need to be making those changes fast. We need to be on the forefront of this revolution, and we need to be taking action now. We need to understand what's happening in the world today, and then we need to take action against it.\n",
                        "\n",
                        "Smart phones make it possible when you do things in the right way. They make it possible to be more productive, more connected, and more productive.\n",
                        "\n",
                        "ğŸŒ¡ï¸ Temperature 1.2 (Creative):\n",
                        "   The future of technology is constantly growing beyond today's smartphone market; it is poised to revolutionize how mobile phones deliver data, and to provide a reliable and convenient way in which consumers interact with information services. It could allow us to make a difference.\"\n",
                        "\n",
                        "Reeved also told TechCrunch, \"By bringing IoT into our homes and businesses, we gain a better understanding of what types and functions of information flow and service delivery services can provide. The next generation IoT technology will be a step at a time before a digital marketplace can become a reality and become as seamless as ever. In our conversations with SmartThings we mentioned the first three devices and the next.\" When asked whether IoT will be a disruptive factor for tech companies, Ellwood said, \"A lot of today's companies, big tech companies â€” from Silicon Valley to Hollywood studios and telecom companies â€“ just don't think we're just going to have to take a stand on this.\"\n",
                        "\n",
                        "When talking to TechCrunch when he left his job at IBM to start at WSU in 2015, Reese noted a number of things that took place on this trip. He said that he and Gartner developed an IBM Home Computer in January to make an office automation system using an IBM Smart Home, giving Dell employees all they could grab, and then\n"
                    ]
                }
            ],
            "source": [
                "prompt = \"The future of technology is\"\n",
                "\n",
                "print(\"ğŸŒ¡ï¸ Temperature Effects on Text Generation\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nâœï¸ Prompt: \\\"{prompt}\\\"\")\n",
                "\n",
                "temperatures = [0.3, 0.7, 1.2]\n",
                "labels = [\"Conservative\", \"Balanced\", \"Creative\"]\n",
                "\n",
                "for temp, label in zip(temperatures, labels):\n",
                "    generated = generator(\n",
                "        prompt,\n",
                "        max_length=60,\n",
                "        temperature=temp,\n",
                "        do_sample=True,\n",
                "        pad_token_id=generator.tokenizer.eos_token_id\n",
                "    )[0]['generated_text']\n",
                "    \n",
                "    print(f\"\\nğŸŒ¡ï¸ Temperature {temp} ({label}):\")\n",
                "    print(f\"   {generated}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 Product Description Generator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ›ï¸ Product Description Generator\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸ“¦ Introducing our new wireless headphones: the HVAC-F3.\n",
                        "\n",
                        "The HVAC-F3 is one of our most popular wireless headphones. It offers a low-power bandwidth and robust performance, making it ideal for casual listening and casual driving. Compared with the older HVAC-F3 models, the HVAC-F3 has a more spacious configuration, which makes it sound more spacious. With an integrated Bluetooth 4.0 interface, we can stream music from any Bluetooth headphones, including the HTC OneM9, HTC One, and the S9.\n",
                        "\n",
                        "The HVAC-F3's audio quality is excellent, with high definition sound reproduction that's balanced and balanced. The built-in mic and low-level volume controls are also great, saving you from being overwhelmed when playing music.\n",
                        "\n",
                        "The HVAC-F3's wireless headphones are rated for 30 dB of power, which is equivalent to a typical headphone of its class.\n",
                        "\n",
                        "Connectivity Features\n",
                        "\n",
                        "Each HVAC-F3 has a 12-pin power connector that plugs into any USB port available in the device. The USB port is located on the front of the device, and is connected directly to the front of your phone.\n",
                        "\n",
                        "HVAC-F3\n",
                        "----------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "ğŸ“¦ Experience our premium organic coffee: we make our own coffee and then buy it from our farmer's market. We sell our organic coffee in our local farmer's market, and we sell it to our customers for free. If you are a coffee aficionado, your choice will be the most important thing you can do. We are a global company with a diverse portfolio of products and services that are all available at our farm in New Zealand.\n",
                        "\n",
                        "All our coffees are produced in the USA, and our coffeehouses in Australia have been around for thousands of years. Our coffeehouse is in the heart of New Zealand's capital city and is the world's largest coffee production facility.\n",
                        "\n",
                        "Our coffeehouses are the result of a partnership between the British Government's Government Coffee Plant and the New Zealand Government and the New Zealand Government's Government Coffee Plant, with the objective of providing the world's largest coffee plant with the best quality coffee for export.\n",
                        "\n",
                        "The New Zealand Government's Government Coffee Plant has been in operation since 1999. In the first three years of operation, we have grown to a capacity of 1,858 coffees per hectare, and have been certified by two coffee suppliers to produce quality coffee. We have also been certified by three different countries to produce quality coffee from natural sources.\n",
                        "----------------------------------------\n",
                        "\n",
                        "ğŸ“¦ Discover the revolutionary fitness tracker:\n",
                        "\n",
                        "If you want to follow the progress of this great app, then sign up for our email list, and we'll send you a link here.\n",
                        "\n",
                        "And remember, you can also follow us on Facebook, Twitter, Tumblr and Google+.\n",
                        "----------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "product_prompts = [\n",
                "    \"Introducing our new wireless headphones:\",\n",
                "    \"Experience our premium organic coffee:\",\n",
                "    \"Discover the revolutionary fitness tracker:\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ›ï¸ Product Description Generator\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for prompt in product_prompts:\n",
                "    generated = generator(\n",
                "        prompt,\n",
                "        max_length=80,\n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "        pad_token_id=generator.tokenizer.eos_token_id\n",
                "    )[0]['generated_text']\n",
                "    \n",
                "    print(f\"\\nğŸ“¦ {generated}\")\n",
                "    print(\"-\" * 40)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8ï¸âƒ£ Question Answering\n",
                "\n",
                "Question Answering (QA) extracts answers from a given context. The model finds the most relevant span of text that answers the question.\n",
                "\n",
                "### 8.1 Travel Destination Q&A"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "16ee86097c4d4cab8a5f8890086174d0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cce46d4685334d3c9da3339bee14a87e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6ce1673527d6476787f719049aa2ca3d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a169a040ffc0431a8a9b803fcbf16d5e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "19a92e43fa9549a58d1cfc707d9a4886",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ—¾ Travel Q&A: Japan\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“– Context: Japan is an island country in East Asia, located in the Pacific Ocean. \n",
                        "Tokyo is the capital and largest city, with a population of over 13 million people. \n",
                        "The best time to visit Japan is during spri...\n",
                        "\n",
                        "â“ Questions & Answers:\n",
                        "----------------------------------------\n",
                        "\n",
                        "â“ What is the capital of Japan?\n",
                        "âœ… Tokyo (confidence: 99.6%)\n",
                        "\n",
                        "â“ When is the best time to visit Japan?\n",
                        "âœ… spring (March to May (confidence: 17.5%)\n",
                        "\n",
                        "â“ What is the currency in Japan?\n",
                        "âœ… Japanese Yen (confidence: 51.2%)\n",
                        "\n",
                        "â“ How tall is Mount Fuji?\n",
                        "âœ… 3,776 meters (confidence: 96.3%)\n",
                        "\n",
                        "â“ What food is Japan famous for?\n",
                        "âœ… sushi, ramen, and tempura (confidence: 57.4%)\n"
                    ]
                }
            ],
            "source": [
                "# Create QA pipeline\n",
                "qa_pipeline = pipeline(\n",
                "    \"question-answering\",\n",
                "    model=\"distilbert-base-cased-distilled-squad\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Travel destination context\n",
                "japan_context = \"\"\"\n",
                "Japan is an island country in East Asia, located in the Pacific Ocean. \n",
                "Tokyo is the capital and largest city, with a population of over 13 million people. \n",
                "The best time to visit Japan is during spring (March to May) for cherry blossoms \n",
                "or autumn (September to November) for colorful foliage. Japan is famous for its \n",
                "traditional cuisine including sushi, ramen, and tempura. The country has a rich \n",
                "cultural heritage with beautiful temples, shrines, and gardens. Mount Fuji, \n",
                "standing at 3,776 meters, is the highest mountain and a UNESCO World Heritage site. \n",
                "The currency used is the Japanese Yen (JPY).\n",
                "\"\"\"\n",
                "\n",
                "questions = [\n",
                "    \"What is the capital of Japan?\",\n",
                "    \"When is the best time to visit Japan?\",\n",
                "    \"What is the currency in Japan?\",\n",
                "    \"How tall is Mount Fuji?\",\n",
                "    \"What food is Japan famous for?\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ—¾ Travel Q&A: Japan\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ“– Context: {japan_context.strip()[:200]}...\")\n",
                "print(\"\\nâ“ Questions & Answers:\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for question in questions:\n",
                "    result = qa_pipeline(question=question, context=japan_context)\n",
                "    print(f\"\\nâ“ {question}\")\n",
                "    print(f\"âœ… {result['answer']} (confidence: {result['score']:.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2 Building a Simple FAQ Bot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– TechCorp FAQ Bot\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ‘¤ Customer: How long do I have to return a product?\n",
                        "ğŸ¤– Bot: 30 days\n",
                        "   (Confidence: 47.3%)\n",
                        "\n",
                        "ğŸ‘¤ Customer: How much is express shipping?\n",
                        "ğŸ¤– Bot: $15\n",
                        "   (Confidence: 75.6%)\n",
                        "\n",
                        "ğŸ‘¤ Customer: What does the warranty cover?\n",
                        "ğŸ¤– Bot: defects in materials and workmanship\n",
                        "   (Confidence: 95.2%)\n",
                        "\n",
                        "ğŸ‘¤ Customer: When is phone support available?\n",
                        "ğŸ¤– Bot: Monday through Friday\n",
                        "   (Confidence: 68.1%)\n"
                    ]
                }
            ],
            "source": [
                "# Company FAQ context\n",
                "faq_context = \"\"\"\n",
                "TechCorp Support FAQ:\n",
                "\n",
                "Return Policy: Customers can return products within 30 days of purchase for a full refund. \n",
                "Items must be in original packaging and unused condition. Electronics must include all accessories.\n",
                "\n",
                "Shipping: Standard shipping takes 5-7 business days. Express shipping is available for \n",
                "an additional $15 and takes 2-3 business days. Free shipping is offered on orders over $50.\n",
                "\n",
                "Warranty: All electronic products come with a 2-year manufacturer warranty covering \n",
                "defects in materials and workmanship. Accidental damage is not covered.\n",
                "\n",
                "Contact: Customer support is available 24/7 via live chat on our website. \n",
                "Phone support is available Monday through Friday, 9 AM to 6 PM EST at 1-800-TECHCORP.\n",
                "Email support typically responds within 24 hours at support@techcorp.example.\n",
                "\"\"\"\n",
                "\n",
                "def ask_faq(question):\n",
                "    \"\"\"Simple FAQ bot function\"\"\"\n",
                "    result = qa_pipeline(question=question, context=faq_context)\n",
                "    return result['answer'], result['score']\n",
                "\n",
                "print(\"ğŸ¤– TechCorp FAQ Bot\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "customer_questions = [\n",
                "    \"How long do I have to return a product?\",\n",
                "    \"How much is express shipping?\",\n",
                "    \"What does the warranty cover?\",\n",
                "    \"When is phone support available?\"\n",
                "]\n",
                "\n",
                "for q in customer_questions:\n",
                "    answer, confidence = ask_faq(q)\n",
                "    print(f\"\\nğŸ‘¤ Customer: {q}\")\n",
                "    print(f\"ğŸ¤– Bot: {answer}\")\n",
                "    print(f\"   (Confidence: {confidence:.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9ï¸âƒ£ Zero-shot Classification\n",
                "\n",
                "Zero-shot classification can categorize text into classes **without requiring training examples**. This is incredibly powerful for flexible categorization.\n",
                "\n",
                "### 9.1 Customer Feedback Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“‹ Customer Feedback Classification (Zero-shot)\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ·ï¸ Categories: complaint, praise, bug report, feature request, question\n",
                        "----------------------------------------\n",
                        "\n",
                        "ğŸ“ \"My package arrived damaged and the customer service was unhe...\"\n",
                        "   â†’ COMPLAINT (77.3%)\n",
                        "\n",
                        "ğŸ“ \"Absolutely love this product! It exceeded all my expectation...\"\n",
                        "   â†’ PRAISE (90.7%)\n",
                        "\n",
                        "ğŸ“ \"The app crashes every time I try to check out. Please fix th...\"\n",
                        "   â†’ COMPLAINT (63.0%)\n",
                        "\n",
                        "ğŸ“ \"Can you add a dark mode feature? The bright screen hurts my ...\"\n",
                        "   â†’ FEATURE REQUEST (45.3%)\n",
                        "\n",
                        "ğŸ“ \"Just wondering when the new collection will be available?\"\n",
                        "   â†’ QUESTION (74.6%)\n"
                    ]
                }
            ],
            "source": [
                "# Create zero-shot classifier\n",
                "zero_shot = pipeline(\n",
                "    \"zero-shot-classification\",\n",
                "    model=\"facebook/bart-large-mnli\",\n",
                "    device=device\n",
                ")\n",
                "\n",
                "# Customer feedback samples\n",
                "customer_feedback = [\n",
                "    \"My package arrived damaged and the customer service was unhelpful.\",\n",
                "    \"Absolutely love this product! It exceeded all my expectations.\",\n",
                "    \"The app crashes every time I try to check out. Please fix this bug.\",\n",
                "    \"Can you add a dark mode feature? The bright screen hurts my eyes.\",\n",
                "    \"Just wondering when the new collection will be available?\"\n",
                "]\n",
                "\n",
                "# Labels without any training!\n",
                "feedback_labels = [\"complaint\", \"praise\", \"bug report\", \"feature request\", \"question\"]\n",
                "\n",
                "print(\"ğŸ“‹ Customer Feedback Classification (Zero-shot)\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ·ï¸ Categories: {', '.join(feedback_labels)}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for feedback in customer_feedback:\n",
                "    result = zero_shot(feedback, feedback_labels)\n",
                "    top_label = result['labels'][0]\n",
                "    top_score = result['scores'][0]\n",
                "    \n",
                "    print(f\"\\nğŸ“ \\\"{feedback[:60]}...\\\"\" if len(feedback) > 60 else f\"\\nğŸ“ \\\"{feedback}\\\"\")\n",
                "    print(f\"   â†’ {top_label.upper()} ({top_score:.1%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9.2 Social Media Post Intent Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“± Social Media Intent Classification\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ’¬ Just finished an amazing 10K run! ğŸƒâ€â™‚ï¸ New personal best!\n",
                        "   Top 3 Intents:\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ personal achievement: 95.6%\n",
                        "    gratitude: 2.4%\n",
                        "    content promotion: 1.7%\n",
                        "\n",
                        "ğŸ’¬ Anyone else frustrated with the new update? It's so slow!\n",
                        "   Top 3 Intents:\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ expressing frustration: 98.3%\n",
                        "    content promotion: 0.7%\n",
                        "    sales/marketing: 0.6%\n",
                        "\n",
                        "ğŸ’¬ Check out my new tutorial on machine learning basics! Link in bio.\n",
                        "   Top 3 Intents:\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ content promotion: 33.2%\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆ expressing frustration: 23.4%\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆ personal achievement: 22.6%\n",
                        "\n",
                        "ğŸ’¬ Grateful for my amazing team. Couldn't have done it without you all! ğŸ™\n",
                        "   Top 3 Intents:\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ gratitude: 97.1%\n",
                        "    content promotion: 1.3%\n",
                        "    sales/marketing: 0.7%\n",
                        "\n",
                        "ğŸ’¬ Flash sale! 50% off all items this weekend only!\n",
                        "   Top 3 Intents:\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ sales/marketing: 74.7%\n",
                        "   â–ˆâ–ˆâ–ˆâ–ˆ content promotion: 20.3%\n",
                        "    personal achievement: 2.1%\n"
                    ]
                }
            ],
            "source": [
                "# Social media posts\n",
                "social_posts = [\n",
                "    \"Just finished an amazing 10K run! ğŸƒâ€â™‚ï¸ New personal best!\",\n",
                "    \"Anyone else frustrated with the new update? It's so slow!\",\n",
                "    \"Check out my new tutorial on machine learning basics! Link in bio.\",\n",
                "    \"Grateful for my amazing team. Couldn't have done it without you all! ğŸ™\",\n",
                "    \"Flash sale! 50% off all items this weekend only!\"\n",
                "]\n",
                "\n",
                "# Intent labels\n",
                "intent_labels = [\n",
                "    \"personal achievement\",\n",
                "    \"expressing frustration\", \n",
                "    \"content promotion\",\n",
                "    \"gratitude\",\n",
                "    \"sales/marketing\"\n",
                "]\n",
                "\n",
                "print(\"ğŸ“± Social Media Intent Classification\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for post in social_posts:\n",
                "    result = zero_shot(post, intent_labels)\n",
                "    \n",
                "    print(f\"\\nğŸ’¬ {post}\")\n",
                "    print(\"   Top 3 Intents:\")\n",
                "    for label, score in zip(result['labels'][:3], result['scores'][:3]):\n",
                "        bar = \"â–ˆ\" * int(score * 20)\n",
                "        print(f\"   {bar} {label}: {score:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9.3 Multi-label Classification\n",
                "\n",
                "Zero-shot can also handle cases where text belongs to **multiple categories**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ·ï¸ Multi-label Classification\n",
                        "============================================================\n",
                        "\n",
                        "ğŸ“° Article: Tech giant Apple announced today that it will invest $5 billion in green energy \n",
                        "initiatives to power its data centers with 100% renewable energy by 2025. \n",
                        "The company's stock rose 3% on the news, reaching an all-time high.\n",
                        "\n",
                        "ğŸ“Š Category Scores:\n",
                        "âœ… technology      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 98.9%\n",
                        "âœ… business        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 91.9%\n",
                        "âœ… environment     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 87.2%\n",
                        "   politics         0.2%\n",
                        "   sports           0.1%\n",
                        "\n",
                        "ğŸ·ï¸ Assigned labels: technology, business, environment\n"
                    ]
                }
            ],
            "source": [
                "# News article that could belong to multiple categories\n",
                "article = \"\"\"\n",
                "Tech giant Apple announced today that it will invest $5 billion in green energy \n",
                "initiatives to power its data centers with 100% renewable energy by 2025. \n",
                "The company's stock rose 3% on the news, reaching an all-time high.\n",
                "\"\"\"\n",
                "\n",
                "categories = [\"technology\", \"business\", \"environment\", \"politics\", \"sports\"]\n",
                "\n",
                "print(\"ğŸ·ï¸ Multi-label Classification\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"\\nğŸ“° Article: {article.strip()}\")\n",
                "\n",
                "# Get all scores\n",
                "result = zero_shot(article, categories, multi_label=True)\n",
                "\n",
                "print(\"\\nğŸ“Š Category Scores:\")\n",
                "for label, score in zip(result['labels'], result['scores']):\n",
                "    bar = \"â–ˆ\" * int(score * 30)\n",
                "    status = \"âœ…\" if score > 0.5 else \"  \"\n",
                "    print(f\"{status} {label:15} {bar} {score:.1%}\")\n",
                "\n",
                "# Show assigned labels\n",
                "assigned = [l for l, s in zip(result['labels'], result['scores']) if s > 0.5]\n",
                "print(f\"\\nğŸ·ï¸ Assigned labels: {', '.join(assigned)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ¯ Conclusion & Key Takeaways\n",
                "\n",
                "### Summary of NLP Tasks\n",
                "\n",
                "| Task | Use Case | Key Model |\n",
                "|------|----------|----------|\n",
                "| Text Classification | Sentiment, spam, topic | DistilBERT, BERT |\n",
                "| NER | Extract entities | BERT-large-NER |\n",
                "| Summarization | Condense text | BART, T5 |\n",
                "| Translation | Language conversion | Helsinki-NLP models |\n",
                "| Text Generation | Creative writing | GPT-2, GPT-Neo |\n",
                "| Question Answering | FAQ bots | DistilBERT-SQuAD |\n",
                "| Zero-shot | Flexible classification | BART-MNLI |\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. **Use `pipeline()` for Quick Prototypes**: It's the fastest way to get started\n",
                "2. **Batch Process When Possible**: Much faster than one-by-one\n",
                "3. **Choose Task-Specific Models**: Fine-tuned models outperform general ones\n",
                "4. **Consider Model Size**: Smaller models (DistilBERT) are faster, larger ones more accurate\n",
                "5. **Use GPU Acceleration**: Significant speedups for larger batches\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- ğŸ“š Explore the [Hugging Face Model Hub](https://huggingface.co/models)\n",
                "- ğŸ”§ Learn to fine-tune models on custom datasets\n",
                "- ğŸ–¼ï¸ Explore computer vision tasks with Hugging Face\n",
                "\n",
                "---\n",
                "\n",
                "**Happy NLP-ing! ğŸš€**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
