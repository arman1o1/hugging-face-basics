{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Chapter 12: Using LLMs to Query Your Local Data\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "1. **Private Data Querying**: How to use GPT4All to query your private documents while maintaining data privacy\n",
    "2. **Document Loading**: Loading and processing different file formats (PDF, CSV, JSON)\n",
    "3. **Text Chunking**: Breaking documents into manageable chunks for LLM processing\n",
    "4. **Vector Embeddings**: Using FAISS and sentence-transformers for semantic search\n",
    "5. **Local LLM Querying**: Building a Q&A system with local models\n",
    "6. **Code Generation for Analysis**: Using LLMs to generate Python code for data analytics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Introduction\n",
    "\n",
    "While cloud-based LLMs like OpenAI and Hugging Face Hub are powerful, **data privacy** is often a concern for businesses and developers. This chapter explores how to:\n",
    "\n",
    "- Run LLMs **locally** using GPT4All without sending data externally\n",
    "- Query **text-based data** (like PDFs) using vector embeddings\n",
    "- Handle **tabular data** (CSV, JSON) by generating analytical code\n",
    "\n",
    "> ğŸ’¡ **Key Insight**: LLMs excel at text-related questions but struggle with data aggregation. For tabular data, it's better to have the LLM generate code to analyze the data rather than trying to answer directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Part 1: Environment Setup\n",
    "\n",
    "### Installing Required Packages\n",
    "\n",
    "We need several packages for this chapter:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|--------|\n",
    "| `langchain` | Chain LLM components together |\n",
    "| `gpt4all` | Run models locally |\n",
    "| `faiss-cpu` | Efficient similarity search |\n",
    "| `sentence-transformers` | Text to vector embeddings |\n",
    "| `pypdf` | Parse PDF documents |\n",
    "| `jq` | Parse JSON files |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-core langchain-community langchain-huggingface langchain-text-splitters -q\n",
    "!pip install gpt4all -q\n",
    "!pip install faiss-cpu -q\n",
    "!pip install sentence-transformers -q\n",
    "!pip install pypdf -q\n",
    "!pip install jq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required modules\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports (updated for latest versions)\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, JSONLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“„ Part 2: Working with PDF Documents\n",
    "\n",
    "### 2.1 Understanding the RAG Pipeline\n",
    "\n",
    "To query local documents with an LLM, we use a **Retrieval-Augmented Generation (RAG)** approach:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        RAG Pipeline                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Load Document â†’ 2. Split into Chunks â†’ 3. Create Embeddings     â”‚\n",
    "â”‚                                                      â†“              â”‚\n",
    "â”‚  5. Generate Answer â† 4. Find Similar Chunks â† User Question        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.2 Creating Sample Documents\n",
    "\n",
    "Let's create some sample data for our experiments. We'll create a sample \"Company Handbook\" PDF simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample company handbook created!\n",
      "ğŸ“„ Document length: 2167 characters\n"
     ]
    }
   ],
   "source": [
    "# Create a sample data directory\n",
    "os.makedirs(\"./sample_data\", exist_ok=True)\n",
    "\n",
    "# For demonstration, we'll create a text file simulating document content\n",
    "# In practice, you would use actual PDF files\n",
    "\n",
    "company_handbook = \"\"\"\n",
    "# TechVision Inc. Employee Handbook 2024\n",
    "\n",
    "## Chapter 1: Company Overview\n",
    "\n",
    "TechVision Inc. was founded in 2015 in San Francisco, California. Our mission is to make\n",
    "artificial intelligence accessible to everyone. We currently employ over 500 people across\n",
    "12 countries.\n",
    "\n",
    "### Core Values\n",
    "- Innovation First: We encourage creative thinking and experimentation\n",
    "- Customer Success: Our customers' success is our success\n",
    "- Transparency: Open communication at all levels\n",
    "- Sustainability: Committed to environmental responsibility\n",
    "\n",
    "## Chapter 2: Working Hours and Leave Policy\n",
    "\n",
    "### Standard Working Hours\n",
    "Our standard working hours are 9:00 AM to 6:00 PM, Monday through Friday. We offer\n",
    "flexible working arrangements for employees who need them.\n",
    "\n",
    "### Paid Time Off (PTO)\n",
    "- Annual Leave: 20 days per year for all employees\n",
    "- Sick Leave: 10 days per year\n",
    "- Parental Leave: 16 weeks for primary caregivers, 8 weeks for secondary caregivers\n",
    "- Mental Health Days: 5 days per year\n",
    "\n",
    "### Holidays\n",
    "We observe 12 public holidays per year. Employees working on holidays receive \n",
    "double compensation or compensatory time off.\n",
    "\n",
    "## Chapter 3: Benefits and Compensation\n",
    "\n",
    "### Health Insurance\n",
    "We provide comprehensive health insurance covering:\n",
    "- Medical: 90% coverage for employees, 80% for dependents\n",
    "- Dental: Full coverage for preventive care\n",
    "- Vision: Annual eye exam and $300 frame allowance\n",
    "\n",
    "### Retirement Plan\n",
    "The company matches 401(k) contributions up to 6% of salary. Employees are fully\n",
    "vested after 3 years of service.\n",
    "\n",
    "### Education Benefits\n",
    "- Annual learning budget: $3,000 per employee\n",
    "- Conference attendance: Up to 2 conferences per year\n",
    "- Tuition reimbursement: Up to $10,000 per year for approved programs\n",
    "\n",
    "## Chapter 4: Remote Work Policy\n",
    "\n",
    "### Hybrid Work Model\n",
    "Employees can work remotely up to 3 days per week. Core collaboration hours are\n",
    "10:00 AM to 3:00 PM in the employee's local timezone.\n",
    "\n",
    "### Home Office Setup\n",
    "New employees receive a $1,500 home office stipend for equipment and furniture.\n",
    "Annual internet reimbursement of $50/month is provided.\n",
    "\n",
    "### International Remote Work\n",
    "Employees may work from abroad for up to 30 days per year with manager approval.\n",
    "\"\"\"\n",
    "\n",
    "# Save as a text file (in practice, this would be a PDF)\n",
    "with open(\"./sample_data/company_handbook.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(company_handbook)\n",
    "\n",
    "print(\"âœ… Sample company handbook created!\")\n",
    "print(f\"ğŸ“„ Document length: {len(company_handbook)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Chunking Explained\n",
    "\n",
    "**Why do we need chunking?**\n",
    "\n",
    "LLMs have a **context window limit** (e.g., 2,000-4,096 tokens). Large documents won't fit, so we must break them into smaller pieces.\n",
    "\n",
    "```\n",
    "Original Document (10,000 words)\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Chunk 1: Words 1-500    (overlap: 50)  â”‚ â† Each chunk fits in context window\n",
    "â”‚ Chunk 2: Words 450-950  (overlap: 50)  â”‚ â† Overlap preserves context\n",
    "â”‚ Chunk 3: Words 900-1400 (overlap: 50)  â”‚\n",
    "â”‚ ...                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- `chunk_size`: Maximum characters per chunk\n",
    "- `chunk_overlap`: Characters shared between consecutive chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Chunking Statistics:\n",
      "   Original document: 2167 characters\n",
      "   Number of chunks: 6\n",
      "\n",
      "ğŸ“ Sample Chunks:\n",
      "\n",
      "--- Chunk 1 (267 chars) ---\n",
      "# TechVision Inc. Employee Handbook 2024\n",
      "\n",
      "## Chapter 1: Company Overview\n",
      "\n",
      "TechVision Inc. was founded in 2015 in San Francisco, California. Our mission is to make\n",
      "artificial intelligence accessible to...\n",
      "\n",
      "--- Chunk 2 (469 chars) ---\n",
      "### Core Values\n",
      "- Innovation First: We encourage creative thinking and experimentation\n",
      "- Customer Success: Our customers' success is our success\n",
      "- Transparency: Open communication at all levels\n",
      "- Sust...\n",
      "\n",
      "--- Chunk 3 (407 chars) ---\n",
      "### Paid Time Off (PTO)\n",
      "- Annual Leave: 20 days per year for all employees\n",
      "- Sick Leave: 10 days per year\n",
      "- Parental Leave: 16 weeks for primary caregivers, 8 weeks for secondary caregivers\n",
      "- Mental H...\n"
     ]
    }
   ],
   "source": [
    "# Read our document\n",
    "with open(\"./sample_data/company_handbook.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document_text = f.read()\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # Each chunk will be ~500 characters\n",
    "    chunk_overlap=50,    # 50 characters overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split priority\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "print(f\"ğŸ“Š Chunking Statistics:\")\n",
    "print(f\"   Original document: {len(document_text)} characters\")\n",
    "print(f\"   Number of chunks: {len(chunks)}\")\n",
    "print(f\"\\nğŸ“ Sample Chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Understanding Embeddings\n",
    "\n",
    "**What are embeddings?**\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts have similar vectors.\n",
    "\n",
    "```\n",
    "\"I love programming\"  â†’  [0.23, -0.45, 0.89, ...]  (384 dimensions)\n",
    "\"Coding is enjoyable\" â†’  [0.21, -0.43, 0.91, ...]  â† Similar vectors!\n",
    "\"I hate vegetables\"   â†’  [-0.56, 0.78, -0.22, ...] â† Different vector\n",
    "```\n",
    "\n",
    "We use the `sentence-transformers/all-MiniLM-L6-v2` model which creates 384-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading embedding model (this may take a moment on first run)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb783af0ced4dd589247900772cd362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21145f2fcbd3471582bb640451670897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f0335946eb435eb164812e9d31ca17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52437b1bc9b74830b278f381a87e4e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7586e1c206d4548a02d11cd07ca25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480d17d3fc3b4682ab689d39fd0c7bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd3c0626ab64cc78abc9a6b0bec1e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68734a4f92fa425588b91f187b74f77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643b52ee648349aa9b500a5de13aca4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d5d5915c5c4e98bc1b3464fae22ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d097227e0f47b886b330a41594802e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding model loaded!\n",
      "\n",
      "ğŸ“ Embedding Examples:\n",
      "\n",
      "'How many vacation days do employees get?'\n",
      "   â†’ Vector dimensions: 384\n",
      "   â†’ First 5 values: [0.03040354885160923, 0.022176217287778854, 0.03267507255077362, 0.0539545863866806, -0.04998943954706192]\n",
      "\n",
      "'What is the annual leave policy?'\n",
      "   â†’ Vector dimensions: 384\n",
      "   â†’ First 5 values: [0.04520489275455475, 0.043770406395196915, 0.014666848815977573, 0.04851733148097992, 0.10103616863489151]\n",
      "\n",
      "'Tell me about the 401k retirement plan'\n",
      "   â†’ Vector dimensions: 384\n",
      "   â†’ First 5 values: [-0.006854142528027296, 0.15956076979637146, -0.09724941849708557, -0.0021323920227587223, 0.00961589440703392]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "print(\"ğŸ”„ Loading embedding model (this may take a moment on first run)...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    ")\n",
    "print(\"âœ… Embedding model loaded!\")\n",
    "\n",
    "# Demonstrate how embeddings work\n",
    "sample_texts = [\n",
    "    \"How many vacation days do employees get?\",\n",
    "    \"What is the annual leave policy?\",\n",
    "    \"Tell me about the 401k retirement plan\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ“ Embedding Examples:\")\n",
    "for text in sample_texts:\n",
    "    vector = embeddings.embed_query(text)\n",
    "    print(f\"\\n'{text}'\")\n",
    "    print(f\"   â†’ Vector dimensions: {len(vector)}\")\n",
    "    print(f\"   â†’ First 5 values: {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Creating a Vector Store with FAISS\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is a library for efficient similarity search. It indexes vectors for fast retrieval.\n",
    "\n",
    "```\n",
    "Query: \"What's the vacation policy?\"\n",
    "        â†“\n",
    "   [embed query]\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           FAISS Vector Store            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚ Chunk 1 vector â† Similarity: 0.23  â”‚ â”‚\n",
    "â”‚  â”‚ Chunk 2 vector â† Similarity: 0.91 âœ“â”‚ â”‚ â† Most similar!\n",
    "â”‚  â”‚ Chunk 3 vector â† Similarity: 0.45  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Creating FAISS index...\n",
      "âœ… FAISS index created!\n",
      "ğŸ’¾ Index saved to './sample_data/handbook_index'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Convert chunks to Document objects\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"company_handbook\"}) \n",
    "             for chunk in chunks]\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"ğŸ”„ Creating FAISS index...\")\n",
    "faiss_index = FAISS.from_documents(documents, embeddings)\n",
    "print(\"âœ… FAISS index created!\")\n",
    "\n",
    "# Save the index for later use\n",
    "faiss_index.save_local(\"./sample_data/handbook_index\")\n",
    "print(\"ğŸ’¾ Index saved to './sample_data/handbook_index'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'How many days of vacation do I get?'\n",
      "\n",
      "ğŸ“„ Top 3 Most Similar Chunks:\n",
      "============================================================\n",
      "\n",
      "[Match 1]\n",
      "### Paid Time Off (PTO)\n",
      "- Annual Leave: 20 days per year for all employees\n",
      "- Sick Leave: 10 days per year\n",
      "- Parental Leave: 16 weeks for primary caregivers, 8 weeks for secondary caregivers\n",
      "- Mental Health Days: 5 days per year\n",
      "\n",
      "### Holidays\n",
      "We observe 12 public holidays per year. Employees working on holidays receive \n",
      "double compensation or compensatory time off.\n",
      "\n",
      "## Chapter 3: Benefits and Compensation\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Match 2]\n",
      "### Education Benefits\n",
      "- Annual learning budget: $3,000 per employee\n",
      "- Conference attendance: Up to 2 conferences per year\n",
      "- Tuition reimbursement: Up to $10,000 per year for approved programs\n",
      "\n",
      "## Chapter 4: Remote Work Policy\n",
      "\n",
      "### Hybrid Work Model\n",
      "Employees can work remotely up to 3 days per week. Core collaboration hours are\n",
      "10:00 AM to 3:00 PM in the employee's local timezone.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Match 3]\n",
      "### Home Office Setup\n",
      "New employees receive a $1,500 home office stipend for equipment and furniture.\n",
      "Annual internet reimbursement of $50/month is provided.\n",
      "\n",
      "### International Remote Work\n",
      "Employees may work from abroad for up to 30 days per year with manager approval.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate similarity search\n",
    "query = \"How many days of vacation do I get?\"\n",
    "\n",
    "print(f\"ğŸ” Query: '{query}'\\n\")\n",
    "print(\"ğŸ“„ Top 3 Most Similar Chunks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "similar_docs = faiss_index.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(similar_docs, 1):\n",
    "    print(f\"\\n[Match {i}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Building the Q&A System with GPT4All\n",
    "\n",
    "Now we'll combine everything to create a complete Q&A system.\n",
    "\n",
    "> âš ï¸ **Note**: Running GPT4All requires downloading models (~4GB). The first run will download the model automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Available GPT4All Models:\n",
      "==================================================\n",
      "1. qwen2.5-coder-7b-instruct-q4_0.gguf\n",
      "   Parameters: 8 billion\n",
      "\n",
      "2. Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
      "   Parameters: 8 billion\n",
      "\n",
      "3. DeepSeek-R1-Distill-Qwen-7B-Q4_0.gguf\n",
      "   Parameters: 7 billion\n",
      "\n",
      "4. DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf\n",
      "   Parameters: 14 billion\n",
      "\n",
      "5. DeepSeek-R1-Distill-Llama-8B-Q4_0.gguf\n",
      "   Parameters: 8 billion\n",
      "\n",
      "6. DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\n",
      "   Parameters: 1.5 billion\n",
      "\n",
      "7. Llama-3.2-3B-Instruct-Q4_0.gguf\n",
      "   Parameters: 3 billion\n",
      "\n",
      "8. Llama-3.2-1B-Instruct-Q4_0.gguf\n",
      "   Parameters: 1 billion\n",
      "\n",
      "9. Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
      "   Parameters: 7 billion\n",
      "\n",
      "10. mistral-7b-instruct-v0.1.Q4_0.gguf\n",
      "   Parameters: 7 billion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's see available GPT4All models\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "print(\"ğŸ“‹ Available GPT4All Models:\")\n",
    "print(\"=\" * 50)\n",
    "models = GPT4All.list_models()\n",
    "\n",
    "# Show first 10 models\n",
    "for i, model in enumerate(models[:10]):\n",
    "    print(f\"{i+1}. {model['filename']}\")\n",
    "    print(f\"   Parameters: {model.get('parameters', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading GPT4All model (this may download ~4GB on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.11G/4.11G [00:49<00:00, 82.9MiB/s]\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model 'mistral-7b-openorca.Q4_0.gguf' loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "warning: model 'mistral-7b-openorca.Q4_0.gguf' is out-of-date, please check for an updated version\n"
     ]
    }
   ],
   "source": [
    "# Download and load a GPT4All model\n",
    "# Using a smaller model for faster inference\n",
    "print(\"ğŸ”„ Loading GPT4All model (this may download ~4GB on first run)...\")\n",
    "\n",
    "# Note: Change this to a model you have or want to download\n",
    "model_name = \"mistral-7b-openorca.Q4_0.gguf\"\n",
    "\n",
    "try:\n",
    "    # Download the model if not present\n",
    "    gpt4all_model = GPT4All(model_name)\n",
    "    print(f\"âœ… Model '{model_name}' loaded successfully!\")\n",
    "    MODEL_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not load model: {e}\")\n",
    "    print(\"\\nğŸ’¡ You can manually download models from: https://gpt4all.io/\")\n",
    "    MODEL_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Q&A function created!\n"
     ]
    }
   ],
   "source": [
    "# Create the Q&A function\n",
    "from langchain_community.llms import GPT4All as LangChainGPT4All\n",
    "\n",
    "# Create prompt template\n",
    "qa_template = \"\"\"\n",
    "You are a helpful HR assistant answering questions about the employee handbook.\n",
    "Use ONLY the provided context to answer. If the answer isn't in the context, \n",
    "say \"I don't have that information in the handbook.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "def ask_handbook_question(question, faiss_index, llm):\n",
    "    \"\"\"\n",
    "    Ask a question about the company handbook.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        faiss_index: The FAISS vector store\n",
    "        llm: The language model\n",
    "    \n",
    "    Returns:\n",
    "        The model's answer\n",
    "    \"\"\"\n",
    "    # Step 1: Find relevant chunks\n",
    "    relevant_docs = faiss_index.similarity_search(question, k=4)\n",
    "    \n",
    "    # Step 2: Combine chunks into context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Step 3: Create prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=qa_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ).partial(context=context)\n",
    "    \n",
    "    # Step 4: Create chain and invoke\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    return chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"âœ… Q&A function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: model 'mistral-7b-openorca.Q4_0.gguf' is out-of-date, please check for an updated version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testing Q&A System\n",
      "============================================================\n",
      "\n",
      "â“ Question: How many vacation days do employees get per year?\n",
      "ğŸ’¬ Answer:  Employees receive 20 days of annual leave per year.\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: What is the 401k matching policy?\n",
      "ğŸ’¬ Answer:  The company matches 401(k) contributions up to 6% of salary.\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: Can I work remotely from another country?\n",
      "ğŸ’¬ Answer:  Yes, you can work remotely from another country for up to 30 days per year with manager approval.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the Q&A system (only if model is available)\n",
    "if MODEL_AVAILABLE:\n",
    "    # Initialize LangChain GPT4All\n",
    "    llm = LangChainGPT4All(model=model_name)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"How many vacation days do employees get per year?\",\n",
    "        \"What is the 401k matching policy?\",\n",
    "        \"Can I work remotely from another country?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ¤– Testing Q&A System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nâ“ Question: {question}\")\n",
    "        answer = ask_handbook_question(question, faiss_index, llm)\n",
    "        print(f\"ğŸ’¬ Answer: {answer}\")\n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping Q&A test - model not available\")\n",
    "    print(\"\\nğŸ’¡ To test, download a GPT4All model and update the 'model_name' variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 3: Working with CSV Data\n",
    "\n",
    "### 3.1 Understanding CSV Limitations with LLMs\n",
    "\n",
    "> âš ï¸ **Important**: LLMs are NOT good at analyzing tabular data directly. They excel at text understanding but struggle with aggregations and calculations.\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "When you search for \"how many male passengers?\", the similarity search only returns a few matching rows (not all of them), leading to incorrect counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Sample Employee Data:\n",
      "employee_id         name  department  salary  hire_date      location\n",
      "       E001   Alice Chen Engineering   95000 2020-03-15 San Francisco\n",
      "       E002    Bob Smith   Marketing   72000 2019-07-22      New York\n",
      "       E003  Carol Davis Engineering  105000 2018-11-08 San Francisco\n",
      "       E004 David Wilson       Sales   68000 2021-02-14       Chicago\n",
      "       E005   Emma Brown Engineering   88000 2020-09-01        Austin\n",
      "       E006 Frank Miller   Marketing   75000 2022-01-10      New York\n",
      "       E007    Grace Lee          HR   62000 2019-05-30 San Francisco\n",
      "       E008 Henry Taylor       Sales   71000 2021-08-20       Chicago\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample employee data\n",
    "employee_data = {\n",
    "    'employee_id': ['E001', 'E002', 'E003', 'E004', 'E005', 'E006', 'E007', 'E008'],\n",
    "    'name': ['Alice Chen', 'Bob Smith', 'Carol Davis', 'David Wilson', \n",
    "             'Emma Brown', 'Frank Miller', 'Grace Lee', 'Henry Taylor'],\n",
    "    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales', \n",
    "                   'Engineering', 'Marketing', 'HR', 'Sales'],\n",
    "    'salary': [95000, 72000, 105000, 68000, 88000, 75000, 62000, 71000],\n",
    "    'hire_date': ['2020-03-15', '2019-07-22', '2018-11-08', '2021-02-14',\n",
    "                  '2020-09-01', '2022-01-10', '2019-05-30', '2021-08-20'],\n",
    "    'location': ['San Francisco', 'New York', 'San Francisco', 'Chicago',\n",
    "                 'Austin', 'New York', 'San Francisco', 'Chicago']\n",
    "}\n",
    "\n",
    "df_employees = pd.DataFrame(employee_data)\n",
    "df_employees.to_csv('./sample_data/employees.csv', index=False)\n",
    "\n",
    "print(\"ğŸ“Š Sample Employee Data:\")\n",
    "print(df_employees.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 8 document chunks from CSV\n",
      "\n",
      "ğŸ“‹ Sample document (first employee):\n",
      "employee_id: E001\n",
      "name: Alice Chen\n",
      "department: Engineering\n",
      "salary: 95000\n",
      "hire_date: 2020-03-15\n",
      "location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "# Load CSV using LangChain\n",
    "csv_loader = CSVLoader('./sample_data/employees.csv')\n",
    "csv_documents = csv_loader.load_and_split()\n",
    "\n",
    "print(f\"ğŸ“„ Loaded {len(csv_documents)} document chunks from CSV\")\n",
    "print(\"\\nğŸ“‹ Sample document (first employee):\")\n",
    "print(csv_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'Who works in Engineering department?'\n",
      "\n",
      "ğŸ“„ Results (Top 4 matches):\n",
      "\n",
      "[Match 1]\n",
      "employee_id: E001\n",
      "name: Alice Chen\n",
      "department: Engineering\n",
      "salary: 95000\n",
      "hire_date: 2020-03-15\n",
      "location: San Francisco\n",
      "\n",
      "[Match 2]\n",
      "employee_id: E003\n",
      "name: Carol Davis\n",
      "department: Engineering\n",
      "salary: 105000\n",
      "hire_date: 2018-11-08\n",
      "location: San Francisco\n",
      "\n",
      "[Match 3]\n",
      "employee_id: E005\n",
      "name: Emma Brown\n",
      "department: Engineering\n",
      "salary: 88000\n",
      "hire_date: 2020-09-01\n",
      "location: Austin\n",
      "\n",
      "[Match 4]\n",
      "employee_id: E007\n",
      "name: Grace Lee\n",
      "department: HR\n",
      "salary: 62000\n",
      "hire_date: 2019-05-30\n",
      "location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for CSV\n",
    "csv_texts = text_splitter.split_documents(csv_documents)\n",
    "csv_faiss_index = FAISS.from_documents(csv_texts, embeddings)\n",
    "\n",
    "# Search for engineering employees\n",
    "query = \"Who works in Engineering department?\"\n",
    "results = csv_faiss_index.similarity_search(query, k=4)\n",
    "\n",
    "print(f\"ğŸ” Query: '{query}'\")\n",
    "print(\"\\nğŸ“„ Results (Top 4 matches):\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n[Match {i}]\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The Problem with CSV Analysis\n",
    "\n",
    "Notice how similarity search only returns a **subset** of matching records. If we asked \"How many engineers are there?\", we'd get an incomplete answer.\n",
    "\n",
    "**Solution**: Use LLMs to **generate code** for data analysis instead of querying data directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‹ Part 4: Working with JSON Data\n",
    "\n",
    "JSON files are common for semi-structured data. LangChain's `JSONLoader` uses `jq` syntax to parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Product catalog created!\n",
      "{\n",
      "  \"products\": [\n",
      "    {\n",
      "      \"id\": \"LAPTOP-001\",\n",
      "      \"name\": \"ProBook Elite 15\",\n",
      "      \"category\": \"Laptops\",\n",
      "      \"price\": 1299.99,\n",
      "      \"specs\": {\n",
      "        \"processor\": \"Intel Core i7-12700H\",\n",
      "        \"ram\": \"16GB DDR5\",\n",
      "        \"storage\": \"512GB NVMe SSD\"\n",
      "      },\n",
      "      \"description\": \"Professional laptop with excellent performance for developers and creators.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"PHONE-001\",\n",
      "      \"name\": \"Galaxy Ultra X\",\n",
      "      \"category\": \"Smartphones\",\n",
      "      \"price\": 999.99,\n",
      "      \"specs\": {\n",
      "        \"display\": \"6.8 inch AMOLED\",\n",
      "        \"camera\": \"200MP main sensor\",\n",
      "        \"battery\": \"5000mAh\"\n",
      "      },\n",
      "      \"description\": \"Flagship smartphone with advanced camera system and all-day battery.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"HEADPHONES-001\",\n",
      "      \"name\": \"SoundMax Pro\",\n",
      "      \"category\": \"Audio\",\n",
      "      \"price\": 349.99,\n",
      "      \"specs\": {\n",
      "        \"type\": \"Over-ear wireless\",\n",
      "        \"noise_cancellation\": \"Active ANC\",\n",
      "        \"battery_life\": \"40 hours\"\n",
      "      },\n",
      "      \"description\": \"Premium wireless headphones with industry-leading noise cancellation.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"TABLET-001\",\n",
      "      \"name\": \"Slate Pro 12.9\",\n",
      "      \"category\": \"Tablets\",\n",
      "      \"price\": 899.99,\n",
      "      \"specs\": {\n",
      "        \"display\": \"12.9 inch Liquid Retina XDR\",\n",
      "        \"chip\": \"M2 Chip\",\n",
      "        \"storage\": \"256GB\"\n",
      "      },\n",
      "      \"description\": \"Powerful tablet for creative professionals with stunning display.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create sample JSON data - a product catalog\n",
    "product_catalog = {\n",
    "    \"products\": [\n",
    "        {\n",
    "            \"id\": \"LAPTOP-001\",\n",
    "            \"name\": \"ProBook Elite 15\",\n",
    "            \"category\": \"Laptops\",\n",
    "            \"price\": 1299.99,\n",
    "            \"specs\": {\n",
    "                \"processor\": \"Intel Core i7-12700H\",\n",
    "                \"ram\": \"16GB DDR5\",\n",
    "                \"storage\": \"512GB NVMe SSD\"\n",
    "            },\n",
    "            \"description\": \"Professional laptop with excellent performance for developers and creators.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"PHONE-001\",\n",
    "            \"name\": \"Galaxy Ultra X\",\n",
    "            \"category\": \"Smartphones\",\n",
    "            \"price\": 999.99,\n",
    "            \"specs\": {\n",
    "                \"display\": \"6.8 inch AMOLED\",\n",
    "                \"camera\": \"200MP main sensor\",\n",
    "                \"battery\": \"5000mAh\"\n",
    "            },\n",
    "            \"description\": \"Flagship smartphone with advanced camera system and all-day battery.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"HEADPHONES-001\",\n",
    "            \"name\": \"SoundMax Pro\",\n",
    "            \"category\": \"Audio\",\n",
    "            \"price\": 349.99,\n",
    "            \"specs\": {\n",
    "                \"type\": \"Over-ear wireless\",\n",
    "                \"noise_cancellation\": \"Active ANC\",\n",
    "                \"battery_life\": \"40 hours\"\n",
    "            },\n",
    "            \"description\": \"Premium wireless headphones with industry-leading noise cancellation.\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"TABLET-001\",\n",
    "            \"name\": \"Slate Pro 12.9\",\n",
    "            \"category\": \"Tablets\",\n",
    "            \"price\": 899.99,\n",
    "            \"specs\": {\n",
    "                \"display\": \"12.9 inch Liquid Retina XDR\",\n",
    "                \"chip\": \"M2 Chip\",\n",
    "                \"storage\": \"256GB\"\n",
    "            },\n",
    "            \"description\": \"Powerful tablet for creative professionals with stunning display.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save JSON file\n",
    "with open('./sample_data/products.json', 'w') as f:\n",
    "    json.dump(product_catalog, f, indent=2)\n",
    "\n",
    "print(\"âœ… Product catalog created!\")\n",
    "print(json.dumps(product_catalog, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Understanding jq Schema Syntax:\n",
      "==================================================\n",
      "\n",
      "jq_schema='.products[]' breaks down as:\n",
      "\n",
      "    .           â†’ Start at root of document\n",
      "    products    â†’ Access the 'products' key\n",
      "    []          â†’ Iterate over array elements\n",
      "\n",
      "This extracts each product as a separate document.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Understanding jq schema for JSON parsing\n",
    "# .products[] means: access the \"products\" key and iterate over each element\n",
    "\n",
    "print(\"ğŸ“– Understanding jq Schema Syntax:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "jq_schema='.products[]' breaks down as:\n",
    "\n",
    "    .           â†’ Start at root of document\n",
    "    products    â†’ Access the 'products' key\n",
    "    []          â†’ Iterate over array elements\n",
    "\n",
    "This extracts each product as a separate document.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loaded 4 documents from JSON\n",
      "\n",
      "ğŸ“‹ First product document:\n",
      "{\"id\": \"LAPTOP-001\", \"name\": \"ProBook Elite 15\", \"category\": \"Laptops\", \"price\": 1299.99, \"specs\": {\"processor\": \"Intel Core i7-12700H\", \"ram\": \"16GB DDR5\", \"storage\": \"512GB NVMe SSD\"}, \"description\": \"Professional laptop with excellent performance for developers and creators.\"}\n"
     ]
    }
   ],
   "source": [
    "# Load JSON with LangChain\n",
    "json_loader = JSONLoader(\n",
    "    file_path='./sample_data/products.json',\n",
    "    jq_schema='.products[]',\n",
    "    text_content=False  # Load as structured data, not just text\n",
    ")\n",
    "\n",
    "json_documents = json_loader.load_and_split()\n",
    "\n",
    "print(f\"ğŸ“„ Loaded {len(json_documents)} documents from JSON\")\n",
    "print(\"\\nğŸ“‹ First product document:\")\n",
    "print(json_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Product Catalog Queries:\n",
      "============================================================\n",
      "\n",
      "â“ Query: Which products have noise cancellation?\n",
      "   ğŸ“¦ {\"id\": \"HEADPHONES-001\", \"name\": \"SoundMax Pro\", \"category\": \"Audio\", \"price\": 349.99, \"specs\": {\"ty...\n",
      "   ğŸ“¦ {\"id\": \"PHONE-001\", \"name\": \"Galaxy Ultra X\", \"category\": \"Smartphones\", \"price\": 999.99, \"specs\": {...\n",
      "\n",
      "â“ Query: What laptop do you recommend for developers?\n",
      "   ğŸ“¦ {\"id\": \"LAPTOP-001\", \"name\": \"ProBook Elite 15\", \"category\": \"Laptops\", \"price\": 1299.99, \"specs\": {...\n",
      "   ğŸ“¦ {\"id\": \"TABLET-001\", \"name\": \"Slate Pro 12.9\", \"category\": \"Tablets\", \"price\": 899.99, \"specs\": {\"di...\n",
      "\n",
      "â“ Query: Show me products under $500\n",
      "   ğŸ“¦ {\"id\": \"LAPTOP-001\", \"name\": \"ProBook Elite 15\", \"category\": \"Laptops\", \"price\": 1299.99, \"specs\": {...\n",
      "   ğŸ“¦ {\"id\": \"TABLET-001\", \"name\": \"Slate Pro 12.9\", \"category\": \"Tablets\", \"price\": 899.99, \"specs\": {\"di...\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index for products\n",
    "json_texts = text_splitter.split_documents(json_documents)\n",
    "product_faiss_index = FAISS.from_documents(json_texts, embeddings)\n",
    "\n",
    "# Query the product catalog\n",
    "queries = [\n",
    "    \"Which products have noise cancellation?\",\n",
    "    \"What laptop do you recommend for developers?\",\n",
    "    \"Show me products under $500\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Product Catalog Queries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nâ“ Query: {query}\")\n",
    "    results = product_faiss_index.similarity_search(query, k=2)\n",
    "    for doc in results:\n",
    "        # Extract product name from content\n",
    "        print(f\"   ğŸ“¦ {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§® Part 5: Using LLMs to Generate Analysis Code\n",
    "\n",
    "### The Best Approach for Tabular Data\n",
    "\n",
    "Instead of asking LLMs to analyze data directly, we ask them to **generate Python code** that we then execute.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Traditional Approach (Problematic)                             â”‚\n",
    "â”‚  User Question â†’ LLM â†’ Wrong Answer (limited context)           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Code Generation Approach (Better)                              â”‚\n",
    "â”‚  User Question â†’ LLM â†’ Python Code â†’ Execute â†’ Correct Answer   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Sales Data Sample (first 10 rows):\n",
      " sale_id salesperson region  product  quantity  unit_price       date   total\n",
      "SALE-001       Frank   East Widget B         2       99.61 2024-03-16  199.22\n",
      "SALE-002       David   East Widget A         3      198.45 2024-03-25  595.35\n",
      "SALE-003       Alice   West Widget B        20       52.46 2024-03-12 1049.20\n",
      "SALE-004       David  South Gadget Y        20       66.37 2024-02-08 1327.40\n",
      "SALE-005       Alice   East Widget A        14      148.64 2024-02-15 2080.96\n",
      "SALE-006       Grace   West Widget A         6      173.58 2024-01-12 1041.48\n",
      "SALE-007       Alice  South Gadget X        14       69.23 2024-01-30  969.22\n",
      "SALE-008       Grace   East Gadget Y        17      133.28 2024-01-07 2265.76\n",
      "SALE-009       Alice  South Widget B         7      168.54 2024-03-14 1179.78\n",
      "SALE-010       David  South Gadget X         7       82.82 2024-03-06  579.74\n",
      "\n",
      "ğŸ“ˆ Total records: 50\n"
     ]
    }
   ],
   "source": [
    "# Create a more interesting dataset for analysis\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample sales data\n",
    "np_names = ['Alice', 'Bob', 'Carol', 'David', 'Emma', 'Frank', 'Grace', 'Henry']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "products = ['Widget A', 'Widget B', 'Gadget X', 'Gadget Y']\n",
    "\n",
    "sales_data = []\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "for i in range(50):\n",
    "    sale = {\n",
    "        'sale_id': f'SALE-{i+1:03d}',\n",
    "        'salesperson': random.choice(np_names),\n",
    "        'region': random.choice(regions),\n",
    "        'product': random.choice(products),\n",
    "        'quantity': random.randint(1, 20),\n",
    "        'unit_price': round(random.uniform(50, 200), 2),\n",
    "        'date': (base_date + timedelta(days=random.randint(0, 90))).strftime('%Y-%m-%d')\n",
    "    }\n",
    "    sale['total'] = round(sale['quantity'] * sale['unit_price'], 2)\n",
    "    sales_data.append(sale)\n",
    "\n",
    "df_sales = pd.DataFrame(sales_data)\n",
    "df_sales.to_csv('./sample_data/sales_data.csv', index=False)\n",
    "\n",
    "print(\"ğŸ“Š Sales Data Sample (first 10 rows):\")\n",
    "print(df_sales.head(10).to_string(index=False))\n",
    "print(f\"\\nğŸ“ˆ Total records: {len(df_sales)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Code generation template created!\n"
     ]
    }
   ],
   "source": [
    "# Create the code generation prompt\n",
    "code_gen_template = \"\"\"\n",
    "You are a Python data analyst. Given the schema of a pandas DataFrame, \n",
    "generate a SINGLE Python statement that answers the user's question.\n",
    "\n",
    "DataFrame Schema:\n",
    "- sale_id: Unique identifier (string)\n",
    "- salesperson: Name of salesperson (string)\n",
    "- region: Sales region - North, South, East, West (string)\n",
    "- product: Product name - Widget A, Widget B, Gadget X, Gadget Y (string)\n",
    "- quantity: Number of units sold (integer)\n",
    "- unit_price: Price per unit (float)\n",
    "- total: Total sale amount (float)\n",
    "- date: Sale date in YYYY-MM-DD format (string)\n",
    "\n",
    "The DataFrame is loaded as 'df'. Return ONLY the Python code, no explanations.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Python code:\"\"\"\n",
    "\n",
    "print(\"âœ… Code generation template created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® LLM-Generated Code Analysis Examples:\n",
      "============================================================\n",
      "\n",
      "â“ Question: What is the total sales amount?\n",
      "ğŸ Generated Code: df['total'].sum()\n",
      "ğŸ“Š Result:\n",
      "76266.76\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: Who is the top salesperson by revenue?\n",
      "ğŸ Generated Code: df.groupby('salesperson')['total'].sum().idxmax()\n",
      "ğŸ“Š Result:\n",
      "Grace\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: Show me sales by region\n",
      "ğŸ Generated Code: df.groupby('region')['total'].sum()\n",
      "ğŸ“Š Result:\n",
      "region\n",
      "East     23454.21\n",
      "North    12160.94\n",
      "South    22711.70\n",
      "West     17939.91\n",
      "Name: total, dtype: float64\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: What is the average sale amount?\n",
      "ğŸ Generated Code: df['total'].mean()\n",
      "ğŸ“Š Result:\n",
      "1525.3352\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: What is the best selling product?\n",
      "ğŸ Generated Code: df.describe()\n",
      "ğŸ“Š Result:\n",
      "        quantity  unit_price        total\n",
      "count  50.000000   50.000000    50.000000\n",
      "mean   11.540000  130.779800  1525.335200\n",
      "std     5.496604   49.504208   991.897909\n",
      "min     2.000000   51.010000   156.870000\n",
      "25%     7.000000   84.755000   715.380000\n",
      "50%    10.000000  135.040000  1211.565000\n",
      "75%    17.000000  173.317500  2180.372500\n",
      "max    20.000000  198.570000  3761.600000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to demonstrate what the LLM might generate\n",
    "# (In practice, you'd use the actual LLM)\n",
    "\n",
    "def simulate_code_generation(question):\n",
    "    \"\"\"\n",
    "    Simulates LLM code generation for common questions.\n",
    "    In production, this would call the actual LLM.\n",
    "    \"\"\"\n",
    "    # Map of common questions to generated pandas code\n",
    "    code_mappings = {\n",
    "        \"total sales\": \"df['total'].sum()\",\n",
    "        \"top salesperson\": \"df.groupby('salesperson')['total'].sum().idxmax()\",\n",
    "        \"sales by region\": \"df.groupby('region')['total'].sum()\",\n",
    "        \"average sale\": \"df['total'].mean()\",\n",
    "        \"best product\": \"df.groupby('product')['quantity'].sum().idxmax()\",\n",
    "        \"sales count\": \"len(df)\"\n",
    "    }\n",
    "    \n",
    "    for keyword, code in code_mappings.items():\n",
    "        if keyword in question.lower():\n",
    "            return code\n",
    "    \n",
    "    return \"df.describe()\"\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('./sample_data/sales_data.csv')\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What is the total sales amount?\",\n",
    "    \"Who is the top salesperson by revenue?\",\n",
    "    \"Show me sales by region\",\n",
    "    \"What is the average sale amount?\",\n",
    "    \"What is the best selling product?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§® LLM-Generated Code Analysis Examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    code = simulate_code_generation(question)\n",
    "    result = eval(code)\n",
    "    \n",
    "    print(f\"\\nâ“ Question: {question}\")\n",
    "    print(f\"ğŸ Generated Code: {code}\")\n",
    "    print(f\"ğŸ“Š Result:\")\n",
    "    print(result)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Using a Real LLM for Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Code generation function created!\n",
      "\n",
      "ğŸ’¡ Usage (when model is available):\n",
      "   code = generate_analysis_code('What is the total sales?', llm)\n",
      "   result = eval(code)\n"
     ]
    }
   ],
   "source": [
    "# Complete code generation function using GPT4All\n",
    "def generate_analysis_code(question, llm):\n",
    "    \"\"\"\n",
    "    Use an LLM to generate pandas code for data analysis.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question about the data\n",
    "        llm: The language model instance\n",
    "    \n",
    "    Returns:\n",
    "        Generated Python code string\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=code_gen_template,\n",
    "        input_variables=[\"question\"]\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    \n",
    "    # Clean the response - extract just the code\n",
    "    code = response.strip()\n",
    "    \n",
    "    # Remove markdown code blocks if present\n",
    "    if code.startswith(\"```python\"):\n",
    "        code = code[10:]\n",
    "    if code.startswith(\"```\"):\n",
    "        code = code[3:]\n",
    "    if code.endswith(\"```\"):\n",
    "        code = code[:-3]\n",
    "    \n",
    "    return code.strip()\n",
    "\n",
    "print(\"âœ… Code generation function created!\")\n",
    "print(\"\\nğŸ’¡ Usage (when model is available):\")\n",
    "print(\"   code = generate_analysis_code('What is the total sales?', llm)\")\n",
    "print(\"   result = eval(code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: model 'mistral-7b-openorca.Q4_0.gguf' is out-of-date, please check for an updated version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® Testing LLM Code Generation\n",
      "============================================================\n",
      "\n",
      "â“ Question: What is the total sales amount?\n",
      "ğŸ Generated Code: total_sales = df['total'].sum()\n",
      "print(total_sales)\n",
      "76266.76\n",
      "ğŸ“Š Result: 76266.76\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: Who is the top salesperson by revenue?\n",
      "ğŸ Generated Code: top_salesperson = df[df['total'].idxmax()]\n",
      "print(f\"Top salesperson by revenue: {top_salesperson['salesperson']}\")\n",
      "âš ï¸ Execution error: 34\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: How many sales were made in the North region?\n",
      "ğŸ Generated Code: len(df[df['region'] == 'North']['sale_id'].unique())\n",
      "ğŸ“Š Result: 8\n",
      "------------------------------------------------------------\n",
      "\n",
      "â“ Question: What is the average quantity sold per sale?\n",
      "ğŸ Generated Code: round(df['quantity'].mean(), 2)\n",
      "ğŸ“Š Result: 11.54\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the code generation function with GPT4All\n",
    "if MODEL_AVAILABLE:\n",
    "    # Initialize the LangChain GPT4All wrapper\n",
    "    from langchain_community.llms import GPT4All as LangChainGPT4All\n",
    "    llm = LangChainGPT4All(model=model_name)\n",
    "    \n",
    "    # Load the sales DataFrame\n",
    "    df = pd.read_csv('./sample_data/sales_data.csv')\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        \"What is the total sales amount?\",\n",
    "        \"Who is the top salesperson by revenue?\",\n",
    "        \"How many sales were made in the North region?\",\n",
    "        \"What is the average quantity sold per sale?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ§® Testing LLM Code Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nâ“ Question: {question}\")\n",
    "        \n",
    "        try:\n",
    "            # Generate code using LLM\n",
    "            generated_code = generate_analysis_code(question, llm)\n",
    "            print(f\"ğŸ Generated Code: {generated_code}\")\n",
    "            \n",
    "            # Try to execute the generated code\n",
    "            try:\n",
    "                # First try eval() for single expressions\n",
    "                result = eval(generated_code)\n",
    "                print(f\"ğŸ“Š Result: {result}\")\n",
    "            except SyntaxError:\n",
    "                # If multi-line code, use exec() instead\n",
    "                try:\n",
    "                    local_vars = {'df': df}\n",
    "                    exec(generated_code, globals(), local_vars)\n",
    "                    # Try to get the result from local variables\n",
    "                    for var_name in ['result', 'total_sales', 'answer', 'output']:\n",
    "                        if var_name in local_vars:\n",
    "                            print(f\"ğŸ“Š Result: {local_vars[var_name]}\")\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"ğŸ“Š Code executed (check output above)\")\n",
    "                except Exception as exec_err:\n",
    "                    print(f\"âš ï¸ Execution error: {exec_err}\")\n",
    "            except Exception as eval_error:\n",
    "                print(f\"âš ï¸ Eval error: {eval_error}\")\n",
    "                \n",
    "        except Exception as gen_error:\n",
    "            print(f\"âš ï¸ Generation error: {gen_error}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"âš ï¸ Model not available. Skipping code generation test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ Part 6: Putting It All Together\n",
    "\n",
    "### Complete RAG Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LocalDocumentQA class created!\n"
     ]
    }
   ],
   "source": [
    "class LocalDocumentQA:\n",
    "    \"\"\"\n",
    "    A complete RAG pipeline for querying local documents with privacy.\n",
    "    \n",
    "    Supports:\n",
    "    - PDF documents\n",
    "    - CSV files\n",
    "    - JSON files\n",
    "    - Text files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        \"\"\"Initialize the QA system with embedding model.\"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1024,\n",
    "            chunk_overlap=64\n",
    "        )\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        \n",
    "    def add_pdf(self, file_path):\n",
    "        \"\"\"Add a PDF document to the knowledge base.\"\"\"\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load_and_split()\n",
    "        self._process_documents(docs)\n",
    "        print(f\"âœ… Added PDF: {file_path}\")\n",
    "        \n",
    "    def add_csv(self, file_path):\n",
    "        \"\"\"Add a CSV file to the knowledge base.\"\"\"\n",
    "        loader = CSVLoader(file_path)\n",
    "        docs = loader.load_and_split()\n",
    "        self._process_documents(docs)\n",
    "        print(f\"âœ… Added CSV: {file_path}\")\n",
    "        \n",
    "    def add_json(self, file_path, jq_schema):\n",
    "        \"\"\"Add a JSON file to the knowledge base.\"\"\"\n",
    "        loader = JSONLoader(file_path, jq_schema=jq_schema, text_content=False)\n",
    "        docs = loader.load_and_split()\n",
    "        self._process_documents(docs)\n",
    "        print(f\"âœ… Added JSON: {file_path}\")\n",
    "        \n",
    "    def add_text(self, text, metadata=None):\n",
    "        \"\"\"Add raw text to the knowledge base.\"\"\"\n",
    "        from langchain_core.documents import Document  # Fixed import\n",
    "        doc = Document(page_content=text, metadata=metadata or {})\n",
    "        self._process_documents([doc])\n",
    "        print(f\"âœ… Added text ({len(text)} characters)\")\n",
    "        \n",
    "    def _process_documents(self, docs):\n",
    "        \"\"\"Split documents and add to index.\"\"\"\n",
    "        chunks = self.text_splitter.split_documents(docs)\n",
    "        self.documents.extend(chunks)\n",
    "        \n",
    "        # Rebuild index\n",
    "        self.index = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        \n",
    "    def search(self, query, k=4):\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        if self.index is None:\n",
    "            return []\n",
    "        return self.index.similarity_search(query, k=k)\n",
    "    \n",
    "    def save_index(self, path):\n",
    "        \"\"\"Save the FAISS index to disk.\"\"\"\n",
    "        if self.index:\n",
    "            self.index.save_local(path)\n",
    "            print(f\"ğŸ’¾ Index saved to: {path}\")\n",
    "            \n",
    "    def load_index(self, path):\n",
    "        \"\"\"Load a FAISS index from disk.\"\"\"\n",
    "        self.index = FAISS.load_local(\n",
    "            path, \n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"ğŸ“‚ Index loaded from: {path}\")\n",
    "\n",
    "print(\"âœ… LocalDocumentQA class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Complete RAG Pipeline Demo\n",
      "============================================================\n",
      "âœ… Added text (2167 characters)\n",
      "âœ… Added CSV: ./sample_data/employees.csv\n",
      "âœ… Added JSON: ./sample_data/products.json\n",
      "ğŸ’¾ Index saved to: ./sample_data/combined_index\n",
      "\n",
      "ğŸ“Š Total documents indexed: 15\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the complete pipeline\n",
    "print(\"ğŸš€ Complete RAG Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize the QA system\n",
    "qa_system = LocalDocumentQA()\n",
    "\n",
    "# Add various document types\n",
    "qa_system.add_text(company_handbook, metadata={\"source\": \"company_handbook\"})\n",
    "qa_system.add_csv('./sample_data/employees.csv')\n",
    "qa_system.add_json('./sample_data/products.json', jq_schema='.products[]')\n",
    "\n",
    "# Save the index\n",
    "qa_system.save_index('./sample_data/combined_index')\n",
    "\n",
    "print(f\"\\nğŸ“Š Total documents indexed: {len(qa_system.documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Testing Combined Knowledge Base:\n",
      "============================================================\n",
      "\n",
      "â“ Query: How many vacation days do employees get?\n",
      "   [1] ### Holidays\n",
      "We observe 12 public holidays per year. Employees working on holidays receive \n",
      "double compensation or compensatory time off.\n",
      "\n",
      "## Chapter ...\n",
      "   [2] # TechVision Inc. Employee Handbook 2024\n",
      "\n",
      "## Chapter 1: Company Overview\n",
      "\n",
      "TechVision Inc. was founded in 2015 in San Francisco, California. Our missio...\n",
      "\n",
      "â“ Query: Who works in the Engineering department?\n",
      "   [1] employee_id: E001\n",
      "name: Alice Chen\n",
      "department: Engineering\n",
      "salary: 95000\n",
      "hire_date: 2020-03-15\n",
      "location: San Francisco...\n",
      "   [2] employee_id: E003\n",
      "name: Carol Davis\n",
      "department: Engineering\n",
      "salary: 105000\n",
      "hire_date: 2018-11-08\n",
      "location: San Francisco...\n",
      "\n",
      "â“ Query: What products have good noise cancellation?\n",
      "   [1] {\"id\": \"HEADPHONES-001\", \"name\": \"SoundMax Pro\", \"category\": \"Audio\", \"price\": 349.99, \"specs\": {\"type\": \"Over-ear wireless\", \"noise_cancellation\": \"A...\n",
      "   [2] {\"id\": \"PHONE-001\", \"name\": \"Galaxy Ultra X\", \"category\": \"Smartphones\", \"price\": 999.99, \"specs\": {\"display\": \"6.8 inch AMOLED\", \"camera\": \"200MP mai...\n",
      "\n",
      "â“ Query: What is the 401k matching policy?\n",
      "   [1] ### Holidays\n",
      "We observe 12 public holidays per year. Employees working on holidays receive \n",
      "double compensation or compensatory time off.\n",
      "\n",
      "## Chapter ...\n",
      "   [2] # TechVision Inc. Employee Handbook 2024\n",
      "\n",
      "## Chapter 1: Company Overview\n",
      "\n",
      "TechVision Inc. was founded in 2015 in San Francisco, California. Our missio...\n"
     ]
    }
   ],
   "source": [
    "# Test various queries\n",
    "test_queries = [\n",
    "    \"How many vacation days do employees get?\",\n",
    "    \"Who works in the Engineering department?\",\n",
    "    \"What products have good noise cancellation?\",\n",
    "    \"What is the 401k matching policy?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Testing Combined Knowledge Base:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nâ“ Query: {query}\")\n",
    "    results = qa_system.search(query, k=2)\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"   [{i}] {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Privacy-First Approach**\n",
    "   - Use GPT4All to run models locally without sending data externally\n",
    "   - Embeddings are stored locally in FAISS indexes\n",
    "\n",
    "2. **Document Processing Pipeline**\n",
    "   - **Load**: Use appropriate loaders (PyPDFLoader, CSVLoader, JSONLoader)\n",
    "   - **Chunk**: Split documents into manageable pieces with overlap\n",
    "   - **Embed**: Convert text to vectors for semantic search\n",
    "   - **Index**: Store vectors in FAISS for fast retrieval\n",
    "\n",
    "3. **Text vs Tabular Data**\n",
    "   | Data Type | Best Approach |\n",
    "   |-----------|---------------|\n",
    "   | Text (PDF) | Direct RAG querying |\n",
    "   | Tabular (CSV, JSON) | LLM generates analysis code |\n",
    "\n",
    "4. **When to Use What**\n",
    "   - âœ… **RAG for**: Document Q&A, finding specific information, text understanding\n",
    "   - âœ… **Code generation for**: Aggregations, statistics, data analysis\n",
    "\n",
    "### ğŸ¯ Practice Exercises\n",
    "\n",
    "1. Add your own PDF documents and query them\n",
    "2. Create a customer support FAQ system using this approach\n",
    "3. Experiment with different chunk sizes and observe the impact\n",
    "4. Try using different embedding models from Hugging Face\n",
    "\n",
    "### ğŸ“š Additional Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [GPT4All Models](https://gpt4all.io/)\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss)\n",
    "- [Sentence Transformers](https://www.sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary files (optional)\n",
    "import shutil\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Remove sample data directory.\"\"\"\n",
    "    if os.path.exists('./sample_data'):\n",
    "        shutil.rmtree('./sample_data')\n",
    "        print(\"ğŸ§¹ Sample data cleaned up!\")\n",
    "\n",
    "# # Uncomment to clean up:\n",
    "# cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
