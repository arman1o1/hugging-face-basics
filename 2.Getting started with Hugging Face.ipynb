{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ Chapter 2: Getting Started with Hugging Face\n",
                "\n",
                "This notebook will guide you through setting up your environment for working with Hugging Face, understanding GPU acceleration, and interacting programmatically with the Hugging Face Hub.\n",
                "\n",
                "---\n",
                "\n",
                "## üìö What You'll Learn\n",
                "\n",
                "| Section | Topic | Description |\n",
                "|---------|-------|-------------|\n",
                "| 1 | **Environment Setup** | Managing Python environments for ML projects |\n",
                "| 2 | **Installing Transformers** | Setting up the core Hugging Face library |\n",
                "| 3 | **GPU Acceleration** | Using CUDA/MPS for faster inference |\n",
                "| 4 | **Hugging Face Hub** | Programmatic access to models and datasets |\n",
                "| 5 | **Cache Management** | Managing downloaded model files |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Environment Setup\n",
                "\n",
                "### Why Virtual Environments Matter\n",
                "\n",
                "Virtual environments are **essential** for machine learning projects because:\n",
                "\n",
                "| Problem | Solution |\n",
                "|---------|----------|\n",
                "| Different projects need different package versions | Isolated environments prevent conflicts |\n",
                "| System Python can break with ML packages | Virtual envs protect your system |\n",
                "| Reproducibility across machines | Export and share environment specs |\n",
                "| Clean experimentation | Easy to delete and recreate |\n",
                "\n",
                "### Environment Options\n",
                "\n",
                "```bash\n",
                "# Option 1: Using Conda (Recommended for ML)\n",
                "conda create -n huggingface python=3.10\n",
                "conda activate huggingface\n",
                "\n",
                "# Option 2: Using venv (Built into Python)\n",
                "python -m venv huggingface_env\n",
                "# Windows: huggingface_env\\Scripts\\activate\n",
                "# macOS/Linux: source huggingface_env/bin/activate\n",
                "\n",
                "# Option 3: Using virtualenv\n",
                "pip install virtualenv\n",
                "virtualenv huggingface_env\n",
                "```\n",
                "\n",
                "> üí° **Tip**: Conda is particularly useful for ML because it can handle non-Python dependencies like CUDA libraries.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Installing the Transformers Library\n",
                "\n",
                "Let's install all the necessary packages for this chapter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to install required packages\n",
                "# %pip install transformers torch accelerate huggingface_hub gputil psutil -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì¶ Installed Library Versions\n",
                        "========================================\n",
                        "ü§ó Transformers: 4.57.3\n",
                        "üî• PyTorch: 2.9.0+cu126\n",
                        "üåê Hugging Face Hub: 0.36.0\n"
                    ]
                }
            ],
            "source": [
                "# Let's verify our installations and check library versions\n",
                "import transformers\n",
                "import torch\n",
                "import huggingface_hub\n",
                "\n",
                "print(\"üì¶ Installed Library Versions\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"ü§ó Transformers: {transformers.__version__}\")\n",
                "print(f\"üî• PyTorch: {torch.__version__}\")\n",
                "print(f\"üåê Hugging Face Hub: {huggingface_hub.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3Ô∏è‚É£ GPU Acceleration\n",
                "\n",
                "GPU acceleration can make your models run **10-100x faster** than on CPU. Let's understand the different hardware options.\n",
                "\n",
                "### Hardware Options\n",
                "\n",
                "| Platform | GPU Technology | Detection Method |\n",
                "|----------|---------------|------------------|\n",
                "| Windows/Linux | NVIDIA CUDA | `torch.cuda.is_available()` |\n",
                "| macOS (Apple Silicon) | MPS (Metal) | `torch.backends.mps.is_available()` |\n",
                "| Any | CPU (fallback) | Always available |\n",
                "\n",
                "### 3.1 Detecting Available Hardware"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üñ•Ô∏è System Information\n",
                        "==================================================\n",
                        "  OS: Linux 6.6.105+\n",
                        "  Python: 3.12.12\n",
                        "  Machine: x86_64\n",
                        "\n",
                        "üéÆ GPU Detection\n",
                        "==================================================\n",
                        "  CUDA Available: ‚úÖ Yes\n",
                        "  CUDA Version: 12.6\n",
                        "  GPU Count: 1\n",
                        "  GPU 0: Tesla T4 (14.7 GB)\n",
                        "  MPS Available: ‚ùå No\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import platform\n",
                "\n",
                "def get_system_info():\n",
                "    \"\"\"Get comprehensive system and hardware information.\"\"\"\n",
                "    \n",
                "    print(\"üñ•Ô∏è System Information\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"  OS: {platform.system()} {platform.release()}\")\n",
                "    print(f\"  Python: {platform.python_version()}\")\n",
                "    print(f\"  Machine: {platform.machine()}\")\n",
                "    print()\n",
                "    \n",
                "    print(\"üéÆ GPU Detection\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Check CUDA (NVIDIA)\n",
                "    cuda_available = torch.cuda.is_available()\n",
                "    print(f\"  CUDA Available: {'‚úÖ Yes' if cuda_available else '‚ùå No'}\")\n",
                "    \n",
                "    if cuda_available:\n",
                "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
                "        print(f\"  GPU Count: {torch.cuda.device_count()}\")\n",
                "        for i in range(torch.cuda.device_count()):\n",
                "            gpu_name = torch.cuda.get_device_name(i)\n",
                "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
                "            print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
                "    \n",
                "    # Check MPS (Apple Silicon)\n",
                "    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
                "    print(f\"  MPS Available: {'‚úÖ Yes' if mps_available else '‚ùå No'}\")\n",
                "    \n",
                "    return cuda_available, mps_available\n",
                "\n",
                "cuda_available, mps_available = get_system_info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Advanced GPU Monitoring\n",
                "\n",
                "For NVIDIA GPUs, we can get detailed utilization statistics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Detailed GPU Statistics\n",
                        "============================================================\n",
                        "\n",
                        "üéÆ GPU 0: Tesla T4\n",
                        "   ‚îú‚îÄ Memory Total: 15360 MB\n",
                        "   ‚îú‚îÄ Memory Used: 5938 MB (38.7%)\n",
                        "   ‚îú‚îÄ Memory Free: 9156 MB\n",
                        "   ‚îú‚îÄ GPU Load: 0.0%\n",
                        "   ‚îî‚îÄ Temperature: 47.0¬∞C\n"
                    ]
                }
            ],
            "source": [
                "def get_gpu_stats():\n",
                "    \"\"\"Get detailed GPU statistics for NVIDIA cards.\"\"\"\n",
                "    \n",
                "    try:\n",
                "        import GPUtil\n",
                "        gpus = GPUtil.getGPUs()\n",
                "        \n",
                "        if not gpus:\n",
                "            print(\"‚ö†Ô∏è No NVIDIA GPUs detected via GPUtil\")\n",
                "            return\n",
                "        \n",
                "        print(\"üìä Detailed GPU Statistics\")\n",
                "        print(\"=\" * 60)\n",
                "        \n",
                "        for gpu in gpus:\n",
                "            print(f\"\\nüéÆ GPU {gpu.id}: {gpu.name}\")\n",
                "            print(f\"   ‚îú‚îÄ Memory Total: {gpu.memoryTotal:.0f} MB\")\n",
                "            print(f\"   ‚îú‚îÄ Memory Used: {gpu.memoryUsed:.0f} MB ({gpu.memoryUsed/gpu.memoryTotal*100:.1f}%)\")\n",
                "            print(f\"   ‚îú‚îÄ Memory Free: {gpu.memoryFree:.0f} MB\")\n",
                "            print(f\"   ‚îú‚îÄ GPU Load: {gpu.load*100:.1f}%\")\n",
                "            print(f\"   ‚îî‚îÄ Temperature: {gpu.temperature}¬∞C\")\n",
                "            \n",
                "    except ImportError:\n",
                "        print(\"üí° Install GPUtil for detailed GPU stats: pip install gputil\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Could not get GPU stats: {e}\")\n",
                "\n",
                "get_gpu_stats()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Smart Device Selection\n",
                "\n",
                "Let's create a utility function that automatically selects the best available device."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Selected: CUDA GPU (Tesla T4)\n"
                    ]
                }
            ],
            "source": [
                "def get_optimal_device(prefer_gpu=True, verbose=True):\n",
                "    \"\"\"\n",
                "    Automatically select the best available compute device.\n",
                "    \n",
                "    Args:\n",
                "        prefer_gpu: Whether to prefer GPU over CPU\n",
                "        verbose: Whether to print selection info\n",
                "    \n",
                "    Returns:\n",
                "        device: The optimal device for computation\n",
                "        device_id: Device ID for pipeline() function (-1 for CPU, 0+ for GPU)\n",
                "    \"\"\"\n",
                "    \n",
                "    if prefer_gpu:\n",
                "        # Priority 1: NVIDIA CUDA\n",
                "        if torch.cuda.is_available():\n",
                "            device = torch.device(\"cuda\")\n",
                "            device_id = 0\n",
                "            device_name = torch.cuda.get_device_name(0)\n",
                "            if verbose:\n",
                "                print(f\"üöÄ Selected: CUDA GPU ({device_name})\")\n",
                "        \n",
                "        # Priority 2: Apple MPS\n",
                "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
                "            device = torch.device(\"mps\")\n",
                "            device_id = \"mps\"\n",
                "            if verbose:\n",
                "                print(\"üçé Selected: Apple Metal (MPS)\")\n",
                "        \n",
                "        # Fallback: CPU\n",
                "        else:\n",
                "            device = torch.device(\"cpu\")\n",
                "            device_id = -1\n",
                "            if verbose:\n",
                "                print(\"üíª Selected: CPU (no GPU available)\")\n",
                "    else:\n",
                "        device = torch.device(\"cpu\")\n",
                "        device_id = -1\n",
                "        if verbose:\n",
                "            print(\"üíª Selected: CPU (GPU disabled)\")\n",
                "    \n",
                "    return device, device_id\n",
                "\n",
                "# Get the optimal device\n",
                "device, device_id = get_optimal_device()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 GPU vs CPU Performance Comparison\n",
                "\n",
                "Let's demonstrate the performance difference between GPU and CPU!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üèéÔ∏è Performance Benchmark: GPU vs CPU\n",
                        "==================================================\n",
                        "Processing 40 movie reviews...\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
                        "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
                        "You are not authenticated with the Hugging Face Hub in this notebook.\n",
                        "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5e73aa75449e4ecc84a17c49fa893631",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "12b7b316680341dd98af191bdb105a14",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "17d080eaf19c4955a323c7ab92f57273",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "df79aba37697411f9df3f389be1c794f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö° Optimal Device (cuda):\n",
                        "   Time: 0.259 seconds\n",
                        "   Throughput: 154.3 reviews/second\n",
                        "\n",
                        "üìä Comparing with CPU...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cpu\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üíª CPU:\n",
                        "   Time: 1.718 seconds\n",
                        "   Throughput: 23.3 reviews/second\n",
                        "\n",
                        "üöÄ GPU Speedup: 6.63x faster!\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "from transformers import pipeline\n",
                "\n",
                "# Sample movie reviews for sentiment analysis\n",
                "movie_reviews = [\n",
                "    \"This film is a masterpiece of modern cinema! The acting was phenomenal.\",\n",
                "    \"Terrible waste of time. The plot made no sense whatsoever.\",\n",
                "    \"A heartwarming story that will make you laugh and cry.\",\n",
                "    \"The special effects were amazing but the story was lacking.\",\n",
                "    \"One of the best movies I've ever seen. Highly recommended!\",\n",
                "    \"Boring and predictable. I fell asleep halfway through.\",\n",
                "    \"An absolute thrill ride from start to finish!\",\n",
                "    \"The director has outdone themselves with this one.\"\n",
                "] * 5  # Multiply for better timing accuracy\n",
                "\n",
                "def benchmark_device(device_id, device_name, texts):\n",
                "    \"\"\"Benchmark sentiment analysis on a specific device.\"\"\"\n",
                "    \n",
                "    try:\n",
                "        # Create pipeline on specified device\n",
                "        classifier = pipeline(\n",
                "            \"sentiment-analysis\",\n",
                "            model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "            device=device_id\n",
                "        )\n",
                "        \n",
                "        # Warm-up run\n",
                "        _ = classifier(texts[0])\n",
                "        \n",
                "        # Timed run\n",
                "        start_time = time.time()\n",
                "        results = classifier(texts)\n",
                "        end_time = time.time()\n",
                "        \n",
                "        elapsed = end_time - start_time\n",
                "        throughput = len(texts) / elapsed\n",
                "        \n",
                "        return elapsed, throughput, results\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è Error on {device_name}: {e}\")\n",
                "        return None, None, None\n",
                "\n",
                "print(\"üèéÔ∏è Performance Benchmark: GPU vs CPU\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Processing {len(movie_reviews)} movie reviews...\\n\")\n",
                "\n",
                "# Benchmark on optimal device\n",
                "optimal_time, optimal_throughput, _ = benchmark_device(\n",
                "    device_id, \n",
                "    \"Optimal Device\", \n",
                "    movie_reviews\n",
                ")\n",
                "\n",
                "if optimal_time:\n",
                "    print(f\"‚ö° Optimal Device ({device}):\")\n",
                "    print(f\"   Time: {optimal_time:.3f} seconds\")\n",
                "    print(f\"   Throughput: {optimal_throughput:.1f} reviews/second\")\n",
                "\n",
                "# Also benchmark CPU for comparison if we're using GPU\n",
                "if device_id != -1:\n",
                "    print(\"\\nüìä Comparing with CPU...\")\n",
                "    cpu_time, cpu_throughput, _ = benchmark_device(-1, \"CPU\", movie_reviews)\n",
                "    \n",
                "    if cpu_time:\n",
                "        print(f\"\\nüíª CPU:\")\n",
                "        print(f\"   Time: {cpu_time:.3f} seconds\")\n",
                "        print(f\"   Throughput: {cpu_throughput:.1f} reviews/second\")\n",
                "        \n",
                "        speedup = cpu_time / optimal_time\n",
                "        print(f\"\\nüöÄ GPU Speedup: {speedup:.2f}x faster!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4Ô∏è‚É£ Working with the Hugging Face Hub\n",
                "\n",
                "The **Hugging Face Hub** is like GitHub for machine learning. You can:\n",
                "\n",
                "- üì• Download models, datasets, and files\n",
                "- üì§ Upload your own models\n",
                "- üîê Access private repositories\n",
                "- ü§ù Collaborate with the community\n",
                "\n",
                "### 4.1 Exploring the Hub Programmatically"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Top Sentiment Analysis Models on Hugging Face Hub\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'list_models': task. Will not be supported from version '1.0'.\n",
                        "\n",
                        "Use `filter` instead.\n",
                        "  warnings.warn(message, FutureWarning)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "1. cross-encoder/ms-marco-MiniLM-L6-v2\n",
                        "   üì• Downloads: 4,645,177\n",
                        "   ‚ù§Ô∏è Likes: 178\n",
                        "\n",
                        "2. facebook/bart-large-mnli\n",
                        "   üì• Downloads: 3,793,595\n",
                        "   ‚ù§Ô∏è Likes: 1507\n",
                        "\n",
                        "3. cardiffnlp/twitter-roberta-base-sentiment-latest\n",
                        "   üì• Downloads: 3,665,449\n",
                        "   ‚ù§Ô∏è Likes: 747\n",
                        "\n",
                        "4. distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
                        "   üì• Downloads: 2,905,900\n",
                        "   ‚ù§Ô∏è Likes: 861\n",
                        "\n",
                        "5. BAAI/bge-reranker-v2-m3\n",
                        "   üì• Downloads: 2,757,640\n",
                        "   ‚ù§Ô∏è Likes: 847\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import HfApi, list_models\n",
                "\n",
                "# Initialize the API\n",
                "api = HfApi()\n",
                "\n",
                "# Search for popular sentiment analysis models\n",
                "print(\"üîç Top Sentiment Analysis Models on Hugging Face Hub\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "models = list(\n",
                "    list_models(\n",
                "        task=\"text-classification\",\n",
                "        sort=\"downloads\",\n",
                "        direction=-1,  # Descending\n",
                "        limit=5\n",
                "    )\n",
                ")\n",
                "\n",
                "for i, model in enumerate(models, 1):\n",
                "    downloads = model.downloads if hasattr(model, 'downloads') else 'N/A'\n",
                "    likes = model.likes if hasattr(model, 'likes') else 'N/A'\n",
                "    print(f\"\\n{i}. {model.id}\")\n",
                "    print(f\"   üì• Downloads: {downloads:,}\" if isinstance(downloads, int) else f\"   üì• Downloads: {downloads}\")\n",
                "    print(f\"   ‚ù§Ô∏è Likes: {likes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Downloading Model Files\n",
                "\n",
                "You can download specific files or entire model repositories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì• Downloading Model Files\n",
                        "==================================================\n",
                        "\n",
                        "1Ô∏è‚É£ Downloading a specific file...\n",
                        "   ‚úÖ Downloaded config to: /root/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/714eb0fa89d2f80546fda750413ed43d93601a13/config.json\n",
                        "   üìÅ File size: 629 bytes\n",
                        "   üè∑Ô∏è Model type: distilbert\n",
                        "   üìä Hidden size: N/A\n",
                        "   üî¢ Vocab size: 30,522\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import hf_hub_download, snapshot_download\n",
                "import os\n",
                "\n",
                "print(\"üì• Downloading Model Files\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Example 1: Download a specific file (config.json is small and fast)\n",
                "print(\"\\n1Ô∏è‚É£ Downloading a specific file...\")\n",
                "config_path = hf_hub_download(\n",
                "    repo_id=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
                "    filename=\"config.json\"\n",
                ")\n",
                "print(f\"   ‚úÖ Downloaded config to: {config_path}\")\n",
                "print(f\"   üìÅ File size: {os.path.getsize(config_path):,} bytes\")\n",
                "\n",
                "# Let's peek at the config\n",
                "import json\n",
                "with open(config_path, 'r') as f:\n",
                "    config = json.load(f)\n",
                "print(f\"   üè∑Ô∏è Model type: {config.get('model_type', 'N/A')}\")\n",
                "print(f\"   üìä Hidden size: {config.get('hidden_size', 'N/A')}\")\n",
                "print(f\"   üî¢ Vocab size: {config.get('vocab_size', 'N/A'):,}\" if config.get('vocab_size') else \"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "2Ô∏è‚É£ Files in a model repository:\n",
                        "\n",
                        "üì¶ Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
                        "üè∑Ô∏è Tags: transformers, pytorch, tf, rust, onnx...\n",
                        "\n",
                        "üìÅ Files:\n",
                        "   ‚Ä¢ .gitattributes (0.0 KB)\n",
                        "   ‚Ä¢ README.md (0.0 KB)\n",
                        "   ‚Ä¢ config.json (0.0 KB)\n",
                        "   ‚Ä¢ map.jpeg (0.0 KB)\n",
                        "   ‚Ä¢ model.safetensors (0.0 KB)\n",
                        "   ‚Ä¢ onnx/added_tokens.json (0.0 KB)\n",
                        "   ‚Ä¢ onnx/config.json (0.0 KB)\n",
                        "   ‚Ä¢ onnx/model.onnx (0.0 KB)\n",
                        "   ‚Ä¢ onnx/special_tokens_map.json (0.0 KB)\n",
                        "   ‚Ä¢ onnx/tokenizer.json (0.0 KB)\n",
                        "   ‚Ä¢ onnx/tokenizer_config.json (0.0 KB)\n",
                        "   ‚Ä¢ onnx/vocab.txt (0.0 KB)\n",
                        "   ‚Ä¢ pytorch_model.bin (0.0 KB)\n",
                        "   ‚Ä¢ rust_model.ot (0.0 KB)\n",
                        "   ‚Ä¢ tf_model.h5 (0.0 KB)\n",
                        "   ‚Ä¢ tokenizer_config.json (0.0 KB)\n",
                        "   ‚Ä¢ vocab.txt (0.0 KB)\n"
                    ]
                }
            ],
            "source": [
                "# Example 2: List files in a repository\n",
                "print(\"\\n2Ô∏è‚É£ Files in a model repository:\")\n",
                "\n",
                "model_info = api.model_info(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
                "\n",
                "print(f\"\\nüì¶ Model: {model_info.id}\")\n",
                "print(f\"üè∑Ô∏è Tags: {', '.join(model_info.tags[:5])}...\" if len(model_info.tags) > 5 else f\"üè∑Ô∏è Tags: {', '.join(model_info.tags)}\")\n",
                "print(f\"\\nüìÅ Files:\")\n",
                "\n",
                "for sibling in model_info.siblings:\n",
                "    size_kb = sibling.size / 1024 if sibling.size else 0\n",
                "    if size_kb > 1024:\n",
                "        size_str = f\"{size_kb/1024:.1f} MB\"\n",
                "    else:\n",
                "        size_str = f\"{size_kb:.1f} KB\"\n",
                "    print(f\"   ‚Ä¢ {sibling.rfilename} ({size_str})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Authentication\n",
                "\n",
                "For private repositories or uploading models, you need to authenticate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîê Authentication Status\n",
                        "==================================================\n",
                        "‚ùå Not logged in\n",
                        "\n",
                        "üí° To login, you have several options:\n",
                        "\n",
                        "   Option 1: Interactive login\n",
                        "   >>> from huggingface_hub import login\n",
                        "   >>> login()\n",
                        "\n",
                        "   Option 2: CLI login\n",
                        "   $ huggingface-cli login\n",
                        "\n",
                        "   Option 3: Environment variable\n",
                        "   $ export HUGGING_FACE_HUB_TOKEN=your_token_here\n",
                        "\n",
                        "üîë Get your token at: https://huggingface.co/settings/tokens\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import whoami, login\n",
                "\n",
                "print(\"üîê Authentication Status\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "try:\n",
                "    user_info = whoami()\n",
                "    print(f\"‚úÖ Logged in as: {user_info['name']}\")\n",
                "    print(f\"üìß Email: {user_info.get('email', 'Not shared')}\")\n",
                "    print(f\"üè¢ Organizations: {', '.join(user_info.get('orgs', [])) or 'None'}\")\n",
                "except Exception:\n",
                "    print(\"‚ùå Not logged in\")\n",
                "    print(\"\\nüí° To login, you have several options:\")\n",
                "    print(\"\\n   Option 1: Interactive login\")\n",
                "    print(\"   >>> from huggingface_hub import login\")\n",
                "    print(\"   >>> login()\")\n",
                "    print(\"\\n   Option 2: CLI login\")\n",
                "    print(\"   $ huggingface-cli login\")\n",
                "    print(\"\\n   Option 3: Environment variable\")\n",
                "    print(\"   $ export HUGGING_FACE_HUB_TOKEN=your_token_here\")\n",
                "    print(\"\\nüîë Get your token at: https://huggingface.co/settings/tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Exploring Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Popular Datasets on Hugging Face Hub\n",
                        "============================================================\n",
                        "\n",
                        "Rank  Dataset ID                              Downloads      \n",
                        "------------------------------------------------------------\n",
                        "1     deepmind/code_contests                  2,539,159      \n",
                        "2     google-research-datasets/mbpp           2,383,896      \n",
                        "3     huggingface/documentation-images        1,744,208      \n",
                        "4     m-a-p/FineFineWeb                       1,198,524      \n",
                        "5     hf-doc-build/doc-build                  1,161,175      \n",
                        "6     nvidia/PhysicalAI-Robotics-GR00T-X-Emb..826,397        \n",
                        "7     Salesforce/wikitext                     826,140        \n",
                        "8     banned-historical-archives/banned-hist..788,066        \n",
                        "9     lavita/medical-qa-shared-task-v1-toy    778,513        \n",
                        "10    MRSAudio/MRSAudio                       653,649        \n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import list_datasets\n",
                "\n",
                "print(\"üìä Popular Datasets on Hugging Face Hub\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Get popular datasets\n",
                "datasets = list(\n",
                "    list_datasets(\n",
                "        sort=\"downloads\",\n",
                "        direction=-1,\n",
                "        limit=10\n",
                "    )\n",
                ")\n",
                "\n",
                "print(f\"\\n{'Rank':<6}{'Dataset ID':<40}{'Downloads':<15}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for i, ds in enumerate(datasets, 1):\n",
                "    downloads = ds.downloads if hasattr(ds, 'downloads') else 'N/A'\n",
                "    downloads_str = f\"{downloads:,}\" if isinstance(downloads, int) else str(downloads)\n",
                "    # Truncate long names\n",
                "    name = ds.id[:38] + \"..\" if len(ds.id) > 40 else ds.id\n",
                "    print(f\"{i:<6}{name:<40}{downloads_str:<15}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5Ô∏è‚É£ Managing the Model Cache\n",
                "\n",
                "When you use Hugging Face models, they're downloaded and cached locally. This can consume significant disk space!\n",
                "\n",
                "### 5.1 Understanding the Cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üíæ Hugging Face Cache Information\n",
                        "============================================================\n",
                        "üìÅ Cache Location: /root/.cache/huggingface\n",
                        "\n",
                        "üìä Cache Statistics:\n",
                        "   ‚Ä¢ Total Size: 6.92 GB\n",
                        "   ‚Ä¢ Number of Repos: 8\n",
                        "\n",
                        "üì¶ Cached Repositories:\n",
                        "   ‚Ä¢ sshleifer/distilbart-cnn-12-6 (2.28 GB)\n",
                        "   ‚Ä¢ facebook/bart-large-mnli (1.52 GB)\n",
                        "   ‚Ä¢ dbmdz/bert-large-cased-finetuned-conll03-english (1.24 GB)\n",
                        "   ‚Ä¢ nlptown/bert-base-multilingual-uncased-sentiment (639.3 MB)\n",
                        "   ‚Ä¢ gpt2 (525.4 MB)\n",
                        "   ‚Ä¢ distilbert/distilbert-base-uncased-finetuned-sst-2-english (255.6 MB)\n",
                        "   ‚Ä¢ distilbert-base-uncased-finetuned-sst-2-english (255.6 MB)\n",
                        "   ‚Ä¢ distilbert/distilbert-base-cased-distilled-squad (249.3 MB)\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import scan_cache_dir, HfFolder\n",
                "import os\n",
                "\n",
                "print(\"üíæ Hugging Face Cache Information\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Get cache location\n",
                "cache_path = os.path.expanduser(\"~/.cache/huggingface\")\n",
                "if os.path.exists(cache_path):\n",
                "    print(f\"üìÅ Cache Location: {cache_path}\")\n",
                "else:\n",
                "    print(f\"üìÅ Default Cache Location: {cache_path} (not created yet)\")\n",
                "\n",
                "# Scan the cache\n",
                "try:\n",
                "    cache_info = scan_cache_dir()\n",
                "    \n",
                "    print(f\"\\nüìä Cache Statistics:\")\n",
                "    print(f\"   ‚Ä¢ Total Size: {cache_info.size_on_disk / (1024**3):.2f} GB\")\n",
                "    print(f\"   ‚Ä¢ Number of Repos: {len(cache_info.repos)}\")\n",
                "    \n",
                "    if cache_info.repos:\n",
                "        print(f\"\\nüì¶ Cached Repositories:\")\n",
                "        \n",
                "        # Sort by size\n",
                "        sorted_repos = sorted(\n",
                "            cache_info.repos, \n",
                "            key=lambda x: x.size_on_disk, \n",
                "            reverse=True\n",
                "        )\n",
                "        \n",
                "        for repo in sorted_repos[:10]:  # Show top 10\n",
                "            size_mb = repo.size_on_disk / (1024**2)\n",
                "            if size_mb > 1024:\n",
                "                size_str = f\"{size_mb/1024:.2f} GB\"\n",
                "            else:\n",
                "                size_str = f\"{size_mb:.1f} MB\"\n",
                "            print(f\"   ‚Ä¢ {repo.repo_id} ({size_str})\")\n",
                "            \n",
                "except Exception as e:\n",
                "    print(f\"\\n‚ö†Ô∏è Could not scan cache: {e}\")\n",
                "    print(\"   This might happen if no models have been downloaded yet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Cache Management Commands\n",
                "\n",
                "You can manage your cache using the Hugging Face CLI:\n",
                "\n",
                "```bash\n",
                "# View all cached models and their sizes\n",
                "huggingface-cli scan-cache\n",
                "\n",
                "# Interactive deletion of cached models\n",
                "huggingface-cli delete-cache\n",
                "\n",
                "# Delete specific revisions (advanced)\n",
                "huggingface-cli delete-cache --revision <commit_hash>\n",
                "```\n",
                "\n",
                "### 5.3 Programmatic Cache Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìà Cache Summary\n",
                        "========================================\n",
                        "üíæ Total Size: 6.92 GB\n",
                        "üì¶ Total Repositories: 8\n",
                        "   ‚Ä¢ Models: 8\n",
                        "   ‚Ä¢ Datasets: 0\n",
                        "   ‚Ä¢ Spaces: 0\n"
                    ]
                }
            ],
            "source": [
                "def get_cache_summary():\n",
                "    \"\"\"\n",
                "    Get a summary of the Hugging Face cache.\n",
                "    \n",
                "    Returns:\n",
                "        dict: Cache statistics\n",
                "    \"\"\"\n",
                "    try:\n",
                "        cache_info = scan_cache_dir()\n",
                "        \n",
                "        total_size_gb = cache_info.size_on_disk / (1024**3)\n",
                "        \n",
                "        # Categorize by type\n",
                "        models = [r for r in cache_info.repos if r.repo_type == \"model\"]\n",
                "        datasets = [r for r in cache_info.repos if r.repo_type == \"dataset\"]\n",
                "        spaces = [r for r in cache_info.repos if r.repo_type == \"space\"]\n",
                "        \n",
                "        return {\n",
                "            \"total_size_gb\": total_size_gb,\n",
                "            \"total_repos\": len(cache_info.repos),\n",
                "            \"models\": len(models),\n",
                "            \"datasets\": len(datasets),\n",
                "            \"spaces\": len(spaces),\n",
                "            \"repos\": cache_info.repos\n",
                "        }\n",
                "    except Exception as e:\n",
                "        return {\"error\": str(e)}\n",
                "\n",
                "summary = get_cache_summary()\n",
                "\n",
                "if \"error\" not in summary:\n",
                "    print(\"üìà Cache Summary\")\n",
                "    print(\"=\" * 40)\n",
                "    print(f\"üíæ Total Size: {summary['total_size_gb']:.2f} GB\")\n",
                "    print(f\"üì¶ Total Repositories: {summary['total_repos']}\")\n",
                "    print(f\"   ‚Ä¢ Models: {summary['models']}\")\n",
                "    print(f\"   ‚Ä¢ Datasets: {summary['datasets']}\")\n",
                "    print(f\"   ‚Ä¢ Spaces: {summary['spaces']}\")\n",
                "else:\n",
                "    print(f\"‚ö†Ô∏è No cache data available: {summary['error']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Practical Example: Building a Smart Text Analyzer\n",
                "\n",
                "Let's combine everything we've learned into a practical example!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ SmartTextAnalyzer class defined!\n",
                        "Run the next cell to see it in action.\n"
                    ]
                }
            ],
            "source": [
                "from transformers import pipeline\n",
                "import torch\n",
                "import time\n",
                "\n",
                "class SmartTextAnalyzer:\n",
                "    \"\"\"\n",
                "    A text analyzer that automatically uses the best available hardware.\n",
                "    Demonstrates GPU detection, model loading, and multiple NLP tasks.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, use_gpu=True, verbose=True):\n",
                "        \"\"\"\n",
                "        Initialize the analyzer with automatic device selection.\n",
                "        \n",
                "        Args:\n",
                "            use_gpu: Whether to use GPU if available\n",
                "            verbose: Whether to print status messages\n",
                "        \"\"\"\n",
                "        self.verbose = verbose\n",
                "        self.device, self.device_id = get_optimal_device(use_gpu, verbose)\n",
                "        \n",
                "        if verbose:\n",
                "            print(\"\\n‚è≥ Loading models...\")\n",
                "        \n",
                "        start = time.time()\n",
                "        \n",
                "        # Load models\n",
                "        self.sentiment = pipeline(\n",
                "            \"sentiment-analysis\",\n",
                "            device=self.device_id\n",
                "        )\n",
                "        \n",
                "        self.summarizer = pipeline(\n",
                "            \"summarization\",\n",
                "            model=\"facebook/bart-large-cnn\",\n",
                "            device=self.device_id\n",
                "        )\n",
                "        \n",
                "        self.classifier = pipeline(\n",
                "            \"zero-shot-classification\",\n",
                "            device=self.device_id\n",
                "        )\n",
                "        \n",
                "        load_time = time.time() - start\n",
                "        \n",
                "        if verbose:\n",
                "            print(f\"‚úÖ Models loaded in {load_time:.2f} seconds\")\n",
                "    \n",
                "    def analyze(self, text, categories=None):\n",
                "        \"\"\"\n",
                "        Perform comprehensive text analysis.\n",
                "        \n",
                "        Args:\n",
                "            text: The text to analyze\n",
                "            categories: Optional list of categories for classification\n",
                "        \n",
                "        Returns:\n",
                "            dict: Analysis results\n",
                "        \"\"\"\n",
                "        if categories is None:\n",
                "            categories = [\"Technology\", \"Business\", \"Science\", \"Sports\", \"Entertainment\"]\n",
                "        \n",
                "        results = {\n",
                "            \"text_preview\": text[:100] + \"...\" if len(text) > 100 else text,\n",
                "            \"word_count\": len(text.split()),\n",
                "        }\n",
                "        \n",
                "        # Sentiment Analysis\n",
                "        sentiment = self.sentiment(text[:512])[0]  # Limit input length\n",
                "        results[\"sentiment\"] = {\n",
                "            \"label\": sentiment[\"label\"],\n",
                "            \"confidence\": f\"{sentiment['score']:.2%}\"\n",
                "        }\n",
                "        \n",
                "        # Summarization (if text is long enough)\n",
                "        if len(text.split()) > 50:\n",
                "            summary = self.summarizer(\n",
                "                text, \n",
                "                max_length=100, \n",
                "                min_length=30, \n",
                "                do_sample=False\n",
                "            )[0]\n",
                "            results[\"summary\"] = summary[\"summary_text\"]\n",
                "        else:\n",
                "            results[\"summary\"] = \"[Text too short for summarization]\"\n",
                "        \n",
                "        # Zero-shot Classification\n",
                "        classification = self.classifier(text[:512], categories)\n",
                "        results[\"category\"] = {\n",
                "            \"predicted\": classification[\"labels\"][0],\n",
                "            \"confidence\": f\"{classification['scores'][0]:.2%}\"\n",
                "        }\n",
                "        \n",
                "        return results\n",
                "\n",
                "# Don't initialize yet - we'll do it in the next cell with a sample analysis\n",
                "print(\"‚úÖ SmartTextAnalyzer class defined!\")\n",
                "print(\"Run the next cell to see it in action.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Selected: CUDA GPU (Tesla T4)\n",
                        "\n",
                        "‚è≥ Loading models...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9d9caa0a70cd4bf9903b336d448d5782",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8d39a030f8d14bc28c485871ae2179bd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0a7942a9a1f84de0a509145caa84760d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3fa91b5ead6e4fe1b16fdc4111f8ac96",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "vocab.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fc0df30960c347759206e6416ecfc8f1",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "merges.txt: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3a8b8ea906054b2d94ac8ac321731451",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Device set to use cuda:0\n",
                        "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
                        "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
                        "Device set to use cuda:0\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Models loaded in 27.65 seconds\n",
                        "\n",
                        "============================================================\n",
                        "üìä Text Analysis Results\n",
                        "============================================================\n",
                        "\n",
                        "üìù Text Preview: \n",
                        "The renewable energy sector has experienced unprecedented growth in recent years, \n",
                        "with solar and w...\n",
                        "üìè Word Count: 111\n",
                        "\n",
                        "üí≠ Sentiment: POSITIVE (98.03%)\n",
                        "\n",
                        "üè∑Ô∏è Category: Technology (30.88%)\n",
                        "\n",
                        "üìã Summary:\n",
                        "   Renewable energy capacity is set to increase 50% over the next five years. This expansion is expected to be led by solar power, which has become the cheapest source of electricity in history in many regions. The transition away from fossil fuels is also creating millions of new jobs and economic opportunities worldwide.\n",
                        "\n",
                        "‚è±Ô∏è Analysis completed in 1.71 seconds\n"
                    ]
                }
            ],
            "source": [
                "# Initialize the analyzer\n",
                "analyzer = SmartTextAnalyzer()\n",
                "\n",
                "# Sample article about renewable energy\n",
                "sample_article = \"\"\"\n",
                "The renewable energy sector has experienced unprecedented growth in recent years, \n",
                "with solar and wind power installations reaching record levels globally. According \n",
                "to the International Energy Agency, renewable energy capacity is set to increase \n",
                "by 50% over the next five years, driven by falling costs and supportive government \n",
                "policies. This expansion is expected to be led by solar power, which has become \n",
                "the cheapest source of electricity in history in many regions. The transition away \n",
                "from fossil fuels is not only beneficial for the environment but is also creating \n",
                "millions of new jobs and economic opportunities worldwide. However, challenges \n",
                "remain, including the need for better energy storage solutions and grid modernization.\n",
                "\"\"\"\n",
                "\n",
                "# Analyze the text\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"üìä Text Analysis Results\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "start = time.time()\n",
                "results = analyzer.analyze(sample_article)\n",
                "analysis_time = time.time() - start\n",
                "\n",
                "print(f\"\\nüìù Text Preview: {results['text_preview']}\")\n",
                "print(f\"üìè Word Count: {results['word_count']}\")\n",
                "print(f\"\\nüí≠ Sentiment: {results['sentiment']['label']} ({results['sentiment']['confidence']})\")\n",
                "print(f\"\\nüè∑Ô∏è Category: {results['category']['predicted']} ({results['category']['confidence']})\")\n",
                "print(f\"\\nüìã Summary:\\n   {results['summary']}\")\n",
                "print(f\"\\n‚è±Ô∏è Analysis completed in {analysis_time:.2f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéØ Summary & Key Takeaways\n",
                "\n",
                "### What You Learned\n",
                "\n",
                "| Concept | Key Points |\n",
                "|---------|------------|\n",
                "| **Environment Setup** | Use virtual environments (conda/venv) to isolate projects |\n",
                "| **GPU Detection** | `torch.cuda.is_available()` for NVIDIA, `torch.backends.mps.is_available()` for Apple |\n",
                "| **Device Selection** | Use `device` parameter in `pipeline()` for hardware acceleration |\n",
                "| **Hub Access** | `huggingface_hub` package for programmatic access |\n",
                "| **File Downloads** | `hf_hub_download()` for files, `snapshot_download()` for repos |\n",
                "| **Cache Management** | `scan_cache_dir()` and CLI tools for managing disk space |\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. ‚úÖ Always check for GPU availability before running models\n",
                "2. ‚úÖ Use virtual environments for project isolation\n",
                "3. ‚úÖ Monitor cache size periodically to avoid disk space issues\n",
                "4. ‚úÖ Store your HF token securely (never commit to version control!)\n",
                "5. ‚úÖ Use the CLI tools (`huggingface-cli`) for quick operations\n",
                "\n",
                "### Useful CLI Commands\n",
                "\n",
                "```bash\n",
                "# Authentication\n",
                "huggingface-cli login\n",
                "huggingface-cli whoami\n",
                "\n",
                "# Cache management\n",
                "huggingface-cli scan-cache\n",
                "huggingface-cli delete-cache\n",
                "\n",
                "# Download models\n",
                "huggingface-cli download <repo_id>\n",
                "```\n",
                "\n",
                "---\n",
                "\n",
                "## üìö Additional Resources\n",
                "\n",
                "- üìñ [Hugging Face Hub Documentation](https://huggingface.co/docs/huggingface_hub)\n",
                "- üîß [Transformers Installation Guide](https://huggingface.co/docs/transformers/installation)\n",
                "- üñ•Ô∏è [GPU Support Guide](https://pytorch.org/get-started/locally/)\n",
                "- üíæ [Cache Management Guide](https://huggingface.co/docs/huggingface_hub/guides/manage-cache)\n",
                "\n",
                "---\n",
                "\n",
                "**Happy Learning! üöÄ**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
